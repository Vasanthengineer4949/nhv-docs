{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Neural Hacks with Vasanth","text":"<p>Here is what you can expect from the channel</p> <p>Subscribe to my channel</p>"},{"location":"#playlists-available-right-now","title":"Playlists Available right now","text":"<p>Currently there are five playlists in Channel. They are:</p> Playlist Number of Videos Status NLP Research Papers 4 Ongoing LLMs Related 6 Ongoing NLP Projects 4 Ongoing ML Bootcamp 7 Completed Transformer Embeddings 3 Completed"},{"location":"#nlp-research-papers","title":"NLP Research Papers","text":"<p>\ud83d\udcda\ud83d\udd2c Elevate your NLP knowledge with our Research Papers Demystified playlist! \ud83e\udde0\ud83d\udcdd Uncover the secrets behind the most impactful and trending papers in the field of Natural Language Processing</p> <p>Following are the videos available in the playlists right now</p> <ul> <li>LoRA Paper Explanation and Implementation</li> <li>DPO Paper Explanation and Implementation</li> <li>Platypus Paper Explanation and Implementation</li> </ul> <p>Playlist Page Link: Playlist Page</p>"},{"location":"#llms-related","title":"LLMs Related","text":"<p>\ud83d\udd25 Unleash the Power of Language Models! \ud83d\udcac\u2728 Dive into the world of cutting-edge AI with our Trending Concepts in LLMs Related playlist! </p> <p>Following are the videos available in the playlists right now</p> <ul> <li>Finetune Llama2 under 50 lines in Google Colab using QLoRA</li> <li>Deploy Local LLM Like a ChatGPT Chatbot</li> <li>DPO Paper Implementation</li> <li>IDEFICS Image Captioner</li> <li>CodeLlama Finetuning and CPU Inferencing <p>Playlist Page Link: Playlist Page</p> </li> </ul>"},{"location":"#nlp-projects","title":"NLP Projects","text":"<p>\ud83d\ude80\ud83d\udcca Unleash the NLP Powerhouse: Transformers in Action! \ud83e\udd16\ud83d\udd2e Ready to dive into the world of End-to-End NLP Projects? Welcome to the ultimate playlist that covers every conceivable task, every magic moment, and every mind-boggling application achievable with Transformers in the realm of Natural Language Processing. \ud83c\udf10\u2728</p> <p>Following are the videos available in the playlists right now</p> <ul> <li>Chatbot using FlanT5 and LoRA</li> <li>Text Classifier using DistilBERT</li> <li>AI In Cricket-1 -&gt; Cricket Post Match Report Generation using Live Commentary</li> <li>Keyword Extraction using BERT(NER)</li> </ul> <p>Playlist Page Link: Playlist Page</p>"},{"location":"#ml-bootcamp","title":"ML Bootcamp","text":"<p>\ud83d\ude80\ud83d\udcca Launch Your Machine Learning Journey: The Ultimate Beginner's Bootcamp! \ud83c\udf1f\ud83d\udd0d Ready to unravel the mysteries of Machine Learning project lifecycle? Welcome to the ML Expedition Bootcamp, a step-by-step guide to mastering every phase from Data Collection to Model Inferencing. \ud83d\udcc8\ud83d\udd2e</p> <p>Following are the videos available in the playlists:</p> <ul> <li>Introduction to ML Lifecycle</li> <li>EDA and its Importance</li> <li>Handling Missing Values</li> <li>Handling Categorical Values and Outliers</li> <li>Feature Scaling and Transformations</li> <li>Feature Selection, Train Test Split and Model Creation</li> <li>Model Hyperparameter Tuning</li> </ul> <p>Playlist Page Link: Playlist Page</p>"},{"location":"#transformer-embeddings","title":"Transformer Embeddings","text":"<p>\ud83d\udd0d\ud83d\udcda Dive into the World of Textual Representations: The Sentence Transformer Embeddings Exploration! \ud83c\udf10\u2728</p> <p>Welcome to a journey of understanding the essence of representations! In this captivating playlist, we delve deep into the transformative magic of Sentence Transformers and their remarkable embeddings. \ud83d\ude80\ud83d\udd2e</p> <p>Following are the videos available in the playlist</p> <ul> <li>Generating Embeddings from Sentence Transformers</li> <li>Reducing Dimension of Embeddings using UMAP</li> <li>Visualization of Embeddings</li> </ul> <p>Playlist Page Link: Playlist Page</p>"},{"location":"#more-updates","title":"More Updates","text":"<p>\ud83d\udd2e\u2728 Get Ready for More Surprises! \ud83c\udf89\ud83c\udfac Stay Tuned for the Unveiling of Our Next Thrilling Playlists! \ud83d\ude80\ud83c\udf1f </p> <p>Stay connected, keep your curiosity alive, and watch this space for the big reveals. Whether you're a tech enthusiast, an AI explorer, or simply hungry for knowledge, our future playlists are tailored to surprise and inspire you. \ud83c\udf88\ud83e\udd13</p>"},{"location":"embeddings/","title":"Transformer Embeddings Playlist","text":"<p>\ud83d\udd0d\ud83d\udcda Dive into the World of Textual Representations: The Sentence Transformer Embeddings Exploration! \ud83c\udf10\u2728</p> <p>Welcome to a journey of understanding the essence of representations! In this captivating playlist, we delve deep into the transformative magic of Sentence Transformers and their remarkable embeddings. \ud83d\ude80\ud83d\udd2e</p> <p>Each video is a key to unlocking a new facet of these innovative </p>"},{"location":"embeddings/#videos-available-playlist-completed","title":"Videos available - Playlist Completed","text":"<p>Here are the videos available in the playlist</p> Video Name Link Generate Embeddings from Sentence Transformers Link Reduce Embedding Dimensions with UMAP Link Visualize the Embeddings Link"},{"location":"embeddings/#1-generate-embeddings-from-sentence-transformers","title":"1. Generate Embeddings from Sentence Transformers","text":"<p>\ud83c\udfa5 Welcome to the Transformer Embeddings Series: Unleashing Text Representations!</p> <p>\ud83d\udcda In this inaugural video, we're diving into the exciting realm of generating embeddings \u2013 those magical vector representations of text \u2013 using sentence-transformer models. These embeddings hold immense power, serving as inputs for various machine learning models and neural networks.</p> <p>\ud83d\udca1 Get ready to explore the heart of AI as we unravel the process of crafting embeddings. Our focus? Harnessing the capabilities of sentence-transformer models. The embeddings we'll create here are your keys to unlocking a world of possibilities in text analysis.</p> <p>\u2699\ufe0f Want to amplify your data analysis and prediction skills? These embeddings are your secret weapon. By the end of this video, you'll understand how to generate these text representations like a pro. No matter your background, whether you're just starting out or have some experience, this tutorial is designed to empower you.</p> <p>\ud83c\udf1f Ready to turn text into powerful vectors? Hit that play button and let's embark on a journey to master the art of generating embeddings using sentence-transformer models. Empower your AI toolkit and revolutionize your approach to text analysis! \ud83d\ude80\ud83d\udcca</p> <p>Resources: Notebook</p>"},{"location":"embeddings/#2-reduce-embedding-dimensions-with-umap","title":"2. Reduce Embedding Dimensions with UMAP","text":"<p>\ud83c\udfa5 Dive into the Transformer Embeddings Series - Part 2: Shrinking Text Magic!</p> <p>\ud83d\udcda In our second video, we're continuing our journey to uncover text's secrets. This time, we're learning how to make those special text codes smaller, while keeping their meaning intact. We'll do this using something called UMAP (Uniform Manifold Approximation and Projection for Dimensionality Reduction). These compact codes are still super useful as inputs for machine learning models and neural networks. And guess what? They're even easier to visualize and understand!</p> <p>\ud83d\udca1 Imagine making text magic even more efficient \u2013 that's what we're doing! We'll take the embeddings we learned about earlier and use UMAP to make them smaller while keeping their power. These tiny codes can be used by computers to do cool things like predicting or understanding text. And the best part? We can now see and understand them better!</p> <p>\u2699\ufe0f Ready to level up your text skills? These compact codes are like mini power-ups for your computer programs. By the end of this video, you'll know how to create them and make your text analyses even more exciting. Whether you're new or have some experience, this tutorial is here to help you.</p> <p>\ud83c\udf1f Excited to transform text into tiny yet mighty codes? Hit that play button and let's embark on a journey to shrink and visualize our text codes using UMAP. Get ready to add another layer of magic to your text analysis skills! \ud83d\ude80\ud83d\udd0d</p> <p>Resources: Notebook</p>"},{"location":"embeddings/#3-visualization-of-the-embeddings","title":"3. Visualization of the Embeddings","text":"<p>\ud83c\udfa5 Step into the Transformer Embeddings Series - Part 3: Visualizing the Magic!</p> <p>\ud83d\udcda Welcome to the third video of our series! Here, we're taking our text codes on a visual adventure. Remember those smaller codes we created using UMAP? Now, we'll learn how to see them in action and uncover fascinating insights. Get ready for some text magic visualization using a hexbin chart!</p> <p>\ud83d\udca1 Imagine turning these compact codes into pictures that tell us stories \u2013 that's exactly what we're doing. We'll take the tiny text codes from before and transform them into a visual format using a hexbin chart. This chart lets us see patterns and learn new things from our text.</p> <p>\u2699\ufe0f Ready to make your text skills even cooler? These visualizations are like maps that guide us through text worlds. By the end of this video, you'll know how to turn codes into pictures and unveil secrets hidden within the text. Whether you're new to this or have some experience, this tutorial is here to make it fun and understandable.</p> <p>\ud83c\udf1f Excited to turn codes into captivating visuals? Press play and let's dive into the art of visualizing our text codes using a hexbin chart. Get ready to add a splash of creativity to your text analysis journey! \ud83d\ude80\ud83d\udcca</p> <p>Resources: Notebook</p>"},{"location":"llm_eval/","title":"Evaluation of LLMs Is All You Need","text":""},{"location":"llm_eval/#introduction","title":"Introduction","text":"<p>\ud83d\udd0d Unlocking the Unseen: Delve into the world of LLM evaluation, a facet often underestimated yet holding the power to transform your model's prowess! \ud83d\udc8e</p> <p>\ud83d\udd22 Crucial Queries Explored:</p> <ul> <li> <p>Why Evaluate LLMs? Discover the critical need for evaluation in maximizing model potential. Uncover why this step is a cornerstone of success, and how it can take your model from good to extraordinary.</p> </li> <li> <p>What to Evaluate? Explore the multifaceted approach to evaluation. Understand the dimensions that demand attention \u2013 from coherence to context handling \u2013 and how they collectively shape model effectiveness.</p> </li> <li> <p>Where to Evaluate From? Learn about the datasets that fuel evaluation. Explore the diverse sources required to gauge the model's performance across various aspects addressed in the 'What' section.</p> </li> <li> <p>How to Execute Evaluation? Unveil the strategies and techniques to conduct effective evaluation. From methodologies to metrics, this section provides the roadmap to assessing and enhancing model powers</p> </li> </ul> <p>\ud83d\ude80 Elevate Your Model: Elevating your model's performance is just a click away! Through comprehensive evaluation, you can turn setbacks into triumphs, failures into stepping stones towards success. \ud83d\udcc8 This video is your complete A-to-Z guide, providing insights that can reshape your approach to LLMs.</p>"},{"location":"llm_eval/#evaluation","title":"Evaluation","text":"<ul> <li>Model evaluation is the process of analyzing the performance of the model with the help of some metrics</li> <li>Evaluating an LLM performance involves assessing factors such as language fluency, coherence, contextual understanding, factual accuracy, and ability to generate relevant and meaningful responses.</li> </ul>"},{"location":"llm_eval/#what-is-a-good-evaluation","title":"What is a good evaluation?","text":"<ul> <li>Correlated with outcomes: Appropriate metrics used for appropriate models</li> <li>Very less number of metrics, in an ideal world single metric: Easy to track and monitor and make a judgement accordingly</li> <li>Fast and automatic as possible to compute: We can't completely automate the evaluation. It is important to have a human intervention but yet the evaluation should be as automated and fast as possible</li> </ul>"},{"location":"llm_eval/#why-the-conventional-methods-of-evaluation-doesnt-work-for-llms","title":"Why the conventional methods of evaluation doesn't work for LLMs?","text":"<ul> <li> <p>The data used while training and production are always not the same. It can be as different as possible</p> </li> <li> <p>Another key bottleneck is that in LLMs we wont have definitive results. It has a complex generation behavior which is hard to understand. Though the sentence generated would be different from the ground truth the generated sentence will provide the same contextual meaning.</p> </li> <li> <p>For eg:</p> </li> </ul> <pre><code>In Traditional ML, lets consider a scenario of sentiment analysis\n\n    pred = [P, N, P, P]\n    label = [P, N, P, N]\n\nFor the above set to be evaluated we can use metrices like accuracy which here will be 0.75 but that cannot be the case for LLMS\n\nFor LLMs, lets consider a case of summarization of a context given\n\n    pred = Usually LLMs works very well with wide variety   of NLP tasks because they are great generalists by nature\n    label = LLMs are great generalists, so they usually work pretty good with variety of NLP tasks\n\nBoth convey the same meaning if we see it in a contextual way then the model can be given 100% but usually traditional methods are not qualitative but quantitative.\n</code></pre> <p>Thus it is hard to have a conventional metric to quantify the evaluation.</p>"},{"location":"llm_eval/#critical-questions-of-evaluation","title":"Critical Questions of Evaluation","text":"<p>There are four main questions to consider in evaluation. They are:</p> <ul> <li>Why to evaluate?</li> <li>What to evaluate?</li> <li>Where to evaluate?</li> <li>How to evaluate?</li> </ul> <p>Now lets answer each question in detail one by one right now</p>"},{"location":"llm_eval/#why-to-evaluate","title":"Why to evaluate?","text":"<p>Here are some of the reasons why evaluation is very necessary and why it is said to be one of the most underrated aspect of the LLM pipeline</p>"},{"location":"llm_eval/#increase-model-performance","title":"Increase model performance","text":"<p>By evaluating the model one can understand the strengths and weaknesses of a model. Once a models weaknesses are known one can then move onto the next steps to increase the performance working on the weaknesses of the model.</p> <p>For eg: PromptBench indicates a fact that the current LLMs are sensitive to adversarial prompts which implies that you can gain better performance with careful prompt engineering</p>"},{"location":"llm_eval/#better-human-llm-interaction","title":"Better Human-LLM interaction","text":"<p>With better evaluations it can provide better guidance for human-LLMs interaction which can lead to some better experience of the users</p> <p>For eg: Once you know if your model is exhibiting an emotion for a specific way of interaction then a work around could be made to make the interaction better to get the desired output from the model</p>"},{"location":"llm_eval/#safety-and-reliability","title":"Safety and Reliability","text":"<p>LLMs have a broad applicability and are used in various sectors even in some sectors which may require safety and reliability like some financial or healthcare institutions. So it is important to ensure the safety and reliability of the model</p> <p>Thus it is important to have evaluation as one of the most important discipline in the LLM building pipeline</p>"},{"location":"llm_eval/#what-to-evaluate","title":"What to evaluate","text":"<p>Once we find the answer to the question we can claim the strengths and weaknesses of LLMs. the answer to the question is the different tasks which are there to evaluate against the model. Here are the broad categories in which there are the tasks against which the model to be evaluated.</p> <ul> <li>Natural Language Processing</li> <li>Robustness, Ethics, Bias and Trustworthiness</li> <li>Social Science</li> <li>STEM</li> <li>Applications</li> </ul>"},{"location":"llm_eval/#natural-language-processing-tasks","title":"Natural Language Processing Tasks","text":"<p>The main objective behind the development of LLMs was to get enhanced performance in NLP tasks be it understanding or generation</p>"},{"location":"llm_eval/#natural-language-understanding","title":"Natural Language Understanding","text":"<p>It represents wide range of tasks that aims at model having a better understanding of the input provided. Here are the tasks which comes under the umbrella of NLU</p>"},{"location":"llm_eval/#sentiment-analysis","title":"Sentiment Analysis","text":"<p>Sentiment analysis involves analyzing and interpreting the emotion in text. Typically it is binary or triple. LLMS have shown great performance overall but with low resourced language it wasnt able to perform as expected</p>"},{"location":"llm_eval/#text-classification","title":"Text Classification","text":"<p>Though it is as similar as sentiment analysis, it just not focuses on the sentiment it also includes all the processing and different aspects of the text. Again as provided in sentiment analysis the inference holds here as well with great performance but there are some future work to be done with low resourced languages</p>"},{"location":"llm_eval/#natural-language-inference","title":"Natural Language Inference","text":"<p>It is a task of determnining whether the given hypothesis logically follows the given context known as the premise. ChatGPT outperforms others in handling factual input. It is possible due to RLHF training process which helps in favoring to human preferences. But usally LLMS have poorly in case of NLI tasks and there are large improvements needed in this field</p>"},{"location":"llm_eval/#semantic-understanding","title":"Semantic Understanding","text":"<p>It refers to understanding of the language and the concepts associated. It involves the interpretation and comprehension of words, phrases, sentences and the relationship between them thus processing goes beyond the surface level and understanding the underlying meaning and intent. In individual events LLMs possess a great understanding but with multiple events the performance among events has been subpar. With evaluating against basic phrases the LLMs perform poorly. So in general LLMs semantic understanding performance is poor with lot of room for improvment</p>"},{"location":"llm_eval/#reasoning","title":"Reasoning","text":"<p>Reasoning intuitively if we see also involves semantic understanding which has been a significant challenge for models. To effectively handle reasoning tasks the model not only needs to understand the provided information but also utitlixe reasoning based on the understood information. The evaluation can be broadly categorized into mathematical reasoning, commonsense reasoning, logical and domain-specific reasoning. </p> <p>Under these categories overall performance has been satisfactory but with high complexity in mathematical or logical reasoning the model performance has been subpar. Models have shown  good performance with commonsense reasoning but with domain specific reasoning there are lots of room for improvement. So in general reasoning capabilities of model needs to improve a lot</p>"},{"location":"llm_eval/#natural-language-generation","title":"Natural Language Generation","text":"<p>It evaluates the capabilities of LLMs in generating specific texts which consists of several tasks including summarization, dialogue generation, machine translation, QA and open ended generation applications</p>"},{"location":"llm_eval/#summarization","title":"Summarization","text":"<p>It is one of the most popular tasks going right now which aims to learn a concise abstract for the given input sentences. In controllable text summarization LLMs have been more extractive compared to human summaries. On general summarization has been having a general performance in summarization tasks and there are requirements for further improvement</p>"},{"location":"llm_eval/#dialogue","title":"Dialogue","text":"<p>Evaluating the performance of LLMs on dialogue tasks is crucial to the develoipment of dialogue system which acts as the interface for human LLM interaction. On general with good NLU capabilites it has given some good performance but here are some of the challenges for the models in which the models tend to make errors usually: long-term multi-turn conversational dependency, fundamental reasoning failure and hallucination</p>"},{"location":"llm_eval/#translation","title":"Translation","text":"<p>LLMs are explicitly not trained for translation task but still they have shown great performance for translation tasks. On an overall factor LLMs have performed translation very well with a scenario like X -&gt; Eng with English being the target language but in the vice versa scenario it has not performed very well and with low resourced languages it has been more worse</p>"},{"location":"llm_eval/#question-answering","title":"Question Answering","text":"<p>QA is a crucial technology in acting as an interface for human-LLM interaction and it has been found useful in many scenarios like search engines, intelligent customer services, QA systems and may more. Overall LLMs have been nearly flawless with the performance on QA tasks but there are potential improvement in answering questions based on social, event and commonsense knowledge</p>"},{"location":"llm_eval/#open-ended-generation-tasks","title":"Open Ended Generation tasks","text":"<p>There are different generation tasks other than the tasks discussed above in which there are tasks like sentence style transfer, variety of writing tasks such as informative, professional, creative and many more. In general LLMs have shown great proficiency in their writing capabilities</p>"},{"location":"llm_eval/#multilingual-tasks","title":"Multilingual Tasks","text":"<p>LLMs generally have performed poorly when it comes to non-Latin languages like Indic languages and even worse with languages with limited resources. So there is a huge room of improvement for LLMs with multilingual tasks</p>"},{"location":"llm_eval/#factuality","title":"Factuality","text":"<p>Factuality in the context of LLMs refers to the extent to which the text generated by the model align with real world truths and verifiable facts. Thus evaluating factuality is of great importance in order to trust the model and use it. For a model to be factual it should not generate misleading or false information also known as factual hallucination. To evaluate the model on this aspect TruthfulQA can be used as a benchmark whcih is designed to cause models to make mistakes in providing factual answers. the findings implicate that increasing sizes does not make the model truthful for which there is a improvement needed</p>"},{"location":"llm_eval/#robustness-ethic-bias-and-trustworthiness","title":"Robustness, Ethic, Bias and Trustworthiness","text":"<p>Some of the crucial aspects of evaluation of LLMs includes robustness, ethics, bias and trustworthiness all checking its personal characteristics under circumstances</p>"},{"location":"llm_eval/#robustness","title":"Robustness","text":"<p>Robustness studies the stability of the model when facing unexpected inputs such as adversarial prompts and OOD. For evaluating on this aspect there are different benchmarks like AdvGLUE, PromptBench and many more. On an overall aspect the model is said to be prompt sensitive and model generally tends to exhibit subpar performance with adversarial prompts</p>"},{"location":"llm_eval/#ethics-and-bias","title":"Ethics and Bias","text":"<p>LLMs have been found to internailize, spread and potentially magnify harmful information existing in the open domain corpus in which it was pretrained on thus exhibiting some bias and its own ethics. When role playing was introduced to the model it caused biased toxicity to some specific entitites upto 6x. Also LLMs were found to have moral biases and cultural values. All these might result in serious risk after deployment of LLMs into the society</p>"},{"location":"llm_eval/#trustworthiness","title":"Trustworthiness","text":"<p>A model is said to be evaluated for its trustworthiness in the following eight aspects: </p> <ul> <li>toxicity</li> <li>stereotype bias</li> <li>adversarial robustness</li> <li>OOD robustness</li> <li>adversarial demonstration robustness</li> <li>privacy</li> <li>machine ethics</li> <li>fairness</li> </ul> <p>To deploy a model into a real world scenario it is important for the model to be trustworthy</p>"},{"location":"llm_eval/#social-science","title":"Social Science","text":"<p>Social science involves the study of human society and the individual behaviours also including some subjects like economics, sociology, political science, law. Evaluating the performance of of LLMs in social science is important to know its social problem solving ability and applicability of knowledge to such problems. Overall LLMs has benefitted individuals in addressing social science related tasks improving productivity</p>"},{"location":"llm_eval/#stem-science-technology-engineering-and-mathematics","title":"STEM - Science, Technology, Engineering and Mathematics","text":"<p>Evaluation of models in STEM can help in various aspects like personal education, research, etc... to increase productivity. </p>"},{"location":"llm_eval/#math","title":"Math","text":"<p>In mathematics with simple arithmetic tasks it has performed very well but with tasks like trigonometry, logarithm the performance has been subpar finding it challenging. It has been competent handling fractions, decimal numbers, negative and irrational numbers as well but has failed poorly with lengthy complex and challenging mathematical equations and problems. In general the effectivenss of LLMs is highly influenced by the complexity of the problem</p>"},{"location":"llm_eval/#science","title":"Science","text":"<p>In science it has provided some commendable performance with biology and general simple science related tasks but there are improvments needed with chemisty and physics related tasks especially in physics since the model performs worse in physics than in chemistry problems. Thus improvement is needed in this field</p>"},{"location":"llm_eval/#engineering","title":"Engineering","text":"<p>In the order of difficulty the tasks can be ordered as code generation, software engineering and commonsense planning where in the code generation and software engineering aspect LLMs outperform SOTA outputs and even human outputs but improvements are needed in common sense planning showing there is need for improvement in complex engineering tasks</p>"},{"location":"llm_eval/#application","title":"Application","text":""},{"location":"llm_eval/#agent-application","title":"Agent Application","text":"<p>Instead of just using LLMs on general language tasks LLMs have been equipped with tools lately to expand the capabilites of LLM like ToolLM a comprehensive framework to equip llms with tool use capabilites and there are some other models like KOSMOS-1 for general patterns understanding, TALM again for utilization of tools, Toolformer for optimal use of specific APIs and so on and so forth.</p>"},{"location":"llm_eval/#search-and-recommendation","title":"Search and Recommendation","text":"<p>Assessment of LLMS in search and recommendation is broadly categoried into two areas where firstly in information retrieval LLMs have outperformed SOTA models right now and people find it easy and less time consuming to search in ChatGPT than in Google search.</p> <p>With enormous NLP capabilites LLMs have proven to be a way to build recommendation systems comprehending user preferences, item descriptions and any type of contextual informations. But there have been scenarios of unfair recommendations from LLMs like ChatGPT which emphasizes the importance of evaluation of fairness in recommendation</p>"},{"location":"llm_eval/#personality-testing","title":"Personality Testing","text":"<p>It measures the personality traits and behavioral tendencies of LLMs applied in wide range ofg tasks showing that LLMs can perform like humans but still there are limitations in current model to effectively understand and generate humour</p>"},{"location":"llm_eval/#specific-applications","title":"Specific applications","text":"<p>LLMs have been said to have a broad applicability but when they are applied on tasks like log parsing, game designing it showed some limitations with the tasks but with potential to improve</p>"},{"location":"llm_eval/#where-to-evaluate","title":"Where to evaluate","text":"<p>Now that we know the tasks/facets in which the model needs to be evaluated lets move on to the next section which is about the benchmarks for LLM. Here are some of the benchmarks for LLMs</p>"},{"location":"llm_eval/#chatbot-arena","title":"Chatbot Arena","text":"<ul> <li>Chatbot Arena is a platform which is used to compare the performances of diverse chatbot models with anonymous user engagement and voting which shows the preferences of the users in realtime scenarios</li> <li>Thus chatbot arena is a benchmark which provides insights about the strengths and weaknesses of Chatbot models</li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#mt-bench","title":"MT-Bench","text":"<ul> <li> <p>MT-Bench is another benchmark to evaluate the conversational capability of a LLM.</p> </li> <li> <p>It evaluates on multi-turn dialogues using questions which are comprehensive in nature created in order to handle the conversations which replicates real case scenarios showing the closest to models practice performance</p> </li> <li> <p>In simple words it is used to evaluate the model's multi turn conversation capability</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#helm-holistic-evaluation-of-language-models","title":"HELM - Holistic Evaluation of Language Models","text":"<ul> <li> <p>A benchmark as the name suggests provides a comprehensive assessment of LLMs</p> </li> <li> <p>It evaluates the LLMs across 42 scenarios with 59 metrics. Some of the scenarios are QA, IR, Summarization, Reasoning, Coherence, etc... including domain specific knowledge. Some of the metrics are pass@1, rouge, f1, etc...</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#bigbench-beyond-the-imitation-game-benchmark","title":"BIGBench - Beyond the Imitation Game Benchmark","text":"<ul> <li> <p>Bigbench is one of the famous benchmarks going around right now which provides around 204 challeging tasks from 450 authors.</p> </li> <li> <p>It covers various domains like math, biology, commonsense reasoning and many more</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#kola-knowledge-oriented-language-model-evaluation","title":"KOLA - Knowledge Oriented Language Model Evaluation","text":"<ul> <li> <p>KoLA focuses on the comprehension capabilities of the models</p> </li> <li> <p>It is an important benchmark to assess the indepth language understanding and reasoning capabilities of the LLMs</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#dynabench","title":"DynaBench","text":"<ul> <li> <p>A benchmark which supports dynamic benchmarking</p> </li> <li> <p>With Dynabench, it collects human-in-the-loop data dynamically, against the current state-of-the-art, in a way that more accurately measures progress</p> </li> <li> <p>It is a more robust way considering it is a crowd sourced mechanism</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#mmlu-measuring-massive-multitask-language-understanding","title":"MMLU - Measuring Massive Multitask Language Understanding","text":"<ul> <li> <p>MMLU is a benchmark designed to evaluate the models in a zero shot and few shot setting making it more challenging</p> </li> <li> <p>It covers 57 different subjects in STEM, humanities, social science and many more domains</p> </li> <li> <p>It has difficulty levels from elementary to professional where it tests both the world knowledge and problem solving ability accordingly</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#alpacaeval","title":"AlpacaEval","text":"<ul> <li> <p>It is an automated evaluation benchmark of instuction following models which asssesses the performance of the models across NLP tasks with almost 20K annotations</p> </li> <li> <p>It provides an idea on models robustness, diversity and many more capabilites of LLMS in various domains</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#open-llm","title":"Open LLM","text":"<ul> <li> <p>Huggingface Open LLM leaderboard servs an evaluation benchmark by having a public competitive platform </p> </li> <li> <p>It compares and assesses the different models performance on various tasks</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#glue-x","title":"GLUE-X","text":"<ul> <li> <p>It is an attempt to create an unified benchmark consisting of 14 publicly available dataset for evaluating the model performance on OOD scenarios</p> </li> <li> <p>It mainly focuses the robustness of the model under various scenarios</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#promptbench","title":"PromptBench","text":"<ul> <li> <p>PromptBench is an another benchmark which focuses on the OOD and adversarial robustness of th model</p> </li> <li> <p>Ir provides a standardized evaluation framework to compare the different prompt engineeering methods and their impact on model performance thus leading to enhancement of model performance</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#specific-task-benchmarks","title":"SPECIFIC TASK BENCHMARKS","text":"<p>The above mentioned benchmarks mainly focuses on the general language level capabilities of the model. The following table shows for specific task</p> Task Name Task Focus Link SOCKET Social Knowledge of LLMs Link ARB Advanced Reasoning Tasks across multiple domains Link TRUSTGPT Ethical considerations - Toxicity, Bias and Values Alignment in context Link CEval Reasoning Capabilities in Chinese Link M3Exam Comprehensive framework with multiple exams in modalities, languages and levels Link MATH Math Reasoning and Problem solving Link APPS Rigorous benchmark to evaluate LLM's capability to generate python code Link CUAD Legal documents review Link CVALUES LLMs safety and responsibility standards Unable to find <p>Along with the above mentioned benchmarks there are lots of other benchmarks like ANLI, LIT, CoQA, LAMBADA, HellaSwag, LogiQA, MultiNLI and many more. There is a very recent benchmark which came by the name of ToolBench</p>"},{"location":"llm_eval/#toolbench","title":"ToolBench","text":"<ul> <li> <p>A benchmark which is used to evaluate tool-augmented LLMs</p> </li> <li> <p>It has 53 commonly used API tools , 264 dialogues and 568 API calls</p> </li> <li> <p>It focuses on enhancing the LLMs practical application</p> </li> </ul> <p>Benchmark: Link</p>"},{"location":"llm_eval/#how-to-evaluate","title":"How to evaluate","text":"<p>Now that we have the benchmark to evaluate it is time to find the answer for the question of how to evaluate. There are two main ways of evaluation. They are:</p>"},{"location":"llm_eval/#automatic-evaluation","title":"Automatic evaluation","text":"<ul> <li> <p>Automatic evaluation is the most popular method of evaluation which involves using popular metrics or indicators like ROUGE, BLEU, Accuracy, F1-score, etc... due to its very less complexity</p> </li> <li> <p>With NLU and math tasks which is deterministic in nature this evaluation protocol is used</p> </li> <li> <p>Though there are lots of advantages it is not the best way to evaluate due its subjective nature</p> </li> <li> <p>LLM-Eval is a recently proposed method fo automatic evaluation of open domain conversations</p> </li> <li> <p>An another LLM based method is PandaLM which uses an LLM as a judge to evaluate different models</p> </li> </ul>"},{"location":"llm_eval/#human-evaluation","title":"Human evaluation","text":"<ul> <li> <p>The capabilites of LLMs has surpassed the standard performance metrics requiring some human evaluation in cases where automatic evaluation is not suitable. For eg: Open ended generation</p> </li> <li> <p>Usually it is more preferable to have human evaluation for generation tasks</p> </li> <li> <p>It is more accurate, comprehensive and close to the real world scenario. But it is highly complex and costly</p> </li> <li> <p>In a practical scenario, both the evaluation methods are considered and weighed based on the situation - AUTO HUMAN EVAL - THE WAY TO GO</p> </li> </ul>"},{"location":"llm_eval/#success-and-failure-cases-of-llms","title":"Success and Failure Cases of LLMs","text":""},{"location":"llm_eval/#success","title":"Success","text":"<ul> <li>Generation of text with fluency</li> <li>NLU tasks such as classification</li> <li>Robust contextual comprehension</li> <li>Satisfying performance across NLP tasks</li> </ul>"},{"location":"llm_eval/#failure","title":"Failure","text":"<ul> <li>Bias and Inaccuracy while generation</li> <li>Limited comprehending abilities with complex reasoning and logic tasks</li> <li>No dynamic knowledge updataion</li> <li>Prompt sensitive</li> <li>Subpar performance in counterfactual tasks</li> </ul>"},{"location":"llmsrelated/","title":"LLMs Related Playlist","text":"<p>\ud83d\udd25 Unleash the Power of Language Models! \ud83d\udcac\u2728 Dive into the world of cutting-edge AI with our Trending Concepts in LLMs playlist! \ud83d\ude80 From mind-bending implementations to mind-blowing breakthroughs, we've curated the ultimate collection of videos that explore the latest and greatest in Language Model advancements. Whether you're an AI enthusiast, a tech guru, or just curious about the future, join us on a journey through the hottest trends shaping the world of AI-powered communication. Get ready to be amazed, inspired, and future-ready \u2013 press play now and ride the wave of linguistic innovation! </p>"},{"location":"llmsrelated/#videos-available","title":"Videos available","text":"<p>Here are the videos available in the playlist</p> Video Name Link Finetune Llama2 using QLoRA Link Deploy Local LLM like a ChatGPT Chatbot Link DPO Implementation Link Idefics Image Captioner Link CodeLlama Finetuning and CPU Inferencing Link Evaluation of LLMs Is All You Need Link"},{"location":"llmsrelated/#1-finetune-llama2-using-qlora","title":"1. Finetune Llama2 using QLoRA","text":"<p>\ud83d\ude80 LLM Model Finetuning in Minutes: Hands-On Guide with Llama2 - 7b!</p> <p>\ud83d\udcf9 Unlock the secrets of custom dataset finetuning with the recently launched Llama2 - 7b parameters model from Meta AI. Join us in this video tutorial as we navigate through the process using just 50 lines of code, all within the convenience of Google Colab's free environment.</p> <p>\ud83d\udca1 Brace yourself for an exciting journey! Learn how to harness the power of QLoRA to quantize the model down to 4-bit precision. From there, we'll dive into the heart of the tutorial \u2013 finetuning the Llama2 - 7b model on your very own custom dataset.</p> <p>\u2699\ufe0f In a matter of minutes, you'll gain hands-on experience, enabling you to unlock the true potential of this cutting-edge model. Don't miss out on this incredible opportunity to elevate your AI skills. Hit play now and revolutionize your understanding of model customization in just a few simple steps. \ud83c\udf08\ud83e\udd16</p> <p>Resources: Notebook</p>"},{"location":"llmsrelated/#2-deploy-local-llm-like-a-chatgpt-chatbot","title":"2. Deploy Local LLM like a ChatGPT Chatbot","text":"<p>\ud83d\ude80 Create Your Own Local LLM Chatbots: Step-by-Step Deployment Guide!</p> <p>\ud83d\udcf9 Get ready to build and run your very own chatbots using large language models (LLMs) on your own computer! In this video, I'll walk you through every step. We'll use the SantaCoder 1B model to help you write Python programs in a chat-like format.</p> <p>\ud83d\udca1 Watch and learn as I show you how to set up the SantaCoder 1B model as a chatbot. By the end, you'll have a chatbot similar to ChatGPT that can assist you with programming tasks.</p> <p>\u2699\ufe0f But that's not all! You can take it further by deploying your own models. Just by specifying the model's path in your local directory, you can make your very own custom chatbot.</p> <p>\ud83c\udf1f Whether you're new to programming or a tech enthusiast, this video simplifies the process of creating interactive chatbots. With easy-to-follow steps, you'll be able to craft your own chatbot that suits your needs. Ready to get started? Hit play and begin your journey into chatbot creation! \ud83e\udd16\ud83d\ude80</p> <p>Resources: To do</p>"},{"location":"llmsrelated/#3-dpo-direct-preference-optimization-paper-implementation","title":"3. DPO - Direct Preference Optimization Paper Implementation","text":"<p>\ud83d\udd25 Unleash DPO Power: Step-by-Step Implementation Guide using TRL Library on FREE Google Colab! \ud83d\ude80</p> <p>\ud83d\udcda Experience the magic of AI optimization hands-on in this electrifying second installment on DPO \u2013 Direct Preference Optimization! Witness the groundbreaking TRL (Transformer Reinforcement Learning) library by @HuggingFace in action, right here on Google Colab for FREE.</p> <p>\ud83c\udf1f Brace yourself for an immersive journey as we demystify the implementation of DPO. Follow along as I guide you through the entire training pipeline, showcasing each intricate step on the SantaCoder1B model \u2013 all within the accessible realms of Google Colab's free version!</p> <p>\u2699\ufe0f Unlock the secret to DPO implementation with two pivotal steps. Step one: the training of an SFT (Supervised Fine-Tuning) model. Discover how we harness the prowess of the SFT Trainer to skillfully mold the model on the Dahoas/full-hh-rlhf dataset, setting the stage for DPO's transformative power.</p> <p>\u2728 But wait, the journey doesn't stop there! Step two is where the true magic happens. Immerse yourself in the art of creating the DPO model using the prowess of the SFT model, all while enjoying the computational efficiency made possible by the ingenious QLoRA technique, perfectly tailored for Google Colab.</p> <p>\ud83d\ude80 Don't miss this golden opportunity to witness AI optimization unfold, right at your fingertips. Whether you're an experienced coder or an enthusiastic learner, this video is your golden ticket to mastering DPO implementation, revolutionizing the way you enhance your models.</p> <p>\ud83c\udf08 Ready to embark on a journey that merges theory with hands-on practice? Hit that play button and join me in unraveling the marvels of DPO implementation \u2013 all done on the Google Colab platform. Your voyage to becoming an AI implementation virtuoso starts NOW! \ud83c\udf93\ud83e\udd16</p> <p>Resources: SFT NB Train NB Inference NB</p>"},{"location":"llmsrelated/#4-idefics-image-captioning","title":"4. IDEFICS - Image Captioning","text":"<p>\ud83d\udd25 Brace yourselves for a mind-bending journey into the future of AI! Join us in this captivating video as we dive deep into the groundbreaking State-of-The-Art multimodal model - IDEFICS. \ud83c\udf0c</p> <p>What if I told you that there's an open-access gem that rivals Google Deepmind's closely guarded Flamingo model? Enter IDEFICS, a cutting-edge creation from HuggingFace that's set to redefine the AI landscape. \ud83d\ude80</p> <p>From Image Question Answering to Image Captioning and beyond, the possibilities of IDEFICS are limitless. Curious about how to fine-tune this incredible model for image captioning on the MSCOCO dataset? We've got you covered, all within the free Google Colab platform. \ud83d\udcf8\u2728</p> <p>But wait, there's more! We're not stopping at image captioning. Witness firsthand how the versatile IDEFICS can seamlessly adapt to diverse use cases involving both text and images. \ud83d\udcda\ud83d\uddbc\ufe0f</p> <p>Here's the million-dollar question: Is multimodality truly the path forward in AI evolution? Get ready to ponder as we not only unravel the enigma of IDEFICS but also explore the very future of AI itself. \ud83e\udd16\ud83d\udd2e</p> <p>\ud83d\udd14 Don't miss this chance to broaden your horizons and witness the next big leap in AI technology. Click now to embark on a journey that might just reshape your perception of what's possible. Your future in AI begins now. \ud83c\udf10\ud83d\ude80</p> <p>Resources: Notebook</p>"},{"location":"llmsrelated/#5-codellama-finetuning-and-cpu-inferencing","title":"5. CodeLlama Finetuning and CPU Inferencing","text":"<p>\ud83d\ude80 Ready to take your coding game to new heights? \ud83d\udd25 Join me in this thrilling tutorial where we unravel the secrets of finetuning the state-of-the-art coding model, Codellama, using Google Colab's free version! \ud83d\udda5\ufe0f\ud83d\udca1</p> <p>\ud83d\udd0d Dive deep into the art of refining Codellama with the power of QLoRA in this step-by-step guide. \ud83c\udfa8\u2728 Learn how to masterfully fine-tune Codellama using QLoRA, while making the most of the free resources in Google Colab. \ud83d\udcbb\ud83d\udcc8</p> <p>And the excitement doesn't stop there! \ud83c\udf1f As a special treat, I'll walk you through the exhilarating process of performing CPU inference for Llama 7b using GGUF. \ud83e\udd99\ud83d\udca8\ud83d\udcbc Watch the magic unfold as we showcase the potential of your local CPU through a Gradio Chat Interface. \ud83c\udf89\ud83e\udd16</p> <p>\ud83d\udcaf Elevate your coding skills without breaking a sweat! Whether you're a novice or a coding maestro, this video is your golden opportunity. \ud83c\udf9f\ufe0f\ud83d\udd11 Let's conquer Codellama together, revolutionizing your coding experience one step at a time! \ud83c\udf10\ud83d\udcdd</p> <p>\ud83d\udd14 Don't miss out! Remember to like, subscribe, and ring the notification bell to stay updated on the latest coding hacks and tech tutorials! \ud83d\udece\ufe0f\ud83d\udd17 Get ready to embark on this coding journey with us. See you in the video! \ud83d\ude80\ud83d\udc68\u200d\ud83d\udcbb\ud83d\udc69\u200d\ud83d\udcbb</p> <p>Resources: Finetune NB CPU Inference NB</p>"},{"location":"llmsrelated/#6-evaluation-of-llms-is-all-you-need","title":"6. Evaluation of LLMs Is All You Need","text":"<p>\ud83d\udd0d Unlocking the Unseen: Delve into the world of LLM evaluation, a facet often underestimated yet holding the power to transform your model's prowess! \ud83d\udc8e</p> <p>\ud83d\udd22 Crucial Queries Explored:</p> <ul> <li> <p>Why Evaluate LLMs? Discover the critical need for evaluation in maximizing model potential. Uncover why this step is a cornerstone of success, and how it can take your model from good to extraordinary.</p> </li> <li> <p>What to Evaluate? Explore the multifaceted approach to evaluation. Understand the dimensions that demand attention \u2013 from coherence to context handling \u2013 and how they collectively shape model effectiveness.</p> </li> <li> <p>Where to Evaluate From? Learn about the datasets that fuel evaluation. Explore the diverse sources required to gauge the model's performance across various aspects addressed in the 'What' section.</p> </li> <li> <p>How to Execute Evaluation? Unveil the strategies and techniques to conduct effective evaluation. From methodologies to metrics, this section provides the roadmap to assessing and enhancing model powers</p> </li> </ul> <p>\ud83d\ude80 Elevate Your Model: Elevating your model's performance is just a click away! Through comprehensive evaluation, you can turn setbacks into triumphs, failures into stepping stones towards success. \ud83d\udcc8 This video is your complete A-to-Z guide, providing insights that can reshape your approach to LLMs.</p> <p>\ud83c\udfa5 Embrace Transformation: Ready to unlock the full potential of LLM evaluation? Click the link below to watch the video and gain insights that can reshape your approach to LLMs.</p> <p>Resources: Eval Page</p>"},{"location":"mlbootcamp/","title":"ML Bootcamp Playlist","text":"<p>\ud83d\ude80\ud83d\udcca Launch Your Machine Learning Journey: The Ultimate Beginner's Bootcamp! \ud83c\udf1f\ud83d\udd0d Ready to unravel the mysteries of Machine Learning project lifecycle? Welcome to the ML Expedition Bootcamp, a step-by-step guide to mastering every phase from Data Collection to Model Inferencing. \ud83d\udcc8\ud83d\udd2e</p> <p>Embark on this thrilling seven-part adventure, where we demystify complex concepts and break down the entire process into bite-sized, beginner-friendly videos. Each video is a treasure trove of knowledge, designed to empower you with the skills needed to conquer real-world ML challenges. \ud83c\udf93\ud83d\udca1</p> <p>Resources are available in the Google Drive Link</p> <p>Resources: Drive</p>"},{"location":"mlbootcamp/#videos-available-playlists-completed","title":"Videos available - Playlists Completed","text":"<p>Here are the videos available in the playlist</p> Video Name Link Introduction to ML Lifecycle Link EDA Link Missing Values Handling Link Categorical &amp; Outlier Values Handling Link Feature Scaling &amp; Trasnformation Link Feature Selection &amp; Train Test Split &amp; Model Creation Link Model Hyperparameter Tuning Link"},{"location":"mlbootcamp/#1-introduction-to-ml-lifecycle","title":"1. Introduction to ML Lifecycle","text":"<p>\ud83d\ude80 Welcome to Day 1 of Bootcamp! \ud83d\udcda</p> <p>\ud83d\udce2 On this exciting start, we're diving deep into the world of Machine Learning (ML). Here's a sneak peek of what we're covering today:</p> <p>\ud83e\udd16 Introduction to ML: Unveil the basics of Machine Learning and its incredible potential. \ud83c\udf1f Types of ML: Discover the different categories that make up the ML landscape. \ud83d\udd04 Lifecycle of an ML Project: Learn the step-by-step journey of bringing ML projects to life. \ud83d\udcca Data Collection Phase: Understand the crucial starting point of gathering data for ML endeavors. \ud83d\udcda Various Data Collection Methods: Explore the different ways data is gathered to fuel ML. \ud83d\udd0d What is EDA: Dive into the exciting realm of Exploratory Data Analysis, a key process in ML. \ud83d\udcc8 Types of EDA: Uncover the diverse techniques used to analyze data for insights. \u23f0 Don't miss a beat \u2013 make sure to watch the entire session to soak in all these vital concepts. Whether you're new to ML or seeking a refresher, this is your foundation for an incredible journey ahead!</p> <p>\ud83c\udf08 Get ready to dive into the ML universe. Hit play and absorb the knowledge that's about to transform your understanding of Machine Learning! \ud83c\udf93\ud83e\udd16</p>"},{"location":"mlbootcamp/#2-eda-exploratory-data-analysis-and-its-importance","title":"2. EDA - Exploratory Data Analysis and its Importance","text":"<p>\ud83d\ude80 Welcome to Day 2 of Bootcamp! \ud83d\udcda</p> <p>\ud83d\udce2 Get ready for another day of insightful learning! Today, we're immersing ourselves in the captivating world of Exploratory Data Analysis (EDA). Here's a snapshot of what we're diving into:</p> <p>\ud83d\udd0d EDA Unveiled: Explore the concept of Exploratory Data Analysis and its significance in the world of data exploration. \ud83c\udf1f Types of EDA: Discover the diverse techniques and strategies used to uncover hidden insights in your data. \u2728 Importance of EDA: Understand why EDA is the foundation of data-driven decision-making in any project. \ud83d\udd90\ufe0f Hands-on EDA: Roll up your sleeves and dive into a practical session where you'll put EDA techniques into action. \u23f0 Make sure to engage fully and absorb every bit of this session. EDA is a cornerstone in data analysis and today's topics will equip you with the tools to harness its power effectively.</p> <p>\ud83c\udf08 Get ready to elevate your data exploration skills. Hit play and delve into the EDA journey that's bound to expand your analytical horizons! \ud83c\udf93\ud83d\udcca</p>"},{"location":"mlbootcamp/#3-feature-engineering-missing-values-handling","title":"3. Feature Engineering - Missing Values Handling","text":"<p>\ud83d\ude80 Day 3 of Bootcamp is Here! \ud83d\udcda</p> <p>\ud83d\udce2 Get ready for an in-depth exploration into the art of Feature Engineering. Today, we're diving deep into the world of crafting effective features that fuel your data analysis. Here's a sneak peek of what we're covering:</p> <p>\ud83d\udd0d Feature Engineering Unveiled: Discover the power of creating insightful features that drive data understanding. \ud83c\udf1f Dive into Missing Value Handling: Explore techniques to tackle missing data effectively, ensuring your insights are accurate and comprehensive.</p> <p>\u23f0 Engage fully to grasp the nuances of feature engineering, starting with the crucial aspect of handling missing values. This is your chance to equip yourself with tools that make your data shine.</p> <p>\ud83c\udf08 Embark on the journey of mastering feature engineering. Hit play and let's unravel the techniques that transform data into actionable insights! \ud83c\udf93\ud83d\udcc8</p>"},{"location":"mlbootcamp/#4-feature-engineering-categorical-outlier-values-handling","title":"4. Feature Engineering - Categorical &amp; Outlier Values Handling","text":"<p>\ud83d\ude80 Day 4 of Bootcamp: Feature Engineering Deep Dive! \ud83d\udcda</p> <p>\ud83d\udce2 Brace yourself for another enriching session on Feature Engineering. Today, we're delving further into the world of crafting impactful features for your data analysis. Here's a glimpse of what's on the agenda:</p> <p>\ud83d\udd0d Unlocking Categorical Features: Explore strategies to effectively handle categorical data, turning them into valuable insights. \ud83c\udf1f Conquering Outliers: Dive into techniques that help identify and manage outliers in your dataset, ensuring accurate and reliable results.</p> <p>\u23f0 Fully engage with the session to grasp the intricacies of feature engineering, as we tackle handling categorical features and the crucial task of outlier management. These skills will empower your data exploration journey.</p> <p>\ud83c\udf08 Get ready to elevate your feature engineering prowess. Hit play and embark on a journey to master the techniques that turn raw data into actionable insights! \ud83c\udf93\ud83d\udcca</p>"},{"location":"mlbootcamp/#5-feature-scaling-and-transformation","title":"5. Feature Scaling and Transformation","text":"<p>\ud83d\ude80 Day 5 of Bootcamp: Navigating Feature Scaling and Transformation! \ud83d\udcda</p> <p>\ud83d\udce2 Get ready to embark on the final steps of Data Processing in this enlightening session. Today, we're delving into the crucial topics of Feature Scaling and Transformation. Here's a sneak peek into what's in store:</p> <p>\ud83d\udd0d Mastering Feature Scaling: Explore techniques to ensure your features are at the right scale, paving the way for accurate analysis. \ud83c\udf1f Unveiling Feature Transformation: Delve into the art of transforming features to enhance their distribution and usefulness for modeling.</p> <p>\u23f0 Dive deep into this session to understand the essential role of feature scaling and transformation. These steps are the finishing touches that refine your data for optimal analysis and model building.</p> <p>\ud83c\udf08 Ready to elevate your data processing skills? Hit play and join us as we wrap up our journey through data processing, setting the stage for more advanced concepts ahead! \ud83c\udf93\ud83d\udd27</p>"},{"location":"mlbootcamp/#6-feature-selection-train-test-split-model-creation","title":"6. Feature Selection &amp; Train Test Split &amp; Model Creation","text":"<p>\ud83d\ude80 Day 6 of Bootcamp: Feature Selection, ML Models, and Performance Metrics Unveiled! \ud83d\udcda</p> <p>\ud83d\udce2 Brace yourself for a power-packed session as we dive into advanced topics. On this sixth day of Bootcamp, we're delving deep into Feature Selection, ML Model Creation, and Performance Metrics. Here's a glimpse of what awaits:</p> <p>\ud83d\udd0d Mastering Feature Selection: Discover techniques to choose the most relevant features, refining your data for optimal analysis. \ud83c\udf1f Building ML Models: Embark on a journey to create powerful Machine Learning models that transform data into insights. \ud83d\udcca Unveiling Performance Metrics: Learn how to gauge the effectiveness of your models using performance metrics, ensuring accuracy and reliability. \ud83c\udfaf Selecting the Best ML Model: Understand the art of choosing the perfect ML model based on performance metrics, aligning your model with the problem at hand.</p> <p>\u23f0 Dive deep into this comprehensive session to grasp the intricacies of feature selection, model creation, and performance assessment. These skills will empower you to transform data into actionable insights.</p> <p>\ud83c\udf08 Ready to take your data skills to the next level? Hit play and join us as we explore the world of feature selection, ML models, and performance metrics \u2013 paving the way for advanced data analysis! \ud83c\udf93\ud83d\udd0d\ud83d\udd27</p>"},{"location":"mlbootcamp/#7-model-hyperparameter-tuning","title":"7. Model Hyperparameter Tuning","text":"<p>\ud83d\ude80 Day 7 of Bootcamp: Rainfall Prediction Project, Hyperparameter Tuning, and Model Deployment Intro! \ud83d\udcda</p> <p>\ud83d\udce2 Get ready for an exciting culmination as we embark on an end-to-end project! On this seventh day of Bootcamp, we're diving into Rainfall Prediction, Hyperparameter Tuning, and the basics of Model Deployment. Here's what's in store:</p> <p>\ud83c\udf27\ufe0f Rainfall Prediction Project: Immerse yourself in a comprehensive project that puts your skills to the test, predicting rainfall using your newfound knowledge. \ud83d\udd27 Hyperparameter Tuning: Explore the art of fine-tuning model settings for optimal performance, unlocking the full potential of your models. \ud83d\ude80 Model Deployment Introduction: Get a sneak peek into the world of deploying your trained models, taking your projects beyond the realm of development.</p> <p>\u23f0 Engage fully in this session to experience the thrill of a real-world project, fine-tuning models, and getting a glimpse of model deployment. These skills are your stepping stones to becoming a data science pro.</p> <p>\ud83c\udf08 Ready to bring it all together? Hit play and join us as we round off Bootcamp with a dynamic project, hyperparameter tuning, and a glimpse into model deployment \u2013 marking the start of your journey into advanced data analysis! \ud83c\udf93\ud83c\udf26\ufe0f\ud83d\udd0d\ud83d\udd27\ud83d\ude80</p>"},{"location":"nlpprojs/","title":"NLP Projects Playlist","text":"<p>\ud83d\ude80\ud83d\udcca Unleash the NLP Powerhouse: Transformers in Action! \ud83e\udd16\ud83d\udd2e Ready to dive into the world of End-to-End NLP Projects? Welcome to the ultimate playlist that covers every conceivable task, every magic moment, and every mind-boggling application achievable with Transformers in the realm of Natural Language Processing. \ud83c\udf10\u2728</p> <p>Get hands-on with the future of language technology as we guide you through building captivating projects from scratch. Whether you're dreaming of sentiment analysis, text generation, question answering, language translation, or any other awe-inspiring NLP challenge, we've got you covered. \ud83d\udca1\ud83d\udcac</p> <p>Our videos take you step by step, code by code, concept by concept, as you craft these cutting-edge projects. Whether you're an AI aficionado, a coding champ, or just eager to explore the frontiers of technology, this playlist is your gateway to becoming an NLP wizard. \ud83c\udfa9\ud83d\udd25</p> <p>So roll up your sleeves, fire up your IDE, and embark on a journey that's limited only by your imagination. The world of NLP awaits your transformational touch \u2013 hit play and let's turn those NLP dreams into reality! \ud83c\udf1f</p>"},{"location":"nlpprojs/#videos-available","title":"Videos available","text":"<p>Here are the videos available in the playlist</p> Video Name Link Chatbot using FlanT5 and LoRA Link Text Classifier using DistilBERT Link Cricket Match Report Generation based on Live Commentary Data Link Keyword Extractor using BERT(NER) Link"},{"location":"nlpprojs/#1-create-your-own-chatbot-using-flant5-and-lora","title":"1. Create your own Chatbot using FlanT5 and LoRA","text":"<p>\ud83d\udcf9 We explore the exciting world of building your own chatbot using LoRA (Low Rank Adaptation) on the robust Flan T5 large (770M) model, all on Colab. But that's not all \u2013 we also delved into the possibilities of enhancing and adapting this bot with alternative models.</p> <p>\ud83d\udca1 Step into the realm of AI implementation as we break down the process of creating your personalized chatbot. With LoRA, you'll discover how to optimize the Flan T5 large model for dynamic conversations. Stay tuned for insights on elevating your chatbot's capabilities.</p> <p>\u2699\ufe0f Looking to take your chatbot to the next level? Our discussion covered strategies to fine-tune and customize your bot by integrating various models to cater to your specific needs.</p> <p>\ud83c\udf1f Relive the excitement! Catch up now to learn, engage, and gain insights into constructing a dynamic chatbot using LoRA with the Flan T5 large model. Your exploration into the world of AI-driven conversations starts here! \ud83e\udd16\ud83c\udf93</p> <p>Resources: Data Notebook Chatbot Notebook</p>"},{"location":"nlpprojs/#2-text-classifier-using-distilbert","title":"2. Text Classifier using DistilBERT","text":"<p>\ud83c\udfac Learn to Create Your Own Text Classifier with DistilBERT: Solving Any Text Classification Challenge!</p> <p>\ud83d\udcda Ever wondered how to tackle diverse text classification problems effortlessly? Look no further! In this video, I walk you through crafting your very own text classifier template using DistilBERT. Whether it's binary or multiclass classification, this template has you covered.</p> <p>\ud83d\udca1 Dive into the world of AI as we unravel the process of building an adaptable text classifier. Witness how we employ DistilBERT's power to address the complexities of various text classification tasks. To illustrate, we've chosen stock sentiment analysis as a captivating use case.</p> <p>\u2699\ufe0f Get ready to gain hands-on experience. With this template, you'll be equipped to conquer a wide range of text classification challenges. No matter your level of expertise, whether you're a newcomer or a seasoned enthusiast, this video serves as your gateway to mastering text classification with DistilBERT.</p> <p>\ud83c\udf1f Ready to unlock the potential of text analysis? Hit that play button to learn, experiment, and build your arsenal of AI tools. Dive into the exciting world of text classification and bring a new dimension to your problem-solving skills! \ud83d\ude80\ud83d\udcca</p> <p>Resources: Notebook</p>"},{"location":"nlpprojs/#3-cricket-match-report-generation-based-on-match-live-commentary","title":"3. Cricket Match Report Generation based on Match Live Commentary","text":"<p>\ud83c\udfa5 Join me on an exciting exploration as we unravel the art of generating match reports from live commentary data, all with the incredible might of AI! In this video, we're diving deep into the world of cricket, using the formidable Flant5 Large model and innovative Lora techniques to transform real-time commentary into captivating post-match narratives.</p> <p>\ud83c\udf10 Watch as I guide you seamlessly through the process, from collecting data to model inferencing. Discover the magic of turning dynamic play-by-play updates into rich, coherent stories using a Streamlit application that brings it all to life. \u26a1\ud83d\udcf0</p> <p>\ud83c\udfdf\ufe0f Whether you're a die-hard cricket fan or a tech enthusiast intrigued by the fusion of AI and sports, this video promises to be a game-changer. Witness the evolution of raw data into engaging narratives as we bridge the gap between the excitement on the field and the captivating stories that follow.</p> <p>\ud83d\ude80\ud83d\udd25 Don't miss out on this thrilling journey where technology meets sports journalism. Step into the future of match reporting with us and experience firsthand the power of AI-driven storytelling. Let's transform cricket commentary into compelling narratives \u2013 hit play and let the magic unfold! \ud83c\udf99\ufe0f\ud83c\udfc6</p> <p>Resources: Data NB Model NB</p>"},{"location":"nlpprojs/#4-keyword-extraction-using-bertner","title":"4. Keyword Extraction using BERT(NER)","text":"<p>\ud83c\udfa5 Easy Guide: Train BERT for Finding Keywords with NER!</p> <p>\ud83d\udcdd Ever wanted to pick out important words from sentences? Join me in this video where I'll show you how to use BERT for that! We'll focus on something called Named Entity Recognition (NER) to find these keywords. Plus, I'll teach you how to do this not only with ready-made datasets but also with your own custom ones.</p> <p>\ud83d\udd0d Discover the magic of AI as we dive into this keyword-finding technique. With BERT, you can make sense of sentences and spot key words. From using existing datasets to your own unique data, I'll walk you through every step in a friendly way.</p> <p>\ud83d\udd27 Ready to give it a try? By the end of this video, you'll have the skills to find important words like a pro. Whether you're new to this or already know a bit, this tutorial will help you confidently work with keywords using BERT.</p> <p>\ud83c\udf1f Ready to turn sentences into goldmines of information? Just hit play and let's learn, explore, and unlock the power of BERT for keyword extraction. Boost your skills and become a keyword-finding champ! \ud83d\ude80\ud83d\udd11</p> <p>Resources: BERT NER Custom BERT NER</p>"},{"location":"nlpresearchpapers/","title":"NLP Research Papers Playlist","text":"<p>Unleash the Power of Language Models! \ud83d\udcac\u2728 Dive into the world of cutting-edge AI with our Trending Concepts in LLMs playlist! \ud83d\ude80 From mind-bending implementations to mind-blowing breakthroughs, we've curated the ultimate collection of videos that explore the latest and greatest in Language Model advancements. Whether you're an AI enthusiast, a tech guru, or just curious about the future, join us on a journey through the hottest trends shaping the world of AI-powered communication. Get ready to be amazed, inspired, and future-ready \u2013 press play now and ride the wave of linguistic innovation! </p>"},{"location":"nlpresearchpapers/#videos-available","title":"Videos available","text":"<p>Here are the videos available in the playlist</p> Video Name Link LoRA Explanation and Implementation Link DPO Explanation Link DPO Implementation Link Platypus Explanation and Implementation Link"},{"location":"nlpresearchpapers/#1-lora-low-rank-adaptation-of-llm-paper-explanation-and-implementation","title":"1. LoRA - Low Rank Adaptation of LLM Paper Explanation and Implementation","text":"<p>\ud83d\udd0d Explore the Cutting-Edge LoRA Paper: Revolutionizing Model Optimization for Experts! \ud83d\udcd6</p> <p>\ud83d\udcda Are you ready to unravel the groundbreaking LoRA method that's sending shockwaves through the research world? \ud83c\udf1f In this video, I'll take you on an in-depth journey through the LoRA paper, where you'll grasp every intricate detail of this game-changing technique.</p> <p>\ud83c\udfaf Imagine boosting your model's performance to new heights with an ingenious approach! The LoRA technique introduces the concept of utilizing low-rank weight decomposition matrices. But that's not all \u2013 brace yourself for a paradigm shift as we delve into the method that achieves unparalleled performance. \ud83d\ude80</p> <p>\ud83d\udd25 Say goodbye to the days of laborious updates to the entire pre-trained model. LoRA's brilliance lies in its ability to update only the weights of these specialized matrices. \ud83d\udcca This translates to lightning-fast computations and remarkable efficiency, propelling your finetuning of Large Language Models (LLMs) into a new era.</p> <p>\ud83c\udf10 The LoRA technique isn't just a hidden gem \u2013 it's now a cornerstone of modern research. \ud83d\udce2 Countless studies have harnessed its power, making it a must-know tool in every researcher's arsenal. Whether you're a seasoned pro or an aspiring academic, embracing LoRA could be the key to unlocking your projects' true potential.</p> <p>Resources: Slide Data Notebook Chatbot Notebook</p>"},{"location":"nlpresearchpapers/#2-dpo-direct-preference-optimization-paper-explanation","title":"2. DPO - Direct Preference Optimization Paper Explanation","text":"<p>\ud83d\ude80 Uncover the Future of Model Optimization: A Deep Dive into the DPO Paper! \ud83d\udcda</p> <p>\ud83d\udd25 Get ready to explore the cutting-edge world of AI optimization in this comprehensive video breakdown of the DPO paper. This revolutionary method presents a compelling alternative to RLHF (Reinforcement Learning with Human Feedback)</p> <p>\ud83c\udf1f Unlike traditional approaches, the DPO technique operates by computing the log probabilities of preferred and dispreferred outcomes under a model's guidance. But here's where it gets truly exciting \u2013 the method strategically optimizes model parameters. How? By elevating the likelihood of preferred responses while decreasing the occurrences of dispreferred ones. The result? A model fine-tuned to align with human preferences in an unprecedented way!</p> <p>\ud83d\udd11 Say farewell to the complexity of conventional RLHF algorithms, because the DPO method operates without the need for a reward model. Instead, it harnesses the power of calculated log probabilities to reshape the model's behavior based on human choices.</p> <p>\u2699\ufe0f This video is your ticket to understanding the groundbreaking DPO paper, whether you're an AI enthusiast aiming to stay on the cutting edge or a curious mind seeking to grasp the future of model optimization. By hitting that play button, you're stepping into the forefront of AI evolution.</p> <p>\ud83c\udf08 Don't miss this opportunity to be a part of the AI revolution. Watch now, expand your horizons, and gain insights that could redefine your approach to enhancing models. Your journey towards mastering model optimization starts here! \ud83c\udf93\ud83e\udd16</p> <p>Resources: Slides</p>"},{"location":"nlpresearchpapers/#3-dpo-direct-preference-optimization-paper-implementation","title":"3. DPO - Direct Preference Optimization Paper Implementation","text":"<p>\ud83d\udd25 Unleash DPO Power: Step-by-Step Implementation Guide using TRL Library on FREE Google Colab! \ud83d\ude80</p> <p>\ud83d\udcda Experience the magic of AI optimization hands-on in this electrifying second installment on DPO \u2013 Direct Preference Optimization! Witness the groundbreaking TRL (Transformer Reinforcement Learning) library by @HuggingFace in action, right here on Google Colab for FREE.</p> <p>\ud83c\udf1f Brace yourself for an immersive journey as we demystify the implementation of DPO. Follow along as I guide you through the entire training pipeline, showcasing each intricate step on the SantaCoder1B model \u2013 all within the accessible realms of Google Colab's free version!</p> <p>\u2699\ufe0f Unlock the secret to DPO implementation with two pivotal steps. Step one: the training of an SFT (Supervised Fine-Tuning) model. Discover how we harness the prowess of the SFT Trainer to skillfully mold the model on the Dahoas/full-hh-rlhf dataset, setting the stage for DPO's transformative power.</p> <p>\u2728 But wait, the journey doesn't stop there! Step two is where the true magic happens. Immerse yourself in the art of creating the DPO model using the prowess of the SFT model, all while enjoying the computational efficiency made possible by the ingenious QLoRA technique, perfectly tailored for Google Colab.</p> <p>\ud83d\ude80 Don't miss this golden opportunity to witness AI optimization unfold, right at your fingertips. Whether you're an experienced coder or an enthusiastic learner, this video is your golden ticket to mastering DPO implementation, revolutionizing the way you enhance your models.</p> <p>\ud83c\udf08 Ready to embark on a journey that merges theory with hands-on practice? Hit that play button and join me in unraveling the marvels of DPO implementation \u2013 all done on the Google Colab platform. Your voyage to becoming an AI implementation virtuoso starts NOW! \ud83c\udf93\ud83e\udd16</p> <p>Resources: SFT NB Train NB Inference NB</p>"},{"location":"nlpresearchpapers/#4-platypus-cheap-quick-way-to-refine-llms-paper-explanation-and-implementation","title":"4. Platypus - Cheap, Quick way to Refine LLMs Paper Explanation and Implementation","text":"<p>\ud83d\udcda Delve into the Intricacies of the Platypus Paper: A Cheap and Quick way to refine LLM Models!</p> <p>\ud83d\udd25 Uncover the secrets behind the groundbreaking Platypus paper in this comprehensive video breakdown. This family of finely-tuned and merged Large Language Models (LLMs) takes the lead on Huggingface's OpenLLM Leaderboard.</p> <p>\ud83d\udca1 The video dissects three key contributions. First, witness the meticulous curation of the Open Platypus dataset. Next, delve into the intricacies of the innovative Finetuning and Merging procedure, spotlighting the ingenious LoRA modules. The Platypus Llama2 model serves as the foundational base, setting the stage for the model's prowess.</p> <p>\ud83d\ude80 The final contribution reveals the step-by-step procedure of Contamination Check and the vital process of removing contaminated data.</p> <p>\ud83c\udf1f Whether you're a language model enthusiast or a research aficionado, this video is your gateway to understanding the monumental Platypus advancements in LLM models. Hit play now to unlock a world of cutting-edge insights! \ud83c\udf08\ud83d\udcd6</p> <p>Resources: Slides Platypus Train Notebook</p>"},{"location":"res/embeddings/Transformer_Embedding/","title":"Sentence Transformer Embeddings","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install transformers==4.15.0 sentence-transformers datasets\n</pre> ! pip install transformers==4.15.0 sentence-transformers datasets <pre>Requirement already satisfied: transformers==4.15.0 in /usr/local/lib/python3.7/dist-packages (4.15.0)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.0.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (2.23.0)\nRequirement already satisfied: tokenizers&lt;0.11,&gt;=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (0.10.3)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (6.0)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (1.21.5)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (0.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (2019.12.20)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (21.3)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (4.11.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (3.6.0)\nRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (0.0.49)\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (4.63.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.1.0-&gt;transformers==4.15.0) (3.10.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;transformers==4.15.0) (3.0.7)\nRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\nRequirement already satisfied: torch&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\nRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\nRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\nRequirement already satisfied: fsspec[http]&gt;=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\nRequirement already satisfied: responses&lt;0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: pyarrow&gt;=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.15.0) (2021.10.8)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.15.0) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.15.0) (2.10)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers==4.15.0) (3.0.4)\nRequirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (0.13.0)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (21.4.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (6.0.2)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (1.3.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (1.2.0)\nRequirement already satisfied: charset-normalizer&lt;3.0,&gt;=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (2.0.12)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (1.7.2)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp-&gt;datasets) (4.0.2)\nRequirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers==4.15.0) (3.7.0)\nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk-&gt;sentence-transformers) (1.15.0)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;datasets) (2.8.2)\nRequirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas-&gt;datasets) (2018.9)\nRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.15.0) (7.1.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers==4.15.0) (1.1.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.0,&gt;=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision-&gt;sentence-transformers) (7.1.2)\n</pre> In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\ndata = load_dataset('emotion', split='train')\n</pre> from datasets import load_dataset data = load_dataset('emotion', split='train') <pre>Using custom data configuration default\nReusing dataset emotion (/root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n</pre> In\u00a0[\u00a0]: Copied! <pre>data[\"label\"]\n</pre> data[\"label\"] Out[\u00a0]: <pre>[0,\n 0,\n 3,\n 2,\n 3,\n 0,\n 5,\n 4,\n 1,\n 2,\n 0,\n 1,\n 3,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 4,\n 3,\n 4,\n 1,\n 1,\n 3,\n 0,\n 0,\n 0,\n 3,\n 1,\n 1,\n 4,\n 5,\n 3,\n 1,\n 1,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 1,\n 2,\n 1,\n 3,\n 1,\n 0,\n 3,\n 4,\n 1,\n 0,\n 0,\n 5,\n 1,\n 1,\n 1,\n 2,\n 4,\n 4,\n 5,\n 3,\n 3,\n 0,\n 2,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 3,\n 0,\n 3,\n 3,\n 3,\n 1,\n 1,\n 1,\n 1,\n 0,\n 4,\n 2,\n 3,\n 0,\n 3,\n 2,\n 0,\n 1,\n 1,\n 0,\n 3,\n 2,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 1,\n 2,\n 0,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 4,\n 4,\n 4,\n 0,\n 2,\n 1,\n 1,\n 2,\n 4,\n 5,\n 1,\n 1,\n 1,\n 1,\n 3,\n 4,\n 1,\n 3,\n 2,\n 3,\n 0,\n 1,\n 0,\n 3,\n 1,\n 5,\n 0,\n 3,\n 3,\n 0,\n 1,\n 4,\n 1,\n 1,\n 4,\n 0,\n 5,\n 5,\n 1,\n 3,\n 4,\n 3,\n 0,\n 3,\n 0,\n 4,\n 0,\n 1,\n 5,\n 4,\n 1,\n 3,\n 1,\n 3,\n 1,\n 4,\n 4,\n 0,\n 1,\n 1,\n 0,\n 5,\n 1,\n 4,\n 1,\n 0,\n 1,\n 1,\n 1,\n 4,\n 1,\n 5,\n 1,\n 3,\n 0,\n 0,\n 1,\n 3,\n 0,\n 1,\n 1,\n 5,\n 1,\n 4,\n 1,\n 4,\n 0,\n 4,\n 2,\n 0,\n 4,\n 2,\n 0,\n 0,\n 3,\n 1,\n 2,\n 3,\n 0,\n 5,\n 3,\n 1,\n 0,\n 3,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 1,\n 2,\n 1,\n 0,\n 3,\n 5,\n 1,\n 3,\n 1,\n 2,\n 0,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 4,\n 0,\n 3,\n 0,\n 3,\n 2,\n 1,\n 2,\n 0,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 3,\n 2,\n 0,\n 2,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 4,\n 1,\n 0,\n 0,\n 1,\n 2,\n 0,\n 3,\n 0,\n 2,\n 1,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 0,\n 2,\n 4,\n 0,\n 4,\n 1,\n 1,\n 4,\n 1,\n 3,\n 3,\n 2,\n 0,\n 5,\n 1,\n 3,\n 0,\n 0,\n 3,\n 2,\n 5,\n 0,\n 2,\n 1,\n 3,\n 1,\n 0,\n 0,\n 1,\n 1,\n 4,\n 0,\n 3,\n 1,\n 2,\n 1,\n 1,\n 1,\n 1,\n 1,\n 2,\n 1,\n 4,\n 4,\n 1,\n 1,\n 3,\n 1,\n 1,\n 1,\n 0,\n 5,\n 1,\n 1,\n 0,\n 5,\n 1,\n 4,\n 1,\n 3,\n 2,\n 3,\n 4,\n 5,\n 1,\n 3,\n 0,\n 0,\n 0,\n 4,\n 1,\n 4,\n 1,\n 3,\n 0,\n 2,\n 3,\n 0,\n 2,\n 2,\n 1,\n 0,\n 1,\n 1,\n 3,\n 1,\n 2,\n 1,\n 1,\n 0,\n 5,\n 1,\n 1,\n 0,\n 5,\n 3,\n 1,\n 1,\n 2,\n 1,\n 2,\n 2,\n 1,\n 0,\n 1,\n 0,\n 4,\n 3,\n 0,\n 3,\n 1,\n 4,\n 5,\n 2,\n 0,\n 0,\n 1,\n 5,\n 2,\n 0,\n 0,\n 1,\n 4,\n 1,\n 1,\n 1,\n 2,\n 4,\n 0,\n 0,\n 4,\n 2,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 0,\n 1,\n 1,\n 3,\n 0,\n 4,\n 0,\n 0,\n 3,\n 5,\n 5,\n 1,\n 5,\n 4,\n 2,\n 3,\n 1,\n 4,\n 0,\n 3,\n 0,\n 1,\n 4,\n 1,\n 1,\n 1,\n 1,\n 5,\n 3,\n 0,\n 1,\n 0,\n 3,\n 2,\n 0,\n 2,\n 3,\n 2,\n 2,\n 1,\n 1,\n 0,\n 1,\n 0,\n 1,\n 3,\n 1,\n 4,\n 0,\n 1,\n 0,\n 5,\n 1,\n 1,\n 2,\n 1,\n 0,\n 3,\n 1,\n 0,\n 0,\n 0,\n 1,\n 1,\n 1,\n 0,\n 0,\n 3,\n 3,\n 1,\n 0,\n 5,\n 1,\n 1,\n 0,\n 3,\n 1,\n 3,\n 1,\n 1,\n 2,\n 3,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 1,\n 4,\n 1,\n 4,\n 0,\n 3,\n 3,\n 0,\n 2,\n 0,\n 0,\n 0,\n 5,\n 4,\n 0,\n 0,\n 1,\n 4,\n 1,\n 0,\n 0,\n 3,\n 0,\n 3,\n 3,\n 2,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 2,\n 2,\n 2,\n 1,\n 5,\n 4,\n 0,\n 3,\n 0,\n 4,\n 1,\n 3,\n 1,\n 1,\n 0,\n 1,\n 4,\n 1,\n 3,\n 0,\n 3,\n 1,\n 2,\n 2,\n 0,\n 4,\n 0,\n 0,\n 0,\n 4,\n 0,\n 3,\n 1,\n 1,\n 0,\n 4,\n 4,\n 3,\n 3,\n 0,\n 0,\n 3,\n 3,\n 1,\n 4,\n 0,\n 0,\n 4,\n 1,\n 4,\n 0,\n 0,\n 1,\n 0,\n 3,\n 0,\n 2,\n 5,\n 1,\n 1,\n 1,\n 1,\n 1,\n 0,\n 0,\n 2,\n 4,\n 3,\n 1,\n 2,\n 1,\n 1,\n 4,\n 3,\n 4,\n 0,\n 3,\n 3,\n 3,\n 4,\n 1,\n 2,\n 3,\n 4,\n 3,\n 1,\n 4,\n 0,\n 0,\n 3,\n 1,\n 3,\n 4,\n 4,\n 5,\n 0,\n 1,\n 1,\n 1,\n 2,\n 0,\n 1,\n 0,\n 3,\n 1,\n 1,\n 4,\n 4,\n 1,\n 4,\n 3,\n 3,\n 3,\n 1,\n 0,\n 1,\n 1,\n 1,\n 3,\n 4,\n 3,\n 1,\n 0,\n 1,\n 4,\n 1,\n 0,\n 3,\n 0,\n 0,\n 1,\n 1,\n 0,\n 2,\n 1,\n 0,\n 3,\n 2,\n 0,\n 1,\n 1,\n 4,\n 3,\n 1,\n 1,\n 2,\n 5,\n 4,\n 4,\n 3,\n 3,\n 2,\n 0,\n 0,\n 1,\n 2,\n 3,\n 1,\n 3,\n 1,\n 1,\n 1,\n 4,\n 4,\n 2,\n 0,\n 2,\n 1,\n 1,\n 0,\n 0,\n 3,\n 3,\n 3,\n 3,\n 0,\n 1,\n 0,\n 1,\n 4,\n 0,\n 1,\n 0,\n 1,\n 5,\n 4,\n 1,\n 1,\n 4,\n 4,\n 0,\n 0,\n 3,\n 0,\n 2,\n 1,\n 5,\n 1,\n 5,\n 1,\n 4,\n 1,\n 1,\n 1,\n 1,\n 5,\n 2,\n 0,\n 2,\n 1,\n 0,\n 1,\n 1,\n 0,\n 1,\n 1,\n 1,\n 1,\n 4,\n 4,\n 4,\n 4,\n 3,\n 2,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 4,\n 1,\n 3,\n 1,\n 1,\n 4,\n 3,\n 1,\n 3,\n 3,\n 3,\n 3,\n 3,\n 1,\n 4,\n 0,\n 2,\n 0,\n 4,\n 2,\n 1,\n 5,\n 1,\n 1,\n 1,\n 1,\n 0,\n 5,\n 1,\n 3,\n 3,\n 3,\n 1,\n 1,\n 0,\n 2,\n 0,\n 1,\n 3,\n 3,\n 3,\n 4,\n 1,\n 1,\n 4,\n 0,\n 5,\n 1,\n 2,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 3,\n 1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 5,\n 1,\n 3,\n 2,\n 1,\n 3,\n 0,\n 1,\n 3,\n 0,\n 4,\n 4,\n 0,\n 0,\n 4,\n 0,\n 2,\n 4,\n 1,\n 1,\n 4,\n 1,\n 3,\n 1,\n 1,\n 3,\n 1,\n 1,\n 2,\n 3,\n 1,\n 1,\n 1,\n 3,\n 1,\n 2,\n 4,\n 3,\n 2,\n 0,\n 4,\n 0,\n 0,\n 2,\n 1,\n 4,\n 2,\n 1,\n 1,\n 0,\n 1,\n 0,\n 4,\n 1,\n 0,\n 2,\n 0,\n 2,\n 0,\n 0,\n 3,\n 2,\n 3,\n 1,\n 3,\n 0,\n 0,\n 1,\n 1,\n 3,\n 0,\n 5,\n 1,\n 2,\n 0,\n 3,\n 4,\n 2,\n 1,\n 1,\n 3,\n 1,\n 0,\n 1,\n 1,\n 1,\n 2,\n 1,\n 0,\n 2,\n 4,\n 0,\n 1,\n 0,\n 3,\n 1,\n 1,\n 0,\n 1,\n 2,\n 1,\n 1,\n 1,\n 3,\n 4,\n 0,\n 1,\n 0,\n 0,\n 2,\n 3,\n 1,\n 2,\n 0,\n 1,\n 4,\n 0,\n 1,\n 1,\n 0,\n 0,\n 2,\n 1,\n 0,\n 0,\n 0,\n 0,\n 3,\n 0,\n 1,\n 0,\n 1,\n 0,\n 2,\n 3,\n 1,\n 4,\n 0,\n 0,\n 1,\n 1,\n 3,\n 4,\n 2,\n 1,\n 0,\n 3,\n 0,\n 1,\n ...]</pre> In\u00a0[\u00a0]: Copied! <pre>data.set_format(\"pandas\")\n</pre> data.set_format(\"pandas\") In\u00a0[\u00a0]: Copied! <pre>df = data[:]\ndf.head()\n</pre> df = data[:] df.head() Out[\u00a0]: text label 0 i didnt feel humiliated 0 1 i can go from feeling so hopeless to so damned... 0 2 im grabbing a minute to post i feel greedy wrong 3 3 i am ever feeling nostalgic about the fireplac... 2 4 i am feeling grouchy 3 In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nimport numpy as np\n</pre> import pandas as pd from sentence_transformers import SentenceTransformer from tqdm import tqdm import numpy as np In\u00a0[\u00a0]: Copied! <pre>def embedding_generator(df_path, model):\n  tqdm.pandas()\n  model = SentenceTransformer(model)\n  df['Embeddings'] = df['text'].progress_apply(lambda x: model.encode(x))\n  return df\n</pre> def embedding_generator(df_path, model):   tqdm.pandas()   model = SentenceTransformer(model)   df['Embeddings'] = df['text'].progress_apply(lambda x: model.encode(x))   return df In\u00a0[\u00a0]: Copied! <pre>def inputgen(df):\n  a = []\n  for i in tqdm(range(len(df))):\n    a.append(np.array(df[\"Embeddings\"][i]))\n  a = np.array(a)\n  print(a.shape)\n  return a\n</pre> def inputgen(df):   a = []   for i in tqdm(range(len(df))):     a.append(np.array(df[\"Embeddings\"][i]))   a = np.array(a)   print(a.shape)   return a In\u00a0[\u00a0]: Copied! <pre>df = df[0:2000]\n</pre> df = df[0:2000] In\u00a0[\u00a0]: Copied! <pre>model1 = \"sentence-transformers/all-MiniLM-L6-v2\" \ninputgen_input = embedding_generator(df, model1)\nprint(\"Model 1 embedding completed\")\nembed_output = inputgen(inputgen_input)\nprint(\"Model 1 array completed\")\n</pre> model1 = \"sentence-transformers/all-MiniLM-L6-v2\"  inputgen_input = embedding_generator(df, model1) print(\"Model 1 embedding completed\") embed_output = inputgen(inputgen_input) print(\"Model 1 array completed\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:13&lt;00:00, 149.65it/s]\n</pre> <pre>Model 1 embedding completed\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 69706.98it/s]</pre> <pre>(2000, 384)\nModel 1 array completed\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>embed = pd.DataFrame(embed_output)\ndf = pd.concat([df, embed], axis=1)\ndf\n</pre> embed = pd.DataFrame(embed_output) df = pd.concat([df, embed], axis=1) df Out[\u00a0]: text label Embeddings 0 1 2 3 4 5 6 ... 374 375 376 377 378 379 380 381 382 383 0 i didnt feel humiliated 0 [-0.055050932, -0.007696963, 0.06353026, -0.03... -0.055051 -0.007697 0.063530 -0.039664 0.116901 -0.123296 0.058080 ... 0.063319 -0.044138 -0.034640 0.021249 -0.029084 0.084679 0.016152 0.015425 -0.135161 -0.064534 1 i can go from feeling so hopeless to so damned... 0 [0.009238882, -0.05296433, 0.01926256, 0.03402... 0.009239 -0.052964 0.019263 0.034021 0.125202 0.027428 0.077058 ... -0.016320 -0.024402 -0.044897 0.132352 -0.082222 0.003469 0.095559 -0.060182 -0.027176 -0.026275 2 im grabbing a minute to post i feel greedy wrong 3 [-0.074502885, -0.010641884, -0.0034595674, -0... -0.074503 -0.010642 -0.003460 -0.073246 -0.018509 -0.026024 0.023559 ... 0.050347 -0.030673 -0.001018 0.019752 0.078385 -0.010269 0.041514 -0.024779 -0.042020 0.024512 3 i am ever feeling nostalgic about the fireplac... 2 [0.10859442, 0.09532223, 0.03647684, 0.0151784... 0.108594 0.095322 0.036477 0.015178 0.089073 -0.012647 -0.089686 ... 0.019334 -0.076964 -0.004122 0.023587 0.056529 0.024166 0.103731 -0.044091 -0.109329 0.034851 4 i am feeling grouchy 3 [-0.016712204, -0.07877089, 0.032170087, -0.05... -0.016712 -0.078771 0.032170 -0.053829 0.115593 -0.051190 0.132093 ... -0.011990 0.003192 -0.077645 -0.016146 0.007182 0.029738 0.059137 -0.062703 -0.019559 -0.057704 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1995 i feel so low and i havent felt this low in a ... 0 [0.00746265, -0.072956935, 0.06638428, 0.05247... 0.007463 -0.072957 0.066384 0.052478 0.011782 -0.002765 0.015952 ... -0.046977 -0.010428 -0.048182 0.081015 -0.056379 -0.043684 0.059703 -0.061770 -0.076507 -0.000722 1996 i absolutely love this skinny fiber it is doin... 1 [-0.05117633, -0.04654245, 0.042185836, 0.0601... -0.051176 -0.046542 0.042186 0.060135 0.005652 -0.068989 0.031247 ... -0.010567 -0.019804 -0.038921 0.050018 -0.011048 0.008017 0.041045 -0.010521 0.005083 -0.002920 1997 i feel as if im in some strange catholic vortex 5 [0.0092844125, -0.094246, 0.026456287, -0.0027... 0.009284 -0.094246 0.026456 -0.002791 0.047796 -0.070760 0.011053 ... 0.087301 -0.005810 0.040688 0.020890 -0.022804 -0.063676 0.061230 -0.020718 -0.048927 -0.092958 1998 i have a feeling that many of you will be surp... 5 [0.11245156, -0.03292613, 0.11352201, -0.03423... 0.112452 -0.032926 0.113522 -0.034240 0.062835 0.008078 0.031408 ... 0.096730 0.014149 -0.011893 0.077310 -0.106346 0.016515 0.078990 0.053615 -0.086291 -0.006568 1999 i am so connected with families that are not m... 1 [-0.03639206, -0.10061437, -0.0018109059, 0.02... -0.036392 -0.100614 -0.001811 0.025786 -0.028809 -0.010301 0.012143 ... 0.034679 -0.006586 0.019008 0.090166 0.052975 0.014769 0.087924 0.023971 -0.021222 -0.063778 <p>2000 rows \u00d7 387 columns</p> In\u00a0[\u00a0]: Copied! <pre>! pip install umap-learn\n</pre> ! pip install umap-learn <pre>Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (0.5.2)\nRequirement already satisfied: scipy&gt;=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.63.0)\nRequirement already satisfied: numba&gt;=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\nRequirement already satisfied: scikit-learn&gt;=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\nRequirement already satisfied: pynndescent&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.5.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.49-&gt;umap-learn) (57.4.0)\nRequirement already satisfied: llvmlite&lt;0.35,&gt;=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba&gt;=0.49-&gt;umap-learn) (0.34.0)\nRequirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent&gt;=0.5-&gt;umap-learn) (1.1.0)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.22-&gt;umap-learn) (3.1.0)\n</pre> In\u00a0[\u00a0]: Copied! <pre>from umap import UMAP\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nimport numpy as np\n</pre> from umap import UMAP import pandas as pd from sklearn.preprocessing import MinMaxScaler import pandas as pd from sentence_transformers import SentenceTransformer from tqdm import tqdm import numpy as np In\u00a0[\u00a0]: Copied! <pre>def embedding_generator(df_path, model):\n  tqdm.pandas()\n  model = SentenceTransformer(model)\n  df['Embeddings'] = df['text'].progress_apply(lambda x: model.encode(x))\n  return df\n</pre> def embedding_generator(df_path, model):   tqdm.pandas()   model = SentenceTransformer(model)   df['Embeddings'] = df['text'].progress_apply(lambda x: model.encode(x))   return df In\u00a0[\u00a0]: Copied! <pre>def inputgen(df):\n  a = []\n  for i in tqdm(range(len(df))):\n    a.append(np.array(df[\"Embeddings\"][i]))\n  a = np.array(a)\n  print(a.shape)\n  return a\n</pre> def inputgen(df):   a = []   for i in tqdm(range(len(df))):     a.append(np.array(df[\"Embeddings\"][i]))   a = np.array(a)   print(a.shape)   return a In\u00a0[\u00a0]: Copied! <pre>def dimensionality_reduction(embed_arr, label):\n  X_scaled = MinMaxScaler().fit_transform(embed_arr)\n  mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n  df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n  df_emb[\"label\"] = label\n  print(df_emb)\n  return df_emb\n</pre> def dimensionality_reduction(embed_arr, label):   X_scaled = MinMaxScaler().fit_transform(embed_arr)   mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)   df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])   df_emb[\"label\"] = label   print(df_emb)   return df_emb In\u00a0[\u00a0]: Copied! <pre>model1 = \"sentence-transformers/all-MiniLM-L6-v2\" \ninputgen_input = embedding_generator(df, model1)\nprint(\"Embedding completed\")\nembed_output = inputgen(inputgen_input)\nprint(\"Array completed\")\ndim_emb_out = dimensionality_reduction(embed_output, df[\"label\"])\nprint(\"UMAP completed\")\n</pre> model1 = \"sentence-transformers/all-MiniLM-L6-v2\"  inputgen_input = embedding_generator(df, model1) print(\"Embedding completed\") embed_output = inputgen(inputgen_input) print(\"Array completed\") dim_emb_out = dimensionality_reduction(embed_output, df[\"label\"]) print(\"UMAP completed\") <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:14&lt;00:00, 137.54it/s]\n</pre> <pre>Embedding completed\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2000/2000 [00:00&lt;00:00, 71517.18it/s]\n</pre> <pre>(2000, 384)\nArray completed\n             X          Y  label\n0     4.410917  10.360144      0\n1     7.095046  11.037721      0\n2     6.351001   9.905983      3\n3     8.679171   9.845695      2\n4     5.573699  12.767012      3\n...        ...        ...    ...\n1995  6.500954  11.825342      0\n1996  9.314243  11.284079      1\n1997  6.900813   9.577901      5\n1998  8.642849   9.790328      5\n1999  7.501454   8.454765      1\n\n[2000 rows x 3 columns]\nUMAP completed\n</pre> In\u00a0[\u00a0]: Copied! <pre>dim_emb_out\n</pre> dim_emb_out Out[\u00a0]: X Y label 0 4.410917 10.360144 0 1 7.095046 11.037721 0 2 6.351001 9.905983 3 3 8.679171 9.845695 2 4 5.573699 12.767012 3 ... ... ... ... 1995 6.500954 11.825342 0 1996 9.314243 11.284079 1 1997 6.900813 9.577901 5 1998 8.642849 9.790328 5 1999 7.501454 8.454765 1 <p>2000 rows \u00d7 3 columns</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n</pre> import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>fig, axes = plt.subplots(2, 3, figsize=(7,5))\naxes = axes.flatten()\ncmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\nlabels =df[\"label\"].unique()\nfor i, (label, cmap) in enumerate(zip(labels, cmaps)):\n    df_emb_sub = dim_emb_out.query(f\"label == {i}\")\n    print(df_emb_sub)\n    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n                   gridsize=10, linewidths=(0,))\n    axes[i].set_title(label)\n    axes[i].set_xticks([]), axes[i].set_yticks([])\n\nplt.tight_layout()\nplt.show()\n</pre> fig, axes = plt.subplots(2, 3, figsize=(7,5)) axes = axes.flatten() cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"] labels =df[\"label\"].unique() for i, (label, cmap) in enumerate(zip(labels, cmaps)):     df_emb_sub = dim_emb_out.query(f\"label == {i}\")     print(df_emb_sub)     axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,                    gridsize=10, linewidths=(0,))     axes[i].set_title(label)     axes[i].set_xticks([]), axes[i].set_yticks([])  plt.tight_layout() plt.show() <pre>             X          Y  label\n0     4.410917  10.360144      0\n1     7.095046  11.037721      0\n5     6.884747  12.149796      0\n10    5.573589  10.928417      0\n13    7.363647  12.668382      0\n...        ...        ...    ...\n1983  7.338718   7.762202      0\n1987  6.602554   9.052503      0\n1989  6.382240  10.995190      0\n1992  7.367487  11.797047      0\n1995  6.500954  11.825342      0\n\n[544 rows x 3 columns]\n             X          Y  label\n8     7.892040  10.033929      1\n11    7.005034   9.483533      1\n14    9.480061   8.485753      1\n15    6.398749  12.128724      1\n22    6.625608   7.409938      1\n...        ...        ...    ...\n1988  8.608582  11.570669      1\n1991  9.189944   9.329336      1\n1993  6.085050   9.369851      1\n1996  9.314243  11.284079      1\n1999  7.501454   8.454765      1\n\n[702 rows x 3 columns]\n             X          Y  label\n3     8.679171   9.845695      2\n9     5.858622   8.135866      2\n47    7.230072   8.507003      2\n61    5.531404  12.353902      2\n68    6.735010   9.334861      2\n...        ...        ...    ...\n1972  5.786281  10.251306      2\n1973  8.701107   9.829437      2\n1978  4.189478   8.249639      2\n1984  7.403680   9.066971      2\n1985  5.714351  12.618199      2\n\n[173 rows x 3 columns]\n             X          Y  label\n2     6.351001   9.905983      3\n4     5.573699  12.767012      3\n12    8.151361  10.702692      3\n20    4.643733   9.135000      3\n24    7.602103  12.617702      3\n...        ...        ...    ...\n1942  4.633220  11.428572      3\n1946  8.804513  12.250484      3\n1953  4.024027  10.380529      3\n1963  9.191159  11.068044      3\n1986  4.087564  11.096752      3\n\n[268 rows x 3 columns]\n             X          Y  label\n7     5.388288   9.851950      4\n19    6.385650  10.144875      4\n21    8.048517  11.486029      4\n31    6.078864  11.757133      4\n53    8.348764   8.977859      4\n...        ...        ...    ...\n1955  7.125514   7.996155      4\n1965  7.390118   9.450252      4\n1975  4.810911   9.063455      4\n1990  5.460900  11.757572      4\n1994  7.669730  12.339661      4\n\n[236 rows x 3 columns]\n             X          Y  label\n6     6.616123  13.090185      5\n32    8.302124  10.460528      5\n57    7.081882   8.609643      5\n64    7.512016  12.824598      5\n129   7.710279   7.798317      5\n...        ...        ...    ...\n1928  5.311825   7.922052      5\n1945  7.558575   7.223899      5\n1954  7.007526  11.439436      5\n1997  6.900813   9.577901      5\n1998  8.642849   9.790328      5\n\n[77 rows x 3 columns]\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/embeddings/Transformer_Embedding/#sentence-transformer-embeddings","title":"Sentence Transformer Embeddings\u00b6","text":""},{"location":"res/embeddings/Transformer_Embedding/#embeddings-dimensionality-reduction","title":"Embeddings Dimensionality Reduction\u00b6","text":""},{"location":"res/embeddings/Transformer_Embedding/#visualization-of-embeddings","title":"Visualization of Embeddings\u00b6","text":""},{"location":"res/llmsrelated/001-finetune-llama2/Finetune_LLamA/","title":"Finetune LLamA","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.30.2 trl==0.4.7\n</pre> ! pip install accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.30.2 trl==0.4.7 In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom trl import SFTTrainer\n\ndef finetune_llama_v2():\n    data = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n    tokenizer.pad_token = tokenizer.eos_token\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        \"meta-llama/Llama-2-7b-hf\", quantization_config=bnb_config, device_map={\"\": 0}\n    )\n    model.config.use_cache=False\n    model.config.pretraining_tp=1\n    peft_config = LoraConfig(\n        r=64, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n    )\n    training_arguments = TrainingArguments(\n        output_dir=\"llama2_finetuned_chatbot\",\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_8bit\",\n        learning_rate=2e-4,\n        lr_scheduler_type=\"linear\",\n        save_strategy=\"epoch\",\n        logging_steps=10,\n        num_train_epochs=1,\n        max_steps=10,\n        fp16=True,\n        push_to_hub=True\n    )\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=data,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        args=training_arguments,\n        tokenizer=tokenizer,\n        packing=False,\n        max_seq_length=512\n    )\n    trainer.train()\n    trainer.push_to_hub()\n\nif __name__ == \"__main__\":\n    finetune_llama_v2()\n</pre> import torch from datasets import load_dataset from peft import LoraConfig from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments from trl import SFTTrainer  def finetune_llama_v2():     data = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")     tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")     tokenizer.pad_token = tokenizer.eos_token     bnb_config = BitsAndBytesConfig(         load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True     )     model = AutoModelForCausalLM.from_pretrained(         \"meta-llama/Llama-2-7b-hf\", quantization_config=bnb_config, device_map={\"\": 0}     )     model.config.use_cache=False     model.config.pretraining_tp=1     peft_config = LoraConfig(         r=64, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"     )     training_arguments = TrainingArguments(         output_dir=\"llama2_finetuned_chatbot\",         per_device_train_batch_size=8,         gradient_accumulation_steps=4,         optim=\"paged_adamw_8bit\",         learning_rate=2e-4,         lr_scheduler_type=\"linear\",         save_strategy=\"epoch\",         logging_steps=10,         num_train_epochs=1,         max_steps=10,         fp16=True,         push_to_hub=True     )     trainer = SFTTrainer(         model=model,         train_dataset=data,         peft_config=peft_config,         dataset_text_field=\"text\",         args=training_arguments,         tokenizer=tokenizer,         packing=False,         max_seq_length=512     )     trainer.train()     trainer.push_to_hub()  if __name__ == \"__main__\":     finetune_llama_v2() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/llmsrelated/004-idefics/notes/","title":"IDEFICS Image Captioning","text":"In\u00a0[1]: Copied! <pre>!pip install -q datasets\n!pip install -q git+https://github.com/huggingface/transformers.git@add-model-idefics\n!pip install -q bitsandbytes sentencepiece accelerate loralib\n!pip install -q -U git+https://github.com/huggingface/peft.git\n</pre> !pip install -q datasets !pip install -q git+https://github.com/huggingface/transformers.git@add-model-idefics !pip install -q bitsandbytes sentencepiece accelerate loralib !pip install -q -U git+https://github.com/huggingface/peft.git <pre>     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 519.3/519.3 kB 8.1 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 115.3/115.3 kB 13.3 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.1/194.1 kB 20.3 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.8/134.8 kB 15.7 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 268.8/268.8 kB 31.4 MB/s eta 0:00:00\n  WARNING: Did not find branch or tag 'add-model-idefics', assuming revision or ref.\nerror: subprocess-exited-with-error\n  \n  \u00d7 git checkout -q add-model-idefics did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500&gt; See above for output.\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: subprocess-exited-with-error\n\n\u00d7 git checkout -q add-model-idefics did not run successfully.\n\u2502 exit code: 1\n\u2570\u2500&gt; See above for output.\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 92.6/92.6 MB 9.1 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 67.6 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 251.2/251.2 kB 28.4 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.5/7.5 MB 15.8 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 32.4 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.8/7.8 MB 46.3 MB/s eta 0:00:00\n  Building wheel for peft (pyproject.toml) ... done\n</pre> In\u00a0[1]: Copied! <pre>from datasets import load_dataset\nfrom peft import LoraConfig, get_peft_model\nfrom PIL import Image\nfrom transformers import IdeficsForVisionText2Text, AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig\nfrom torchvision import transforms as transforms\nimport torch\n</pre> from datasets import load_dataset from peft import LoraConfig, get_peft_model from PIL import Image from transformers import IdeficsForVisionText2Text, AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig from torchvision import transforms as transforms import torch In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[3]: Copied! <pre>class CocoCaptionDataset:\n\n    def __init__(self):\n        self.model_ckpt = \"HuggingFaceM4/idefics-9b-instruct\"\n        self.processor = AutoProcessor.from_pretrained(self.model_ckpt, use_auth_token=True)\n\n    def load_data(self):\n        data = load_dataset(\"cat-state/mscoco-1st-caption\")\n        data = data[\"train\"].train_test_split(test_size=0.0002)\n        train_data = data[\"train\"]\n        val_data = data[\"test\"]\n        return train_data, val_data\n\n    def img_convert_to_rgb(self, image):\n        if image.mode == \"RGB\":\n            return image\n        image_rgba = image.convert(\"RGBA\")\n        background = Image.new(\"RGBA\", image_rgba.size, (255, 255, 255))\n        alpha_composite = Image.alpha_composite(background, image_rgba)\n        alpha_composite = alpha_composite.convert(\"RGB\")\n        return alpha_composite\n\n    def transform_data(self, example):\n        img_size = self.processor.image_processor.image_size\n        img_mean = self.processor.image_processor.image_mean\n        img_std = self.processor.image_processor.image_std\n\n        img_transform = transforms.Compose([\n            self.img_convert_to_rgb,\n            transforms.RandomResizedCrop((img_size, img_size), scale=(0.9, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=img_mean, std=img_std),\n        ])\n\n        prompts = []\n        for i in range(len(example['caption'])):\n            caption = example['caption'][i]\n            prompts.append(\n                [\n                    example['url'][i],\n                    f\"Question: Explain the picture. Answer: {caption}&lt;/s&gt;\",\n                ],\n            )\n\n        inputs = self.processor(prompts, transform=img_transform, return_tensors=\"pt\").to(\"cuda\")\n        inputs[\"labels\"] = inputs[\"input_ids\"]\n\n        return inputs\n\n    def gen_data(self):\n        train_dataset, val_dataset = self.load_data()\n        train_dataset.set_transform(self.transform_data)\n        val_dataset.set_transform(self.transform_data)\n        return train_dataset, val_dataset\n</pre> class CocoCaptionDataset:      def __init__(self):         self.model_ckpt = \"HuggingFaceM4/idefics-9b-instruct\"         self.processor = AutoProcessor.from_pretrained(self.model_ckpt, use_auth_token=True)      def load_data(self):         data = load_dataset(\"cat-state/mscoco-1st-caption\")         data = data[\"train\"].train_test_split(test_size=0.0002)         train_data = data[\"train\"]         val_data = data[\"test\"]         return train_data, val_data      def img_convert_to_rgb(self, image):         if image.mode == \"RGB\":             return image         image_rgba = image.convert(\"RGBA\")         background = Image.new(\"RGBA\", image_rgba.size, (255, 255, 255))         alpha_composite = Image.alpha_composite(background, image_rgba)         alpha_composite = alpha_composite.convert(\"RGB\")         return alpha_composite      def transform_data(self, example):         img_size = self.processor.image_processor.image_size         img_mean = self.processor.image_processor.image_mean         img_std = self.processor.image_processor.image_std          img_transform = transforms.Compose([             self.img_convert_to_rgb,             transforms.RandomResizedCrop((img_size, img_size), scale=(0.9, 1.0), interpolation=transforms.InterpolationMode.BICUBIC),             transforms.ToTensor(),             transforms.Normalize(mean=img_mean, std=img_std),         ])          prompts = []         for i in range(len(example['caption'])):             caption = example['caption'][i]             prompts.append(                 [                     example['url'][i],                     f\"Question: Explain the picture. Answer: {caption}\",                 ],             )          inputs = self.processor(prompts, transform=img_transform, return_tensors=\"pt\").to(\"cuda\")         inputs[\"labels\"] = inputs[\"input_ids\"]          return inputs      def gen_data(self):         train_dataset, val_dataset = self.load_data()         train_dataset.set_transform(self.transform_data)         val_dataset.set_transform(self.transform_data)         return train_dataset, val_dataset  In\u00a0[4]: Copied! <pre>class ImageCaptioningModel:\n\n    def __init__(self):\n        cococaptiondataset = CocoCaptionDataset()\n        self.train_data, self.val_data = cococaptiondataset.gen_data()\n        self.model_ckpt = \"HuggingFaceM4/idefics-9b-instruct\"\n\n    def load_model(self):\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n            llm_int8_skip_modules=[\"lm_head\", \"embed_tokens\"],\n        )\n\n        model = IdeficsForVisionText2Text.from_pretrained(self.model_ckpt, quantization_config=bnb_config, device_map=\"auto\")\n        return model\n\n    def create_lora_model(self, model):\n        config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n        )\n        lora_model = get_peft_model(model, config)\n        lora_model.print_trainable_parameters()\n        return lora_model\n\n    def set_training_args(self):\n        training_args = TrainingArguments(\n            output_dir=\"idefics-mscoco-captioner\",\n            learning_rate=2e-4,\n            fp16=True,\n            per_device_train_batch_size=2,\n            per_device_eval_batch_size=2,\n            gradient_accumulation_steps=8,\n            dataloader_pin_memory=False,\n            save_total_limit=1,\n            evaluation_strategy=\"steps\",\n            save_strategy=\"steps\",\n            save_steps=50,\n            eval_steps=50,\n            logging_steps=50,\n            max_steps=100,\n            remove_unused_columns=False,\n            load_best_model_at_end=True,\n            optim=\"paged_adamw_8bit\",\n            label_names=[\"labels\"]\n        )\n        return training_args\n\n\n    def train_and_push_to_hub(self):\n        img_cap_model = self.load_model()\n        img_cap_model = self.create_lora_model(img_cap_model)\n        trainer = Trainer(\n            model = img_cap_model,\n            args = self.set_training_args(),\n            train_dataset = self.train_data,\n            eval_dataset = self.val_data\n        )\n        trainer.train()\n        img_cap_model.push_to_hub(\"idefics-mscoco-captioner\", private=False)\n</pre> class ImageCaptioningModel:      def __init__(self):         cococaptiondataset = CocoCaptionDataset()         self.train_data, self.val_data = cococaptiondataset.gen_data()         self.model_ckpt = \"HuggingFaceM4/idefics-9b-instruct\"      def load_model(self):         bnb_config = BitsAndBytesConfig(             load_in_4bit=True,             bnb_4bit_use_double_quant=True,             bnb_4bit_quant_type=\"nf4\",             bnb_4bit_compute_dtype=torch.float16,             llm_int8_skip_modules=[\"lm_head\", \"embed_tokens\"],         )          model = IdeficsForVisionText2Text.from_pretrained(self.model_ckpt, quantization_config=bnb_config, device_map=\"auto\")         return model      def create_lora_model(self, model):         config = LoraConfig(             r=16,             lora_alpha=32,             target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],             lora_dropout=0.05,             bias=\"none\",         )         lora_model = get_peft_model(model, config)         lora_model.print_trainable_parameters()         return lora_model      def set_training_args(self):         training_args = TrainingArguments(             output_dir=\"idefics-mscoco-captioner\",             learning_rate=2e-4,             fp16=True,             per_device_train_batch_size=2,             per_device_eval_batch_size=2,             gradient_accumulation_steps=8,             dataloader_pin_memory=False,             save_total_limit=1,             evaluation_strategy=\"steps\",             save_strategy=\"steps\",             save_steps=50,             eval_steps=50,             logging_steps=50,             max_steps=100,             remove_unused_columns=False,             load_best_model_at_end=True,             optim=\"paged_adamw_8bit\",             label_names=[\"labels\"]         )         return training_args       def train_and_push_to_hub(self):         img_cap_model = self.load_model()         img_cap_model = self.create_lora_model(img_cap_model)         trainer = Trainer(             model = img_cap_model,             args = self.set_training_args(),             train_dataset = self.train_data,             eval_dataset = self.val_data         )         trainer.train()         img_cap_model.push_to_hub(\"idefics-mscoco-captioner\", private=False) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    imagecaptioningmodel = ImageCaptioningModel()\n    imagecaptioningmodel.train_and_push_to_hub()\n</pre> if __name__ == \"__main__\":     imagecaptioningmodel = ImageCaptioningModel()     imagecaptioningmodel.train_and_push_to_hub() In\u00a0[6]: Copied! <pre>def check_inference(model, processor, prompts, max_new_tokens=50):\n    tokenizer = processor.tokenizer\n    bad_words = [\"&lt;image&gt;\", \"&lt;fake_token_around_image&gt;\"]\n    if len(bad_words) &gt; 0:\n        bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids\n\n    eos_token = \"&lt;/s&gt;\"\n    eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n\n    inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")\n    generated_ids = model.generate(**inputs, eos_token_id=[eos_token_id], bad_words_ids=bad_words_ids, max_new_tokens=max_new_tokens, early_stopping=True)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    print(generated_text)\n</pre> def check_inference(model, processor, prompts, max_new_tokens=50):     tokenizer = processor.tokenizer     bad_words = [\"\", \"\"]     if len(bad_words) &gt; 0:         bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids      eos_token = \"\"     eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)      inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")     generated_ids = model.generate(**inputs, eos_token_id=[eos_token_id], bad_words_ids=bad_words_ids, max_new_tokens=max_new_tokens, early_stopping=True)     generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]     print(generated_text) In\u00a0[\u00a0]: Copied! <pre>bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n            llm_int8_skip_modules=[\"lm_head\", \"embed_tokens\"],\n)\n\nmodel = IdeficsForVisionText2Text.from_pretrained(\"HuggingFaceM4/idefics-9b-instruct\", quantization_config=bnb_config, device_map=\"auto\")\n</pre> bnb_config = BitsAndBytesConfig(             load_in_4bit=True,             bnb_4bit_use_double_quant=True,             bnb_4bit_quant_type=\"nf4\",             bnb_4bit_compute_dtype=torch.float16,             llm_int8_skip_modules=[\"lm_head\", \"embed_tokens\"], )  model = IdeficsForVisionText2Text.from_pretrained(\"HuggingFaceM4/idefics-9b-instruct\", quantization_config=bnb_config, device_map=\"auto\") In\u00a0[3]: Copied! <pre>processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics-9b-instruct\", use_auth_token=True)\n</pre> processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics-9b-instruct\", use_auth_token=True) <pre>/usr/local/lib/python3.10/dist-packages/transformers/models/auto/processing_auto.py:203: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n</pre> In\u00a0[4]: Copied! <pre>from peft import PeftModel, PeftConfig\nconfig = PeftConfig.from_pretrained(\"/content/idefics-mscoco-captioner\")\nmodel = PeftModel.from_pretrained(model,\"/content/idefics-mscoco-captioner\")\n</pre> from peft import PeftModel, PeftConfig config = PeftConfig.from_pretrained(\"/content/idefics-mscoco-captioner\") model = PeftModel.from_pretrained(model,\"/content/idefics-mscoco-captioner\") In\u00a0[7]: Copied! <pre>url = \"https://cdn.pixabay.com/photo/2018/01/14/23/12/nature-3082832_1280.jpg\"\nprompts = [\n    url,\n    \"Question: Explain the picture. Answer:\",\n]\ncheck_inference(model, processor, prompts, max_new_tokens=50)\n</pre> url = \"https://cdn.pixabay.com/photo/2018/01/14/23/12/nature-3082832_1280.jpg\" prompts = [     url,     \"Question: Explain the picture. Answer:\", ] check_inference(model, processor, prompts, max_new_tokens=50) <pre>/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams&gt;1` or unset `early_stopping`.\n  warnings.warn(\n</pre> <pre>Question: Explain the picture. Answer: A mountain lake with lightning striking the mountain in the background.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/llmsrelated/004-idefics/notes/#idefics-image-captioning","title":"IDEFICS Image Captioning\u00b6","text":""},{"location":"res/llmsrelated/004-idefics/notes/#installing-dependencies","title":"Installing Dependencies\u00b6","text":""},{"location":"res/llmsrelated/004-idefics/notes/#importing-dependencies","title":"Importing dependencies\u00b6","text":""},{"location":"res/llmsrelated/004-idefics/notes/#dataset","title":"Dataset\u00b6","text":""},{"location":"res/llmsrelated/004-idefics/notes/#model-training","title":"Model Training\u00b6","text":""},{"location":"res/llmsrelated/004-idefics/notes/#inference","title":"Inference\u00b6","text":""},{"location":"res/llmsrelated/004-idefics/notes/#sample-image","title":"Sample Image:\u00b6","text":"<p>Let's run prediction with the quantized model for the image below which pictures two kittens. \\ </p>"},{"location":"res/llmsrelated/005-codellama/Finetune_CodeLLamA_with_inference/","title":"Model Training","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install accelerate peft bitsandbytes transformers trl\n</pre> ! pip install accelerate peft bitsandbytes transformers trl In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom datasets import load_dataset, Dataset\nfrom peft import LoraConfig, AutoPeftModelForCausalLM\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom trl import SFTTrainer\nimport os\n\ndef finetune_llama_v2():\n    data = load_dataset(\"code_x_glue_tc_nl_code_search_adv\", split=\"validation\")\n    data_df = data.to_pandas()\n    data_df[\"text\"] = data_df[[\"docstring\", \"code\"]].apply(lambda x: \"&lt;s&gt;[INST] Docstring: \" + x[\"docstring\"] + \" [/INST] Code: \" + x[\"code\"] + \"&lt;/s&gt;\", axis=1)\n    data = Dataset.from_pandas(data_df)\n    tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n    tokenizer.pad_token = tokenizer.eos_token\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        \"codellama/CodeLlama-7b-hf\", quantization_config=bnb_config, device_map=\"auto\"\n    )\n    model.config.use_cache=False\n    model.config.pretraining_tp=1\n    peft_config = LoraConfig(\n        r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n    )\n    training_arguments = TrainingArguments(\n        output_dir=\"codellama2-finetuned-codex\",\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_32bit\",\n        learning_rate=2e-4,\n        lr_scheduler_type=\"cosine\",\n        save_strategy=\"epoch\",\n        logging_steps=10,\n        num_train_epochs=1,\n        max_steps=100,\n        fp16=True,\n        push_to_hub=True\n    )\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=data,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        args=training_arguments,\n        tokenizer=tokenizer,\n        packing=False,\n        max_seq_length=512\n    )\n    trainer.train()\n    trainer.push_to_hub()\n\nif __name__ == \"__main__\":\n    finetune_llama_v2()\n</pre> import torch from datasets import load_dataset, Dataset from peft import LoraConfig, AutoPeftModelForCausalLM from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments from trl import SFTTrainer import os  def finetune_llama_v2():     data = load_dataset(\"code_x_glue_tc_nl_code_search_adv\", split=\"validation\")     data_df = data.to_pandas()     data_df[\"text\"] = data_df[[\"docstring\", \"code\"]].apply(lambda x: \"[INST] Docstring: \" + x[\"docstring\"] + \" [/INST] Code: \" + x[\"code\"] + \"\", axis=1)     data = Dataset.from_pandas(data_df)     tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")     tokenizer.pad_token = tokenizer.eos_token     bnb_config = BitsAndBytesConfig(         load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True     )     model = AutoModelForCausalLM.from_pretrained(         \"codellama/CodeLlama-7b-hf\", quantization_config=bnb_config, device_map=\"auto\"     )     model.config.use_cache=False     model.config.pretraining_tp=1     peft_config = LoraConfig(         r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"     )     training_arguments = TrainingArguments(         output_dir=\"codellama2-finetuned-codex\",         per_device_train_batch_size=8,         gradient_accumulation_steps=4,         optim=\"paged_adamw_32bit\",         learning_rate=2e-4,         lr_scheduler_type=\"cosine\",         save_strategy=\"epoch\",         logging_steps=10,         num_train_epochs=1,         max_steps=100,         fp16=True,         push_to_hub=True     )     trainer = SFTTrainer(         model=model,         train_dataset=data,         peft_config=peft_config,         dataset_text_field=\"text\",         args=training_arguments,         tokenizer=tokenizer,         packing=False,         max_seq_length=512     )     trainer.train()     trainer.push_to_hub()  if __name__ == \"__main__\":     finetune_llama_v2() In\u00a0[\u00a0]: Copied! <pre>! cp -r /content/codellama2-finetuned-codex /content/drive/MyDrive/\n</pre> ! cp -r /content/codellama2-finetuned-codex /content/drive/MyDrive/ In\u00a0[\u00a0]: Copied! <pre>from peft import AutoPeftModelForCausalLM, PeftModel\nfrom transformers import AutoModelForCausalLM\nimport torch\nimport os\nmodel = AutoModelForCausalLM.from_pretrained(\n            \"codellama/CodeLlama-7b-hf\", torch_dtype=torch.float16, load_in_8bit=False, device_map=\"auto\", trust_remote_code=True)\n</pre> from peft import AutoPeftModelForCausalLM, PeftModel from transformers import AutoModelForCausalLM import torch import os model = AutoModelForCausalLM.from_pretrained(             \"codellama/CodeLlama-7b-hf\", torch_dtype=torch.float16, load_in_8bit=False, device_map=\"auto\", trust_remote_code=True) In\u00a0[\u00a0]: Copied! <pre>peft_model = PeftModel.from_pretrained(model, \"Vasanth/codellama2-finetuned-codex\", from_transformers=True, device_map={\"\":0})\n</pre> peft_model = PeftModel.from_pretrained(model, \"Vasanth/codellama2-finetuned-codex\", from_transformers=True, device_map={\"\":0}) In\u00a0[\u00a0]: Copied! <pre>model = peft_model.merge_and_unload()\n</pre> model = peft_model.merge_and_unload() In\u00a0[\u00a0]: Copied! <pre>model.push_to_hub(\"codellama2-finetuned-codex-fin\")\n</pre> model.push_to_hub(\"codellama2-finetuned-codex-fin\") In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\ntokenizer.push_to_hub(\"codellama2-finetuned-codex-fin\")\n</pre> from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\") tokenizer.push_to_hub(\"codellama2-finetuned-codex-fin\") In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoTokenizer\nfrom transformers import pipeline\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"Vasanth/codellama2-finetuned-codex-fin\")\npipe = pipeline(\n    \"text-generation\",\n    model=\"Vasanth/codellama2-finetuned-codex-fin\",\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\n\nsequences = pipe(\n    'def fibonacci(',\n    do_sample=True,\n    temperature=0.2,\n    top_p=0.9,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    max_length=100,\n)\nfor seq in sequences:\n    print(f\"Result: {seq['generated_text']}\")\n</pre> from transformers import AutoTokenizer from transformers import pipeline import torch  tokenizer = AutoTokenizer.from_pretrained(\"Vasanth/codellama2-finetuned-codex-fin\") pipe = pipeline(     \"text-generation\",     model=\"Vasanth/codellama2-finetuned-codex-fin\",     torch_dtype=torch.float16,     device_map=\"auto\", )  sequences = pipe(     'def fibonacci(',     do_sample=True,     temperature=0.2,     top_p=0.9,     num_return_sequences=1,     eos_token_id=tokenizer.eos_token_id,     max_length=100, ) for seq in sequences:     print(f\"Result: {seq['generated_text']}\")  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/llmsrelated/005-codellama/Finetune_CodeLLamA_with_inference/#model-training","title":"Model Training\u00b6","text":""},{"location":"res/llmsrelated/005-codellama/Finetune_CodeLLamA_with_inference/#model-saving","title":"Model Saving\u00b6","text":""},{"location":"res/llmsrelated/005-codellama/Finetune_CodeLLamA_with_inference/#model-push-to-hub","title":"Model Push to Hub\u00b6","text":""},{"location":"res/llmsrelated/005-codellama/Finetune_CodeLLamA_with_inference/#model-inferencing","title":"Model Inferencing\u00b6","text":""},{"location":"res/llmsrelated/005-codellama/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>import gradio as gr\nfrom ctransformers import AutoModelForCausalLM\n\ntitle = \"CodeLlama 7B GGUF Demo\"\n\ndef load_model():\n    model = AutoModelForCausalLM.from_pretrained(\"codellama-7b-instruct.Q4_K_M.gguf\",\n    model_type='llama',\n    max_new_tokens = 64\n    )\n    return model\n\ndef chat_with_model(inp_chat, chat_history):\n\n    model = load_model()\n    response = model(\n        inp_chat\n    )\n    return response\n\nexamples = [\n    'Write the python code to train a linear regression model without using scikit-learn library.',\n    'Write a Python code to generate even numbers from 0 to n given numbers',\n    'Write a Python code to implement Stack data structure'\n]\n\ngr.ChatInterface(\n    fn=chat_with_model,\n    title=title,\n    examples=examples\n).launch()\n</pre> import gradio as gr from ctransformers import AutoModelForCausalLM  title = \"CodeLlama 7B GGUF Demo\"  def load_model():     model = AutoModelForCausalLM.from_pretrained(\"codellama-7b-instruct.Q4_K_M.gguf\",     model_type='llama',     max_new_tokens = 64     )     return model  def chat_with_model(inp_chat, chat_history):      model = load_model()     response = model(         inp_chat     )     return response  examples = [     'Write the python code to train a linear regression model without using scikit-learn library.',     'Write a Python code to generate even numbers from 0 to n given numbers',     'Write a Python code to implement Stack data structure' ]  gr.ChatInterface(     fn=chat_with_model,     title=title,     examples=examples ).launch()"},{"location":"res/nlpprojs/001-flan-chatbot/Chatbot/","title":"Chatbot","text":"In\u00a0[\u00a0]: Copied! <pre>! nvidia-smi\n</pre> ! nvidia-smi In\u00a0[\u00a0]: Copied! <pre>from google.colab import drive\ndrive.mount('/content/drive')\n</pre> from google.colab import drive drive.mount('/content/drive') In\u00a0[\u00a0]: Copied! <pre>! pip install transformers datasets accelerate peft\n</pre> ! pip install transformers datasets accelerate peft In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n</pre> import pandas as pd from datasets import Dataset from transformers import AutoTokenizer, AutoModelForSeq2SeqLM from peft import LoraConfig, get_peft_model, TaskType from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq In\u00a0[\u00a0]: Copied! <pre>train_df = pd.read_parquet(\"train.parquet\")\ntest_df = pd.read_parquet(\"test.parquet\")\ntrain_data = Dataset.from_pandas(train_df)\ntest_data = Dataset.from_pandas(test_df)\n</pre> train_df = pd.read_parquet(\"train.parquet\") test_df = pd.read_parquet(\"test.parquet\") train_data = Dataset.from_pandas(train_df) test_data = Dataset.from_pandas(test_df) In\u00a0[\u00a0]: Copied! <pre>model_id=\"google/flan-t5-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n</pre> model_id=\"google/flan-t5-large\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForSeq2SeqLM.from_pretrained(model_id) In\u00a0[\u00a0]: Copied! <pre>def preprocess_function(sample,padding=\"max_length\"):\n    model_inputs = tokenizer(sample[\"Human\"], max_length=256, padding=padding, truncation=True)\n    labels = tokenizer(sample[\"Assistant\"], max_length=256, padding=padding, truncation=True)\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n</pre> def preprocess_function(sample,padding=\"max_length\"):     model_inputs = tokenizer(sample[\"Human\"], max_length=256, padding=padding, truncation=True)     labels = tokenizer(sample[\"Assistant\"], max_length=256, padding=padding, truncation=True)     if padding == \"max_length\":         labels[\"input_ids\"] = [             [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]         ]     model_inputs[\"labels\"] = labels[\"input_ids\"]     return model_inputs In\u00a0[\u00a0]: Copied! <pre>train_tokenized_dataset = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\ntest_tokenized_dataset = test_data.map(preprocess_function, batched=True, remove_columns=test_data.column_names)\nprint(f\"Keys of tokenized dataset: {list(train_tokenized_dataset.features)}\")\n</pre> train_tokenized_dataset = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names) test_tokenized_dataset = test_data.map(preprocess_function, batched=True, remove_columns=test_data.column_names) print(f\"Keys of tokenized dataset: {list(train_tokenized_dataset.features)}\") In\u00a0[\u00a0]: Copied! <pre>lora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n target_modules=[\"q\", \"v\"],\n lora_dropout=0.1,\n bias=\"none\",\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n</pre> lora_config = LoraConfig(  r=16,  lora_alpha=32,  target_modules=[\"q\", \"v\"],  lora_dropout=0.1,  bias=\"none\",  task_type=TaskType.SEQ_2_SEQ_LM ) In\u00a0[\u00a0]: Copied! <pre>model = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n</pre> model = get_peft_model(model, lora_config) model.print_trainable_parameters() In\u00a0[\u00a0]: Copied! <pre>label_pad_token_id = -100\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)\n</pre> label_pad_token_id = -100 data_collator = DataCollatorForSeq2Seq(     tokenizer,     model=model,     label_pad_token_id=label_pad_token_id,     pad_to_multiple_of=8 ) In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>output_dir=\"lora-flan-t5-large-chat\"\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=4,\n    learning_rate=1e-3,\n    num_train_epochs=1,\n    logging_dir=f\"{output_dir}/logs\",\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    report_to=\"tensorboard\",\n    push_to_hub = True\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_tokenized_dataset,\n)\nmodel.config.use_cache = False\n\ntrainer.train()\npeft_save_model_id=\"lora-flan-t5-large-chat\"\ntrainer.model.save_pretrained(peft_save_model_id, push_to_hub=True)\ntokenizer.save_pretrained(peft_save_model_id, push_to_hub=True)\ntrainer.model.base_model.save_pretrained(peft_save_model_id, push_to_hub=True)\n</pre> output_dir=\"lora-flan-t5-large-chat\" training_args = Seq2SeqTrainingArguments(     output_dir=output_dir,     per_device_train_batch_size=4,     learning_rate=1e-3,     num_train_epochs=1,     logging_dir=f\"{output_dir}/logs\",     logging_strategy=\"epoch\",     save_strategy=\"epoch\",     report_to=\"tensorboard\",     push_to_hub = True )  trainer = Seq2SeqTrainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=train_tokenized_dataset, ) model.config.use_cache = False  trainer.train() peft_save_model_id=\"lora-flan-t5-large-chat\" trainer.model.save_pretrained(peft_save_model_id, push_to_hub=True) tokenizer.save_pretrained(peft_save_model_id, push_to_hub=True) trainer.model.base_model.save_pretrained(peft_save_model_id, push_to_hub=True) In\u00a0[\u00a0]: Copied! <pre>! cp -r /content/lora-flan-t5-large-chat/ /content/drive/MyDrive/Chatbot/\n</pre> ! cp -r /content/lora-flan-t5-large-chat/ /content/drive/MyDrive/Chatbot/ In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load peft config for pre-trained checkpoint etc.\npeft_model_id = \"lora-flan-t5-large-chat\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\nmodel = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0}).cuda()\nmodel.eval()\n\nsample = \"Human: \\nExplain me about the working of Artificial Intelligence. \\nAssistant: \"\ninput_ids = tokenizer(sample, return_tensors=\"pt\", truncation=True, max_length=256).input_ids.cuda()\noutputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9, max_length=256)\nprint(f\"{sample}\")\n\nprint(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\n</pre> import torch from peft import PeftModel, PeftConfig from transformers import AutoModelForSeq2SeqLM, AutoTokenizer  # Load peft config for pre-trained checkpoint etc. peft_model_id = \"lora-flan-t5-large-chat\" config = PeftConfig.from_pretrained(peft_model_id)  model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path) tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)  model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0}).cuda() model.eval()  sample = \"Human: \\nExplain me about the working of Artificial Intelligence. \\nAssistant: \" input_ids = tokenizer(sample, return_tensors=\"pt\", truncation=True, max_length=256).input_ids.cuda() outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9, max_length=256) print(f\"{sample}\")  print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpprojs/001-flan-chatbot/data/","title":"Data","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom datasets import load_dataset\n</pre> import pandas as pd from datasets import load_dataset In\u00a0[\u00a0]: Copied! <pre>data = load_dataset(\"Fredithefish/Instruction-Tuning-with-GPT-4-RedPajama-Chat\")\ndata\n</pre> data = load_dataset(\"Fredithefish/Instruction-Tuning-with-GPT-4-RedPajama-Chat\") data In\u00a0[\u00a0]: Copied! <pre>df = data[\"train\"].to_pandas()\ndf\n</pre> df = data[\"train\"].to_pandas() df In\u00a0[\u00a0]: Copied! <pre>df[\"Assistant\"] = df[\"text\"].apply(lambda x: x.split(\"&lt;bot&gt;:\")[-1])\ndf[\"Human\"] = df[\"text\"].apply(lambda x: \"Human:\" + x.split(\"&lt;bot&gt;:\")[0].replace(\"&lt;human&gt;:\", \"\").replace(\"\\n\", \"\") + \". Assistant: \")\ndf\n</pre> df[\"Assistant\"] = df[\"text\"].apply(lambda x: x.split(\":\")[-1]) df[\"Human\"] = df[\"text\"].apply(lambda x: \"Human:\" + x.split(\":\")[0].replace(\":\", \"\").replace(\"\\n\", \"\") + \". Assistant: \") df In\u00a0[\u00a0]: Copied! <pre>df = df.sample(20000)\ndf\n</pre> df = df.sample(20000) df In\u00a0[\u00a0]: Copied! <pre>df = df[[\"Human\", \"Assistant\"]]\n</pre> df = df[[\"Human\", \"Assistant\"]] In\u00a0[\u00a0]: Copied! <pre>df.iloc[0][\"Human\"]\n</pre> df.iloc[0][\"Human\"] In\u00a0[\u00a0]: Copied! <pre>df[:15000].to_parquet(\"train.parquet\", index=False)\ndf[15000:].to_parquet(\"test.parquet\", index=False)\n</pre> df[:15000].to_parquet(\"train.parquet\", index=False) df[15000:].to_parquet(\"test.parquet\", index=False) In\u00a0[\u00a0]: Copied! <pre>df.to_parquet(\"chat_data.parquet\", index=False)\n</pre> df.to_parquet(\"chat_data.parquet\", index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpprojs/002-text-classifier/YT_2_TextClassification/","title":"Config","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install transformers datasets accelerate evaluate\n</pre> ! pip install transformers datasets accelerate evaluate In\u00a0[\u00a0]: Copied! <pre>import torch\n\nclass Config:\n    DATASET_ID = \"emad12/stock_tweets_sentiment\"\n    MODEL_CKPT = \"distilbert-base-uncased\"\n    SRC_COLUMN = \"tweet\"\n    TGT_COLUMN = \"sentiment\"\n    TEST_SIZE = 0.2\n    SEED = 0\n    MAX_LEN = 32\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    ID2LABEL = {0: \"NEUTRAL\", 1: \"POSITIVE\", 2:\"NEGATIVE\"}\n    LABEL2ID = {\"NEUTRAL\": 0, \"POSITIVE\": 1, \"NEGATIVE\": 2}\n    EVAL_METRIC = \"accuracy\"\n    MODEL_OUT_DIR = \"distilbert-stock-tweet-sentiment-analysis\"\n    NUM_EPOCHS = 3\n    LR = 2E-5\n    BATCH_SIZE = 16\n    WEIGHT_DECAY = 0.01\n    EVAL_STRATEGY = \"epoch\"\n    SAVE_STRATEGY = \"epoch\"\n    LOGGING_STRATEGY = \"epoch\"\n    PUSH_TO_HUB = True\n\nconfig = Config()\n</pre> import torch  class Config:     DATASET_ID = \"emad12/stock_tweets_sentiment\"     MODEL_CKPT = \"distilbert-base-uncased\"     SRC_COLUMN = \"tweet\"     TGT_COLUMN = \"sentiment\"     TEST_SIZE = 0.2     SEED = 0     MAX_LEN = 32     DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"     ID2LABEL = {0: \"NEUTRAL\", 1: \"POSITIVE\", 2:\"NEGATIVE\"}     LABEL2ID = {\"NEUTRAL\": 0, \"POSITIVE\": 1, \"NEGATIVE\": 2}     EVAL_METRIC = \"accuracy\"     MODEL_OUT_DIR = \"distilbert-stock-tweet-sentiment-analysis\"     NUM_EPOCHS = 3     LR = 2E-5     BATCH_SIZE = 16     WEIGHT_DECAY = 0.01     EVAL_STRATEGY = \"epoch\"     SAVE_STRATEGY = \"epoch\"     LOGGING_STRATEGY = \"epoch\"     PUSH_TO_HUB = True  config = Config() In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset, load_dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch\n# import config\n\nclass TextClassificationDataset:\n    def __init__(self):\n        self.dataset_id = config.DATASET_ID\n        self.model_ckpt = config.MODEL_CKPT\n        self.src_column = config.SRC_COLUMN\n        self.tgt_column = config.TGT_COLUMN\n        self.test_size = config.TEST_SIZE\n        self.seed = config.SEED\n        self.max_len = config.MAX_LEN\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_ckpt)\n\n    def create_data(self):\n        self.data = load_dataset(self.dataset_id, split=\"train\") # load_dataset(\"parquet\", data_files=file_path)\n        self.df = self.data.to_pandas()\n        self.df = self.df[[self.src_column, self.tgt_column]]\n        self.df[self.src_column] = self.df[self.src_column].apply(lambda x: x.lower()) # If needed\n        self.df[self.tgt_column] = self.df[self.tgt_column].apply(lambda x: 2 if x==-1 else x) # If needed\n        self.df = self.df.sample(20000) # If needed\n        self.train_df, self.test_df = train_test_split(self.df, test_size=self.test_size, shuffle=True, random_state=self.seed, stratify=self.df[self.tgt_column])\n        self.train_data = Dataset.from_pandas(self.train_df)\n        self.test_data = Dataset.from_pandas(self.test_df)\n        return self.train_data, self.test_data\n\n    def tokenize_function(self, example):\n        model_inp = self.tokenizer(example[self.src_column], truncation=True, padding=True, max_length=self.max_len)\n        labels = torch.tensor(example[self.tgt_column], dtype=torch.int)\n        model_inp[\"labels\"] = labels\n        return model_inp\n\n    def preprocess_function(self, data):\n        model_inp = data.map(self.tokenize_function, batched=True, remove_columns=data.column_names)\n        return model_inp\n\n    def gen_classification_dataset(self):\n        train_data, test_data = self.create_data()\n        train_tokenized_data = self.preprocess_function(train_data)\n        test_tokenized_data = self.preprocess_function(test_data)\n        return train_tokenized_data, test_tokenized_data\n</pre> from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments from datasets import Dataset, load_dataset import pandas as pd from sklearn.model_selection import train_test_split import torch # import config  class TextClassificationDataset:     def __init__(self):         self.dataset_id = config.DATASET_ID         self.model_ckpt = config.MODEL_CKPT         self.src_column = config.SRC_COLUMN         self.tgt_column = config.TGT_COLUMN         self.test_size = config.TEST_SIZE         self.seed = config.SEED         self.max_len = config.MAX_LEN         self.tokenizer = AutoTokenizer.from_pretrained(self.model_ckpt)      def create_data(self):         self.data = load_dataset(self.dataset_id, split=\"train\") # load_dataset(\"parquet\", data_files=file_path)         self.df = self.data.to_pandas()         self.df = self.df[[self.src_column, self.tgt_column]]         self.df[self.src_column] = self.df[self.src_column].apply(lambda x: x.lower()) # If needed         self.df[self.tgt_column] = self.df[self.tgt_column].apply(lambda x: 2 if x==-1 else x) # If needed         self.df = self.df.sample(20000) # If needed         self.train_df, self.test_df = train_test_split(self.df, test_size=self.test_size, shuffle=True, random_state=self.seed, stratify=self.df[self.tgt_column])         self.train_data = Dataset.from_pandas(self.train_df)         self.test_data = Dataset.from_pandas(self.test_df)         return self.train_data, self.test_data      def tokenize_function(self, example):         model_inp = self.tokenizer(example[self.src_column], truncation=True, padding=True, max_length=self.max_len)         labels = torch.tensor(example[self.tgt_column], dtype=torch.int)         model_inp[\"labels\"] = labels         return model_inp      def preprocess_function(self, data):         model_inp = data.map(self.tokenize_function, batched=True, remove_columns=data.column_names)         return model_inp      def gen_classification_dataset(self):         train_data, test_data = self.create_data()         train_tokenized_data = self.preprocess_function(train_data)         test_tokenized_data = self.preprocess_function(test_data)         return train_tokenized_data, test_tokenized_data In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\nimport evaluate\nimport numpy as np\n\nclass TextClassificationModelTrainer:\n    def __init__(self, train_data, test_data):\n        self.train_data = train_data\n        self.test_data = test_data\n        self.model_ckpt = config.MODEL_CKPT\n        self.id2label = config.ID2LABEL\n        self.label2id = config.LABEL2ID\n        self.num_labels = len(self.id2label)\n        self.device = config.DEVICE\n        self.eval_metric = config.EVAL_METRIC\n        self.model_out_dir = config.MODEL_OUT_DIR\n        self.num_epochs = config.NUM_EPOCHS\n        self.lr = config.LR\n        self.batch_size = config.BATCH_SIZE\n        self.weight_decay = config.WEIGHT_DECAY\n        self.eval_strategy = config.EVAL_STRATEGY\n        self.save_strategy = config.SAVE_STRATEGY\n        self.logging_strategy = config.LOGGING_STRATEGY\n        self.push_to_hub = config.PUSH_TO_HUB\n        self.model = AutoModelForSequenceClassification.from_pretrained(\n                                                                        self.model_ckpt,\n                                                                        id2label=self.id2label,\n                                                                        label2id=self.label2id,\n                                                                        num_labels=self.num_labels\n                                                                        ).to(self.device)\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_ckpt)\n        self.eval_metric_computer = evaluate.load(self.eval_metric)\n        self.data_collator = DataCollatorWithPadding(self.tokenizer)\n\n    def compute_metrics(self, eval_pred):\n        predictions, labels = eval_pred\n        predictions = np.argmax(predictions, axis=1)\n        return self.eval_metric_computer.compute(predictions=predictions, references=labels)\n\n    def set_training_args(self):\n        return TrainingArguments(\n        output_dir = self.model_out_dir,\n        num_train_epochs=self.num_epochs,\n        learning_rate = self.lr,\n        per_device_train_batch_size = self.batch_size,\n        per_device_eval_batch_size = self.batch_size,\n        weight_decay = self.weight_decay,\n        evaluation_strategy = self.eval_strategy,\n        save_strategy = self.save_strategy,\n        logging_strategy = self.logging_strategy,\n        push_to_hub = self.push_to_hub\n        )\n\n    def model_trainer(self):\n        return Trainer(\n            model = self.model,\n            args = self.set_training_args(),\n            data_collator = self.data_collator,\n            train_dataset = self.train_data,\n            eval_dataset = self.test_data,\n            compute_metrics = self.compute_metrics\n        )\n\n    def train_and_save_and_push_to_hub(self):\n        trainer = self.model_trainer()\n        trainer.train()\n        trainer.push_to_hub()\n</pre> import torch from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding import evaluate import numpy as np  class TextClassificationModelTrainer:     def __init__(self, train_data, test_data):         self.train_data = train_data         self.test_data = test_data         self.model_ckpt = config.MODEL_CKPT         self.id2label = config.ID2LABEL         self.label2id = config.LABEL2ID         self.num_labels = len(self.id2label)         self.device = config.DEVICE         self.eval_metric = config.EVAL_METRIC         self.model_out_dir = config.MODEL_OUT_DIR         self.num_epochs = config.NUM_EPOCHS         self.lr = config.LR         self.batch_size = config.BATCH_SIZE         self.weight_decay = config.WEIGHT_DECAY         self.eval_strategy = config.EVAL_STRATEGY         self.save_strategy = config.SAVE_STRATEGY         self.logging_strategy = config.LOGGING_STRATEGY         self.push_to_hub = config.PUSH_TO_HUB         self.model = AutoModelForSequenceClassification.from_pretrained(                                                                         self.model_ckpt,                                                                         id2label=self.id2label,                                                                         label2id=self.label2id,                                                                         num_labels=self.num_labels                                                                         ).to(self.device)         self.tokenizer = AutoTokenizer.from_pretrained(self.model_ckpt)         self.eval_metric_computer = evaluate.load(self.eval_metric)         self.data_collator = DataCollatorWithPadding(self.tokenizer)      def compute_metrics(self, eval_pred):         predictions, labels = eval_pred         predictions = np.argmax(predictions, axis=1)         return self.eval_metric_computer.compute(predictions=predictions, references=labels)      def set_training_args(self):         return TrainingArguments(         output_dir = self.model_out_dir,         num_train_epochs=self.num_epochs,         learning_rate = self.lr,         per_device_train_batch_size = self.batch_size,         per_device_eval_batch_size = self.batch_size,         weight_decay = self.weight_decay,         evaluation_strategy = self.eval_strategy,         save_strategy = self.save_strategy,         logging_strategy = self.logging_strategy,         push_to_hub = self.push_to_hub         )      def model_trainer(self):         return Trainer(             model = self.model,             args = self.set_training_args(),             data_collator = self.data_collator,             train_dataset = self.train_data,             eval_dataset = self.test_data,             compute_metrics = self.compute_metrics         )      def train_and_save_and_push_to_hub(self):         trainer = self.model_trainer()         trainer.train()         trainer.push_to_hub() In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    textclassificationdataset = TextClassificationDataset()\n    train_data, test_data = textclassificationdataset.gen_classification_dataset()\n    textclassificationtrainer = TextClassificationModelTrainer(train_data, test_data)\n    textclassificationtrainer.train_and_save_and_push_to_hub()\n</pre> if __name__ == \"__main__\":     textclassificationdataset = TextClassificationDataset()     train_data, test_data = textclassificationdataset.gen_classification_dataset()     textclassificationtrainer = TextClassificationModelTrainer(train_data, test_data)     textclassificationtrainer.train_and_save_and_push_to_hub() In\u00a0[\u00a0]: Copied! <pre>from transformers import pipeline\nclassifier = pipeline(\"sentiment-analysis\", model=config.MODEL_OUT_DIR, tokenizer=\"distilbert-base-uncased\")\nclassifier(\"have a great weekend everyone will be back to full schedule next week spy aapl baba\")\n</pre> from transformers import pipeline classifier = pipeline(\"sentiment-analysis\", model=config.MODEL_OUT_DIR, tokenizer=\"distilbert-base-uncased\") classifier(\"have a great weekend everyone will be back to full schedule next week spy aapl baba\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpprojs/002-text-classifier/YT_2_TextClassification/#config","title":"Config\u00b6","text":""},{"location":"res/nlpprojs/002-text-classifier/YT_2_TextClassification/#dataset","title":"Dataset\u00b6","text":""},{"location":"res/nlpprojs/002-text-classifier/YT_2_TextClassification/#model-trainer","title":"Model Trainer\u00b6","text":""},{"location":"res/nlpprojs/002-text-classifier/YT_2_TextClassification/#main","title":"Main\u00b6","text":""},{"location":"res/nlpprojs/002-text-classifier/YT_2_TextClassification/#inference","title":"Inference\u00b6","text":""},{"location":"res/nlpprojs/003-criccommsumm/Cricket_News_To_Cricket_Match_Report/","title":"Cricket News To Cricket Match Report","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install transformers datasets accelerate peft\n</pre> ! pip install transformers datasets accelerate peft In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>class config:\n    DATA_PATH = \"/content/match_report_gen.csv\"\n    MODEL_CKPT = \"google/flan-t5-large\"\n    SRC_COLUMN = \"Commentary_Highlights\"\n    TGT_COLUMN = \"Match_Report\"\n    SRC_MAX_LEN = 1200\n    TGT_MAX_LEN = 256\n    LABEL_PAD_TOKEN_ID = -100\n    PADDING = \"max_length\"\n    TRUNCATION = True\n    LORA_R = 8\n    LORA_ALPHA = 16\n    LORA_TGT_MODULES = [\"q\", \"v\"]\n    LORA_DROPOUT = 0.1\n    LORA_TASK_TYPE = \"SEQ_2_SEQ_LM\"\n    PAD_TO_MULTIPLE = 8\n    MODEL_OUT_DIR = \"criccomm_to_cricnewss\"\n    TRAIN_BATCH_SIZE = 1\n    LR = 2e-4\n    NUM_TRAIN_EPOCHS = 3\n    LOGGING_DIR = f\"{MODEL_OUT_DIR}/logs\"\n    LOGGING_STRATEGY = \"epoch\"\n    SAVE_STRATEGY = \"epoch\"\n    SAVE_LIMIT = 1\n    PUSH_TO_HUB = True\n</pre> class config:     DATA_PATH = \"/content/match_report_gen.csv\"     MODEL_CKPT = \"google/flan-t5-large\"     SRC_COLUMN = \"Commentary_Highlights\"     TGT_COLUMN = \"Match_Report\"     SRC_MAX_LEN = 1200     TGT_MAX_LEN = 256     LABEL_PAD_TOKEN_ID = -100     PADDING = \"max_length\"     TRUNCATION = True     LORA_R = 8     LORA_ALPHA = 16     LORA_TGT_MODULES = [\"q\", \"v\"]     LORA_DROPOUT = 0.1     LORA_TASK_TYPE = \"SEQ_2_SEQ_LM\"     PAD_TO_MULTIPLE = 8     MODEL_OUT_DIR = \"criccomm_to_cricnewss\"     TRAIN_BATCH_SIZE = 1     LR = 2e-4     NUM_TRAIN_EPOCHS = 3     LOGGING_DIR = f\"{MODEL_OUT_DIR}/logs\"     LOGGING_STRATEGY = \"epoch\"     SAVE_STRATEGY = \"epoch\"     SAVE_LIMIT = 1     PUSH_TO_HUB = True In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\nclass CricketCommentaryReportGenerationDatasetModule:\n\n    def __init__(self):\n        self.data_path = config.DATA_PATH\n        self.model_ckpt = config.MODEL_CKPT\n        self.src_column = config.SRC_COLUMN\n        self.tgt_column = config.TGT_COLUMN\n        self.src_max_len = config.SRC_MAX_LEN\n        self.tgt_max_len = config.TGT_MAX_LEN\n        self.padding = config.PADDING\n        self.truncation = config.TRUNCATION\n        self.label_pad_token_id = config.LABEL_PAD_TOKEN_ID\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_ckpt)\n\n    def load_data(self):\n        df = pd.read_csv(self.data_path)\n        data = Dataset.from_pandas(df)\n        return data\n\n    def preprocess_function(self, example):\n        model_inp = self.tokenizer(\n            example[self.src_column],\n            max_length = self.src_max_len,\n            padding = self.padding,\n            truncation = self.truncation\n        )\n\n        labels = self.tokenizer(\n            example[self.tgt_column],\n            max_length = self.tgt_max_len,\n            padding = self.padding,\n            truncation = self.truncation\n        )\n\n        labels[\"input_ids\"] = [\n            (label if label != self.tokenizer.pad_token_id else self.label_pad_token_id) for label in labels[\"input_ids\"]\n        ]\n\n        model_inp[\"labels\"] = labels[\"input_ids\"]\n\n        return model_inp\n\n    def gen_data(self):\n        data = self.load_data()\n        tokenized_data = data.map(\n            self.preprocess_function,\n            batched = False,\n            remove_columns = data.column_names\n        )\n        return tokenized_data\n</pre> import pandas as pd from datasets import Dataset from transformers import AutoTokenizer  class CricketCommentaryReportGenerationDatasetModule:      def __init__(self):         self.data_path = config.DATA_PATH         self.model_ckpt = config.MODEL_CKPT         self.src_column = config.SRC_COLUMN         self.tgt_column = config.TGT_COLUMN         self.src_max_len = config.SRC_MAX_LEN         self.tgt_max_len = config.TGT_MAX_LEN         self.padding = config.PADDING         self.truncation = config.TRUNCATION         self.label_pad_token_id = config.LABEL_PAD_TOKEN_ID         self.tokenizer = AutoTokenizer.from_pretrained(self.model_ckpt)      def load_data(self):         df = pd.read_csv(self.data_path)         data = Dataset.from_pandas(df)         return data      def preprocess_function(self, example):         model_inp = self.tokenizer(             example[self.src_column],             max_length = self.src_max_len,             padding = self.padding,             truncation = self.truncation         )          labels = self.tokenizer(             example[self.tgt_column],             max_length = self.tgt_max_len,             padding = self.padding,             truncation = self.truncation         )          labels[\"input_ids\"] = [             (label if label != self.tokenizer.pad_token_id else self.label_pad_token_id) for label in labels[\"input_ids\"]         ]          model_inp[\"labels\"] = labels[\"input_ids\"]          return model_inp      def gen_data(self):         data = self.load_data()         tokenized_data = data.map(             self.preprocess_function,             batched = False,             remove_columns = data.column_names         )         return tokenized_data In\u00a0[\u00a0]: Copied! <pre>from peft import get_peft_model, LoraConfig\nfrom transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n\nclass CricketCommentaryReportGenerationUtils:\n\n    def __init__(self):\n        self.lora_r = config.LORA_R\n        self.lora_alpha = config.LORA_ALPHA\n        self.lora_tgt_modules = config.LORA_TGT_MODULES\n        self.lora_dropout = config.LORA_DROPOUT\n        self.lora_task_type = config.LORA_TASK_TYPE\n        self.label_pad_token_id = config.LABEL_PAD_TOKEN_ID\n        self.pad_to_multiple = config.PAD_TO_MULTIPLE\n        self.model_out_dir = config.MODEL_OUT_DIR\n        self.train_batch_size =config.TRAIN_BATCH_SIZE\n        self.lr = config.LR\n        self.num_train_epochs = config.NUM_TRAIN_EPOCHS\n        self.logging_dir = config.LOGGING_DIR\n        self.logging_strategy = config.LOGGING_STRATEGY\n        self.save_strategy = config.SAVE_STRATEGY\n        self.save_limit = config.SAVE_LIMIT\n        self.push_to_hub = config.PUSH_TO_HUB\n\n    def lora_model(self, model):\n        lora_config = LoraConfig(\n            r = self.lora_r,\n            target_modules= self.lora_tgt_modules,\n            lora_alpha = self.lora_alpha,\n            lora_dropout = self.lora_dropout,\n            bias = \"none\",\n            task_type = self.lora_task_type\n        )\n        lora_model = get_peft_model(model, lora_config)\n        print(lora_model.print_trainable_parameters())\n        return lora_model\n\n    def train_data_collator(self, tokenizer, model):\n        training_data_collator = DataCollatorForSeq2Seq(\n            tokenizer,\n            model = model,\n            label_pad_token_id = self.label_pad_token_id,\n            pad_to_multiple_of = self.pad_to_multiple\n        )\n\n        return training_data_collator\n\n    def set_training_args(self):\n\n        training_args = Seq2SeqTrainingArguments(\n            output_dir = self.model_out_dir,\n            per_device_train_batch_size = self.train_batch_size,\n            learning_rate = self.lr,\n            num_train_epochs = self.num_train_epochs,\n            logging_dir = self.logging_dir,\n            logging_strategy = self.logging_strategy,\n            save_strategy = self.save_strategy,\n            save_total_limit = self.save_limit,\n            push_to_hub = self.push_to_hub\n        )\n        return training_args\n</pre> from peft import get_peft_model, LoraConfig from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments  class CricketCommentaryReportGenerationUtils:      def __init__(self):         self.lora_r = config.LORA_R         self.lora_alpha = config.LORA_ALPHA         self.lora_tgt_modules = config.LORA_TGT_MODULES         self.lora_dropout = config.LORA_DROPOUT         self.lora_task_type = config.LORA_TASK_TYPE         self.label_pad_token_id = config.LABEL_PAD_TOKEN_ID         self.pad_to_multiple = config.PAD_TO_MULTIPLE         self.model_out_dir = config.MODEL_OUT_DIR         self.train_batch_size =config.TRAIN_BATCH_SIZE         self.lr = config.LR         self.num_train_epochs = config.NUM_TRAIN_EPOCHS         self.logging_dir = config.LOGGING_DIR         self.logging_strategy = config.LOGGING_STRATEGY         self.save_strategy = config.SAVE_STRATEGY         self.save_limit = config.SAVE_LIMIT         self.push_to_hub = config.PUSH_TO_HUB      def lora_model(self, model):         lora_config = LoraConfig(             r = self.lora_r,             target_modules= self.lora_tgt_modules,             lora_alpha = self.lora_alpha,             lora_dropout = self.lora_dropout,             bias = \"none\",             task_type = self.lora_task_type         )         lora_model = get_peft_model(model, lora_config)         print(lora_model.print_trainable_parameters())         return lora_model      def train_data_collator(self, tokenizer, model):         training_data_collator = DataCollatorForSeq2Seq(             tokenizer,             model = model,             label_pad_token_id = self.label_pad_token_id,             pad_to_multiple_of = self.pad_to_multiple         )          return training_data_collator      def set_training_args(self):          training_args = Seq2SeqTrainingArguments(             output_dir = self.model_out_dir,             per_device_train_batch_size = self.train_batch_size,             learning_rate = self.lr,             num_train_epochs = self.num_train_epochs,             logging_dir = self.logging_dir,             logging_strategy = self.logging_strategy,             save_strategy = self.save_strategy,             save_total_limit = self.save_limit,             push_to_hub = self.push_to_hub         )         return training_args In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer\n\nclass CricketCommentaryReportGenerationTrainer:\n\n    def __init__(self):\n        self.model_ckpt = config.MODEL_CKPT\n        self.model_out_dir = config.MODEL_OUT_DIR\n        self.data_module = CricketCommentaryReportGenerationDatasetModule()\n        self.train_dataset = self.data_module.gen_data()\n        self.utils_module = CricketCommentaryReportGenerationUtils()\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_ckpt)\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_ckpt)\n\n    def prepare_training(self):\n        self.lora_model = self.utils_module.lora_model(self.model)\n        self.data_collator = self.utils_module.train_data_collator(self.tokenizer, self.model)\n        self.training_args = self.utils_module.set_training_args()\n        self.model.config.use_cache = False\n\n    def model_trainer(self):\n        self.prepare_training()\n        trainer = Seq2SeqTrainer(\n            model=self.lora_model,\n            args=self.training_args,\n            data_collator=self.data_collator,\n            train_dataset=self.train_dataset,\n        )\n        return trainer\n\n    def model_train_save_push_to_hub(self):\n        trainer = self.model_trainer()\n        print(\"Training Started\")\n        trainer.train()\n        trainer.push_to_hub()\n        trainer.model.base_model.push_to_hub(self.model_out_dir)\n        self.tokenizer.push_to_hub(self.model_out_dir)\n</pre> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainer  class CricketCommentaryReportGenerationTrainer:      def __init__(self):         self.model_ckpt = config.MODEL_CKPT         self.model_out_dir = config.MODEL_OUT_DIR         self.data_module = CricketCommentaryReportGenerationDatasetModule()         self.train_dataset = self.data_module.gen_data()         self.utils_module = CricketCommentaryReportGenerationUtils()         self.tokenizer = AutoTokenizer.from_pretrained(self.model_ckpt)         self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_ckpt)      def prepare_training(self):         self.lora_model = self.utils_module.lora_model(self.model)         self.data_collator = self.utils_module.train_data_collator(self.tokenizer, self.model)         self.training_args = self.utils_module.set_training_args()         self.model.config.use_cache = False      def model_trainer(self):         self.prepare_training()         trainer = Seq2SeqTrainer(             model=self.lora_model,             args=self.training_args,             data_collator=self.data_collator,             train_dataset=self.train_dataset,         )         return trainer      def model_train_save_push_to_hub(self):         trainer = self.model_trainer()         print(\"Training Started\")         trainer.train()         trainer.push_to_hub()         trainer.model.base_model.push_to_hub(self.model_out_dir)         self.tokenizer.push_to_hub(self.model_out_dir) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    model_trainer = CricketCommentaryReportGenerationTrainer()\n    model_trainer.model_train_save_push_to_hub()\n</pre> if __name__ == \"__main__\":     model_trainer = CricketCommentaryReportGenerationTrainer()     model_trainer.model_train_save_push_to_hub() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpprojs/003-criccommsumm/data_prep/","title":"Data prep","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport glob\n</pre> import pandas as pd import glob In\u00a0[\u00a0]: Copied! <pre>comm_paths = glob.glob(\"data/Commentaries/*.csv\")\nlen(comm_paths)\n</pre> comm_paths = glob.glob(\"data/Commentaries/*.csv\") len(comm_paths) In\u00a0[\u00a0]: Copied! <pre>report_paths = glob.glob(\"data/Reports/*.txt\")\nlen(report_paths)\n</pre> report_paths = glob.glob(\"data/Reports/*.txt\") len(report_paths) In\u00a0[\u00a0]: Copied! <pre>comm_fno = []\nfor i in comm_paths:\n    fno = int(i.split(\"\\\\\")[-1].split(\"_\")[-1].split(\".\")[0])\n    comm_fno.append(fno)\n\nreport_fno = []\nfor j in report_paths:\n    fno = int(j.split(\"\\\\\")[-1].replace(\"report\", \"\").split(\".\")[0])\n    report_fno.append(fno)\n\ncomm_fno = set(comm_fno)\nreport_fno = set(report_fno)\n</pre> comm_fno = [] for i in comm_paths:     fno = int(i.split(\"\\\\\")[-1].split(\"_\")[-1].split(\".\")[0])     comm_fno.append(fno)  report_fno = [] for j in report_paths:     fno = int(j.split(\"\\\\\")[-1].replace(\"report\", \"\").split(\".\")[0])     report_fno.append(fno)  comm_fno = set(comm_fno) report_fno = set(report_fno) In\u00a0[\u00a0]: Copied! <pre>to_read_fno = list(comm_fno.intersection(report_fno))\n</pre> to_read_fno = list(comm_fno.intersection(report_fno)) In\u00a0[\u00a0]: Copied! <pre>comm_read_paths = []\nfor i in comm_paths:\n    fno = int(i.split(\"\\\\\")[-1].split(\"_\")[-1].split(\".\")[0])\n    if fno in to_read_fno:\n        comm_read_paths.append(i)\n\nreport_read_paths = []\nfor j in report_paths:\n    fno = int(j.split(\"\\\\\")[-1].replace(\"report\", \"\").split(\".\")[0])\n    if fno in to_read_fno:\n        report_read_paths.append(j)\n</pre> comm_read_paths = [] for i in comm_paths:     fno = int(i.split(\"\\\\\")[-1].split(\"_\")[-1].split(\".\")[0])     if fno in to_read_fno:         comm_read_paths.append(i)  report_read_paths = [] for j in report_paths:     fno = int(j.split(\"\\\\\")[-1].replace(\"report\", \"\").split(\".\")[0])     if fno in to_read_fno:         report_read_paths.append(j) In\u00a0[\u00a0]: Copied! <pre>len(comm_read_paths) == len(report_read_paths)\n</pre> len(comm_read_paths) == len(report_read_paths) In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n</pre> from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\") In\u00a0[\u00a0]: Copied! <pre>def commentary_read(path):\n    test_str_lst = pd.read_csv(path)[\"Data\"].to_list()\n    comm_inp_lst = []\n    cnt = 0\n    for i in test_str_lst:\n        j = i.lower()\n        if \"run\" not in j:\n            if cnt==0 or cnt&gt;=len(test_str_lst)-3:\n                comm_inp_lst.append(i)\n            elif \" bye \" not in j and \" wide \" not in j and \"extra\" not in j:\n                if \" out \" in j:\n                    comm_inp_lst.append(i)\n                elif \"wicket\" in j:\n                    comm_inp_lst.append(i)\n            else:\n                if random.choice([0,1]) == 1:\n                    comm_inp_lst.append(i)\n        elif \" six \" in j:\n            if random.choice([0,1,0,0]) == 1:\n                comm_inp_lst.append(i)\n        cnt = cnt + 1\n    return ' '.join(comm_inp_lst)\n</pre> def commentary_read(path):     test_str_lst = pd.read_csv(path)[\"Data\"].to_list()     comm_inp_lst = []     cnt = 0     for i in test_str_lst:         j = i.lower()         if \"run\" not in j:             if cnt==0 or cnt&gt;=len(test_str_lst)-3:                 comm_inp_lst.append(i)             elif \" bye \" not in j and \" wide \" not in j and \"extra\" not in j:                 if \" out \" in j:                     comm_inp_lst.append(i)                 elif \"wicket\" in j:                     comm_inp_lst.append(i)             else:                 if random.choice([0,1]) == 1:                     comm_inp_lst.append(i)         elif \" six \" in j:             if random.choice([0,1,0,0]) == 1:                 comm_inp_lst.append(i)         cnt = cnt + 1     return ' '.join(comm_inp_lst) In\u00a0[\u00a0]: Copied! <pre>def report_read(path):\n    with open(path) as f:\n        report_str = f.read()\n    return report_str\n</pre> def report_read(path):     with open(path) as f:         report_str = f.read()     return report_str In\u00a0[\u00a0]: Copied! <pre>final_commentaries = []\nfinal_reports = []\nfor i in comm_read_paths:\n    final_commentaries.append(commentary_read(i))\nfor j in report_read_paths:\n    final_reports.append(report_read(j))\n</pre> final_commentaries = [] final_reports = [] for i in comm_read_paths:     final_commentaries.append(commentary_read(i)) for j in report_read_paths:     final_reports.append(report_read(j)) In\u00a0[\u00a0]: Copied! <pre>final_df = pd.DataFrame({\n    \"Commentary_Highlights\" : final_commentaries,\n    \"Match_Report\" : final_reports\n})\nfinal_df\n</pre> final_df = pd.DataFrame({     \"Commentary_Highlights\" : final_commentaries,     \"Match_Report\" : final_reports }) final_df In\u00a0[\u00a0]: Copied! <pre>final_df.to_parquet(\"data/match_report_gen.parquet\", index=False)\nfinal_df.to_csv(\"data/match_report_gen.csv\", index=False)\n</pre> final_df.to_parquet(\"data/match_report_gen.parquet\", index=False) final_df.to_csv(\"data/match_report_gen.csv\", index=False) In\u00a0[\u00a0]: Copied! <pre>final_df\n</pre> final_df In\u00a0[\u00a0]: Copied! <pre>final_df[\"src_len\"] = final_df[\"Commentary_Highlights\"].map(lambda x : len(tokenizer(x)[\"input_ids\"]))\nfinal_df[\"tgt_len\"] = final_df[\"Match_Report\"].map(lambda x : len(tokenizer(x)[\"input_ids\"]))\n</pre> final_df[\"src_len\"] = final_df[\"Commentary_Highlights\"].map(lambda x : len(tokenizer(x)[\"input_ids\"])) final_df[\"tgt_len\"] = final_df[\"Match_Report\"].map(lambda x : len(tokenizer(x)[\"input_ids\"])) In\u00a0[\u00a0]: Copied! <pre>final_df\n</pre> final_df In\u00a0[\u00a0]: Copied! <pre>final_df[\"Commentary_Highlights\"][0]\n</pre> final_df[\"Commentary_Highlights\"][0] In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\nsns.distplot(final_df[\"src_len\"])\n</pre> import seaborn as sns sns.distplot(final_df[\"src_len\"]) In\u00a0[\u00a0]: Copied! <pre>final_df.describe()\n</pre> final_df.describe() In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\ndf = pd.read_excel(\"data_new_T5.xlsx\")\n</pre> import pandas as pd df = pd.read_excel(\"data_new_T5.xlsx\") In\u00a0[\u00a0]: Copied! <pre>df.iloc[0]\n</pre> df.iloc[0] In\u00a0[\u00a0]: Copied! <pre>df[\"src_len\"] = df[\"input_text\"].map(lambda x : len(tokenizer(x)[\"input_ids\"]))\ndf[\"tgt_len\"] = df[\"target_text\"].map(lambda x : len(tokenizer(x)[\"input_ids\"]))\n</pre> df[\"src_len\"] = df[\"input_text\"].map(lambda x : len(tokenizer(x)[\"input_ids\"])) df[\"tgt_len\"] = df[\"target_text\"].map(lambda x : len(tokenizer(x)[\"input_ids\"])) In\u00a0[\u00a0]: Copied! <pre>df.describe()\n</pre> df.describe() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpprojs/004-key-ext-bert/BERT_NER/","title":"Imports and Config","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install transformers datasets evaluate seqeval accelerate\n</pre> ! pip install transformers datasets evaluate seqeval accelerate In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\nimport numpy as np\nimport evaluate\n</pre> from datasets import load_dataset from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer import numpy as np import evaluate In\u00a0[\u00a0]: Copied! <pre>class NERDataset:\n\n    def __init__(self, data_id, tokenizer_ckpt):\n        self.data_id = data_id\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)\n\n    def load_data(self):\n        self.dataset = load_dataset(self.data_id)\n        self.train = self.dataset[\"train\"]\n        self.test = self.dataset[\"test\"]\n        ner_feature = self.dataset[\"train\"].features[\"ner_tags\"]\n        label_names = ner_feature.feature.names\n        return self.train, self.test, label_names\n\n    def align_labels_with_tokens(self, labels, word_ids):\n        new_labels = []\n        current_word = None\n        for word_id in word_ids:\n            if word_id != current_word: \n                current_word = word_id\n                try:\n                    label = -100 if word_id is None else labels[word_id]\n                except:\n                    label = -100\n                new_labels.append(label)\n            elif word_id is None:\n                new_labels.append(-100)\n            else:\n                label = labels[word_id]\n                if label % 2 == 1:\n                    label += 1\n                new_labels.append(label)\n\n        return new_labels\n\n    def preprocess_function(self, examples):\n        tokenized_inputs = self.tokenizer(\n            examples[\"tokens\"], truncation=True, is_split_into_words=True\n        )\n        all_labels = examples[\"ner_tags\"]\n        new_labels = []\n        for i, labels in enumerate(all_labels):\n            word_ids = tokenized_inputs.word_ids(i)\n            new_labels.append(self.align_labels_with_tokens(labels, word_ids))\n\n        tokenized_inputs[\"labels\"] = new_labels\n        return tokenized_inputs\n\n    def create_data(self):\n\n        self.train, self.test, label_names = self.load_data()\n\n        tokenized_train_dataset = self.train.map(\n            self.preprocess_function,\n            batched=True,\n            remove_columns=self.train.column_names\n        )\n\n        tokenized_test_dataset = self.test.map(\n            self.preprocess_function,\n            batched=True,\n            remove_columns=self.train.column_names\n        )\n\n        return tokenized_train_dataset, tokenized_test_dataset, label_names\n</pre> class NERDataset:      def __init__(self, data_id, tokenizer_ckpt):         self.data_id = data_id         self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)      def load_data(self):         self.dataset = load_dataset(self.data_id)         self.train = self.dataset[\"train\"]         self.test = self.dataset[\"test\"]         ner_feature = self.dataset[\"train\"].features[\"ner_tags\"]         label_names = ner_feature.feature.names         return self.train, self.test, label_names      def align_labels_with_tokens(self, labels, word_ids):         new_labels = []         current_word = None         for word_id in word_ids:             if word_id != current_word:                  current_word = word_id                 try:                     label = -100 if word_id is None else labels[word_id]                 except:                     label = -100                 new_labels.append(label)             elif word_id is None:                 new_labels.append(-100)             else:                 label = labels[word_id]                 if label % 2 == 1:                     label += 1                 new_labels.append(label)          return new_labels      def preprocess_function(self, examples):         tokenized_inputs = self.tokenizer(             examples[\"tokens\"], truncation=True, is_split_into_words=True         )         all_labels = examples[\"ner_tags\"]         new_labels = []         for i, labels in enumerate(all_labels):             word_ids = tokenized_inputs.word_ids(i)             new_labels.append(self.align_labels_with_tokens(labels, word_ids))          tokenized_inputs[\"labels\"] = new_labels         return tokenized_inputs      def create_data(self):          self.train, self.test, label_names = self.load_data()          tokenized_train_dataset = self.train.map(             self.preprocess_function,             batched=True,             remove_columns=self.train.column_names         )          tokenized_test_dataset = self.test.map(             self.preprocess_function,             batched=True,             remove_columns=self.train.column_names         )          return tokenized_train_dataset, tokenized_test_dataset, label_names <p>If notebook:</p> <pre>from huggingface_hub import notebook_login\n\nnotebook_login()\n</pre> <p>If script</p> <pre>huggingface-cli login\n</pre> In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\n\nnotebook_login()\n</pre> from huggingface_hub import notebook_login  notebook_login() In\u00a0[\u00a0]: Copied! <pre>class NERTrainer:\n\n    def __init__(self):\n        self.nerdataset = NERDataset(\"conll2003\", \"bert-base-cased\")\n        self.train_data, self.test_data, self.ner_labels = self.nerdataset.create_data()\n        self.id2label = {i: label for i, label in enumerate(self.ner_labels)}\n        self.label2id = {v: k for k, v in self.id2label.items()}\n        self.model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", id2label=self.id2label, label2id= self.label2id)\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n    def compute_metrics(self, eval_preds):\n        metric = evaluate.load(\"seqeval\")\n        logits, labels = eval_preds\n        predictions = np.argmax(logits, axis=-1)\n\n        true_labels = [[self.ner_labels[l] for l in label if l != -100] for label in labels]\n        true_predictions = [\n            [self.ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n        return {\n            \"precision\": all_metrics[\"overall_precision\"],\n            \"recall\": all_metrics[\"overall_recall\"],\n            \"f1\": all_metrics[\"overall_f1\"],\n            \"accuracy\": all_metrics[\"overall_accuracy\"],\n        }\n\n    def set_training_args(self):\n        return TrainingArguments(\n        output_dir=\"bert-ner-custom\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=2e-5,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        push_to_hub=True\n    )\n\n    def train_and_save_model(self):\n        trainer = Trainer(\n            model=self.model,\n            args=self.set_training_args(),\n            train_dataset=self.train_data,\n            eval_dataset=self.test_data,\n            data_collator=DataCollatorForTokenClassification(tokenizer=self.tokenizer),\n            compute_metrics=self.compute_metrics,\n            tokenizer=self.tokenizer,\n        )\n        trainer.train()\n\nnertrainer = NERTrainer()\nnertrainer.train_and_save_model()\n</pre> class NERTrainer:      def __init__(self):         self.nerdataset = NERDataset(\"conll2003\", \"bert-base-cased\")         self.train_data, self.test_data, self.ner_labels = self.nerdataset.create_data()         self.id2label = {i: label for i, label in enumerate(self.ner_labels)}         self.label2id = {v: k for k, v in self.id2label.items()}         self.model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", id2label=self.id2label, label2id= self.label2id)         self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")      def compute_metrics(self, eval_preds):         metric = evaluate.load(\"seqeval\")         logits, labels = eval_preds         predictions = np.argmax(logits, axis=-1)          true_labels = [[self.ner_labels[l] for l in label if l != -100] for label in labels]         true_predictions = [             [self.ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]             for prediction, label in zip(predictions, labels)         ]         all_metrics = metric.compute(predictions=true_predictions, references=true_labels)         return {             \"precision\": all_metrics[\"overall_precision\"],             \"recall\": all_metrics[\"overall_recall\"],             \"f1\": all_metrics[\"overall_f1\"],             \"accuracy\": all_metrics[\"overall_accuracy\"],         }      def set_training_args(self):         return TrainingArguments(         output_dir=\"bert-ner-custom\",         evaluation_strategy=\"epoch\",         save_strategy=\"epoch\",         learning_rate=2e-5,         num_train_epochs=3,         weight_decay=0.01,         push_to_hub=True     )      def train_and_save_model(self):         trainer = Trainer(             model=self.model,             args=self.set_training_args(),             train_dataset=self.train_data,             eval_dataset=self.test_data,             data_collator=DataCollatorForTokenClassification(tokenizer=self.tokenizer),             compute_metrics=self.compute_metrics,             tokenizer=self.tokenizer,         )         trainer.train()  nertrainer = NERTrainer() nertrainer.train_and_save_model() In\u00a0[\u00a0]: Copied! <pre>from transformers import pipeline\n\nmodel_checkpoint = \"Vasanth/bert-ner-custom\"\ntoken_classifier = pipeline(\n    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n)\ntoken_classifier(\"Vasanth lives in Chennai.\")\n</pre> from transformers import pipeline  model_checkpoint = \"Vasanth/bert-ner-custom\" token_classifier = pipeline(     \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\" ) token_classifier(\"Vasanth lives in Chennai.\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpprojs/004-key-ext-bert/BERT_NER/#imports-and-config","title":"Imports and Config\u00b6","text":""},{"location":"res/nlpprojs/004-key-ext-bert/BERT_NER/#dataset","title":"Dataset\u00b6","text":""},{"location":"res/nlpprojs/004-key-ext-bert/BERT_NER/#model-training-make-sure-to-do-login","title":"Model Training - Make sure to do login\u00b6","text":""},{"location":"res/nlpprojs/004-key-ext-bert/BERT_NER/#inference","title":"Inference\u00b6","text":""},{"location":"res/nlpprojs/004-key-ext-bert/Custom_BERT_NER/","title":"Prepare Data for NER","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install transformers datasets evaluate seqeval accelerate\n</pre> ! pip install transformers datasets evaluate seqeval accelerate In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport json\n</pre> import pandas as pd from sklearn.model_selection import train_test_split import json In\u00a0[\u00a0]: Copied! <pre>data = pd.read_csv(\"ner.csv\")\ndata\n</pre> data = pd.read_csv(\"ner.csv\") data In\u00a0[\u00a0]: Copied! <pre>unique_ner_tags = ['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',\n       'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',\n       'I-eve', 'I-nat']\n\ntags2id = {}\nfor i, tag in enumerate(unique_ner_tags):\n    tags2id[tag] = i\ntags2id\n</pre> unique_ner_tags = ['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim',        'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve',        'I-eve', 'I-nat']  tags2id = {} for i, tag in enumerate(unique_ner_tags):     tags2id[tag] = i tags2id In\u00a0[\u00a0]: Copied! <pre>from ast import literal_eval\ndata[\"Tags\"] = data[\"Tag\"].apply(lambda x: literal_eval(x)) # Converting list in string format to list\ndata\n</pre> from ast import literal_eval data[\"Tags\"] = data[\"Tag\"].apply(lambda x: literal_eval(x)) # Converting list in string format to list data In\u00a0[\u00a0]: Copied! <pre>def map_tags2id(x):\n    return [tags2id[i] for i in x]\ndata[\"ner_tags\"] = data[\"Tags\"].apply(lambda x: map_tags2id(x))\ndata\n</pre> def map_tags2id(x):     return [tags2id[i] for i in x] data[\"ner_tags\"] = data[\"Tags\"].apply(lambda x: map_tags2id(x)) data In\u00a0[\u00a0]: Copied! <pre>data[\"tokens\"] = data[\"Sentence\"].apply(lambda x: x.split())\ndata\n</pre> data[\"tokens\"] = data[\"Sentence\"].apply(lambda x: x.split()) data In\u00a0[\u00a0]: Copied! <pre>data = data[[\"tokens\", \"ner_tags\"]]\ntrain_data, test_data = train_test_split(data, test_size=0.2, shuffle=True, random_state=0)\n</pre> data = data[[\"tokens\", \"ner_tags\"]] train_data, test_data = train_test_split(data, test_size=0.2, shuffle=True, random_state=0) In\u00a0[\u00a0]: Copied! <pre>train_data\n</pre> train_data In\u00a0[\u00a0]: Copied! <pre>train_data.dropna(inplace=True)\n</pre> train_data.dropna(inplace=True) In\u00a0[\u00a0]: Copied! <pre>test_data.dropna(inplace=True)\n</pre> test_data.dropna(inplace=True) In\u00a0[\u00a0]: Copied! <pre>train_data\n</pre> train_data In\u00a0[\u00a0]: Copied! <pre>tags2id[\"ner_categories\"] = unique_ner_tags\n</pre> tags2id[\"ner_categories\"] = unique_ner_tags In\u00a0[\u00a0]: Copied! <pre>train_data.to_parquet(\"train_ner.parquet\", index=False)\ntest_data.to_parquet(\"test_ner.parquet\", index=False)\n\nwith open(\"tags.json\", \"w\") as outfile:\n    json.dump(tags2id, outfile)\n</pre> train_data.to_parquet(\"train_ner.parquet\", index=False) test_data.to_parquet(\"test_ner.parquet\", index=False)  with open(\"tags.json\", \"w\") as outfile:     json.dump(tags2id, outfile) In\u00a0[\u00a0]: Copied! <pre>with open(\"tags.json\", \"r\") as outfile:\n    print(json.load(outfile))\n</pre> with open(\"tags.json\", \"r\") as outfile:     print(json.load(outfile)) In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\nimport numpy as np\nimport evaluate\nimport json\n</pre> from datasets import load_dataset from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer import numpy as np import evaluate import json In\u00a0[\u00a0]: Copied! <pre>class NERDataset:\n\n    def __init__(self, train_data_path, test_data_path, tokenizer_ckpt):\n        self.train_data_path = train_data_path\n        self.test_data_path = test_data_path\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)\n\n    def load_data(self):\n        self.train = load_dataset(\"parquet\", data_files = self.train_data_path)[\"train\"]\n        self.test = load_dataset(\"parquet\", data_files = self.test_data_path)[\"train\"]\n\n    def align_labels_with_tokens(self, labels, word_ids):\n        new_labels = []\n        current_word = None\n        for word_id in word_ids:\n            if word_id != current_word:\n                # Start of a new word!\n                current_word = word_id\n                try:\n                    label = -100 if word_id is None else labels[word_id]\n                except:\n                    label = -100\n                new_labels.append(label)\n            elif word_id is None:\n                # Special token\n                new_labels.append(-100)\n            else:\n                # Same word as previous token\n                label = labels[word_id]\n                # If the label is B-XXX we change it to I-XXX\n                if label % 2 == 1:\n                    label += 1\n                new_labels.append(label)\n\n        return new_labels\n\n    def preprocess_function(self, examples):\n        tokenized_inputs = self.tokenizer(\n        examples[\"tokens\"], truncation=True, is_split_into_words=True)\n        all_labels = examples[\"ner_tags\"]\n        new_labels = []\n        for i, labels in enumerate(all_labels):\n            word_ids = tokenized_inputs.word_ids(i)\n            new_labels.append(self.align_labels_with_tokens(labels, word_ids))\n\n        tokenized_inputs[\"labels\"] = new_labels\n        return tokenized_inputs\n\n    def create_data(self):\n\n        self.load_data()\n\n        tokenized_train_dataset = self.train.map(\n            self.preprocess_function,\n            batched=True,\n            remove_columns=self.train.column_names\n        )\n\n        tokenized_test_dataset = self.test.map(\n            self.preprocess_function,\n            batched=True,\n            remove_columns=self.train.column_names\n        )\n\n        return tokenized_train_dataset, tokenized_test_dataset\n</pre> class NERDataset:      def __init__(self, train_data_path, test_data_path, tokenizer_ckpt):         self.train_data_path = train_data_path         self.test_data_path = test_data_path         self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_ckpt)      def load_data(self):         self.train = load_dataset(\"parquet\", data_files = self.train_data_path)[\"train\"]         self.test = load_dataset(\"parquet\", data_files = self.test_data_path)[\"train\"]      def align_labels_with_tokens(self, labels, word_ids):         new_labels = []         current_word = None         for word_id in word_ids:             if word_id != current_word:                 # Start of a new word!                 current_word = word_id                 try:                     label = -100 if word_id is None else labels[word_id]                 except:                     label = -100                 new_labels.append(label)             elif word_id is None:                 # Special token                 new_labels.append(-100)             else:                 # Same word as previous token                 label = labels[word_id]                 # If the label is B-XXX we change it to I-XXX                 if label % 2 == 1:                     label += 1                 new_labels.append(label)          return new_labels      def preprocess_function(self, examples):         tokenized_inputs = self.tokenizer(         examples[\"tokens\"], truncation=True, is_split_into_words=True)         all_labels = examples[\"ner_tags\"]         new_labels = []         for i, labels in enumerate(all_labels):             word_ids = tokenized_inputs.word_ids(i)             new_labels.append(self.align_labels_with_tokens(labels, word_ids))          tokenized_inputs[\"labels\"] = new_labels         return tokenized_inputs      def create_data(self):          self.load_data()          tokenized_train_dataset = self.train.map(             self.preprocess_function,             batched=True,             remove_columns=self.train.column_names         )          tokenized_test_dataset = self.test.map(             self.preprocess_function,             batched=True,             remove_columns=self.train.column_names         )          return tokenized_train_dataset, tokenized_test_dataset <p>If notebook:</p> <pre>from huggingface_hub import notebook_login\n\nnotebook_login()\n</pre> <p>If script</p> <pre>huggingface-cli login\n</pre> In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\n\nnotebook_login()\n</pre> from huggingface_hub import notebook_login  notebook_login() In\u00a0[\u00a0]: Copied! <pre>class NERTrainer:\n\n    def __init__(self):\n        with open(\"tags.json\", \"r\") as outfile:\n            self.ner_labels = json.load(outfile)[\"ner_categories\"]\n\n        self.nerdataset = NERDataset(\"/content/train_ner.parquet\", \"/content/test_ner.parquet\", \"bert-base-uncased\")\n        self.train_data, self.test_data = self.nerdataset.create_data()\n        self.id2label = {i: label for i, label in enumerate(self.ner_labels)}\n        self.label2id = {v: k for k, v in self.id2label.items()}\n        self.model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", id2label=self.id2label, label2id= self.label2id)\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n    def compute_metrics(self, eval_preds):\n        metric = evaluate.load(\"seqeval\")\n        logits, labels = eval_preds\n        predictions = np.argmax(logits, axis=-1)\n\n        # Remove ignored index (special tokens) and convert to labels\n        true_labels = [[self.ner_labels[l] for l in label if l != -100] for label in labels]\n        true_predictions = [\n            [self.ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n        all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n        return {\n            \"precision\": all_metrics[\"overall_precision\"],\n            \"recall\": all_metrics[\"overall_recall\"],\n            \"f1\": all_metrics[\"overall_f1\"],\n            \"accuracy\": all_metrics[\"overall_accuracy\"],\n        }\n\n    def set_training_args(self):\n        return TrainingArguments(\n        output_dir=\"bert-ner-custom\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        learning_rate=2e-5,\n        num_train_epochs=3,\n        weight_decay=0.01,\n        push_to_hub=True\n    )\n\n    def train_and_save_model(self):\n        trainer = Trainer(\n            model=self.model,\n            args=self.set_training_args(),\n            train_dataset=self.train_data,\n            eval_dataset=self.test_data,\n            data_collator=DataCollatorForTokenClassification(tokenizer=self.tokenizer),\n            compute_metrics=self.compute_metrics,\n            tokenizer=self.tokenizer,\n        )\n        trainer.train()\n\nnertrainer = NERTrainer()\nnertrainer.train_and_save_model()\n</pre> class NERTrainer:      def __init__(self):         with open(\"tags.json\", \"r\") as outfile:             self.ner_labels = json.load(outfile)[\"ner_categories\"]          self.nerdataset = NERDataset(\"/content/train_ner.parquet\", \"/content/test_ner.parquet\", \"bert-base-uncased\")         self.train_data, self.test_data = self.nerdataset.create_data()         self.id2label = {i: label for i, label in enumerate(self.ner_labels)}         self.label2id = {v: k for k, v in self.id2label.items()}         self.model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", id2label=self.id2label, label2id= self.label2id)         self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")      def compute_metrics(self, eval_preds):         metric = evaluate.load(\"seqeval\")         logits, labels = eval_preds         predictions = np.argmax(logits, axis=-1)          # Remove ignored index (special tokens) and convert to labels         true_labels = [[self.ner_labels[l] for l in label if l != -100] for label in labels]         true_predictions = [             [self.ner_labels[p] for (p, l) in zip(prediction, label) if l != -100]             for prediction, label in zip(predictions, labels)         ]         all_metrics = metric.compute(predictions=true_predictions, references=true_labels)         return {             \"precision\": all_metrics[\"overall_precision\"],             \"recall\": all_metrics[\"overall_recall\"],             \"f1\": all_metrics[\"overall_f1\"],             \"accuracy\": all_metrics[\"overall_accuracy\"],         }      def set_training_args(self):         return TrainingArguments(         output_dir=\"bert-ner-custom\",         evaluation_strategy=\"epoch\",         save_strategy=\"epoch\",         learning_rate=2e-5,         num_train_epochs=3,         weight_decay=0.01,         push_to_hub=True     )      def train_and_save_model(self):         trainer = Trainer(             model=self.model,             args=self.set_training_args(),             train_dataset=self.train_data,             eval_dataset=self.test_data,             data_collator=DataCollatorForTokenClassification(tokenizer=self.tokenizer),             compute_metrics=self.compute_metrics,             tokenizer=self.tokenizer,         )         trainer.train()  nertrainer = NERTrainer() nertrainer.train_and_save_model() In\u00a0[\u00a0]: Copied! <pre>from transformers import pipeline\n\n# Replace this with your own checkpoint\nmodel_checkpoint = \"Vasanth/bert-ner-custom\"\ntoken_classifier = pipeline(\n    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n)\ntoken_classifier(\"I live in Chennai.\")\n</pre> from transformers import pipeline  # Replace this with your own checkpoint model_checkpoint = \"Vasanth/bert-ner-custom\" token_classifier = pipeline(     \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\" ) token_classifier(\"I live in Chennai.\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpprojs/004-key-ext-bert/Custom_BERT_NER/#prepare-data-for-ner","title":"Prepare Data for NER\u00b6","text":""},{"location":"res/nlpprojs/004-key-ext-bert/Custom_BERT_NER/#imports-and-config","title":"Imports and Config\u00b6","text":""},{"location":"res/nlpprojs/004-key-ext-bert/Custom_BERT_NER/#dataset","title":"Dataset\u00b6","text":""},{"location":"res/nlpprojs/004-key-ext-bert/Custom_BERT_NER/#model-training-make-sure-to-do-login","title":"Model Training - Make sure to do login\u00b6","text":""},{"location":"res/nlpprojs/004-key-ext-bert/Custom_BERT_NER/#inference","title":"Inference\u00b6","text":""},{"location":"res/nlpresearchpapers/001_LoRA/Chatbot/","title":"Chatbot","text":"In\u00a0[\u00a0]: Copied! <pre>! nvidia-smi\n</pre> ! nvidia-smi In\u00a0[\u00a0]: Copied! <pre>from google.colab import drive\ndrive.mount('/content/drive')\n</pre> from google.colab import drive drive.mount('/content/drive') In\u00a0[\u00a0]: Copied! <pre>! pip install transformers datasets accelerate peft\n</pre> ! pip install transformers datasets accelerate peft In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom peft import LoraConfig, get_peft_model, TaskType\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n</pre> import pandas as pd from datasets import Dataset from transformers import AutoTokenizer, AutoModelForSeq2SeqLM from peft import LoraConfig, get_peft_model, TaskType from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq In\u00a0[\u00a0]: Copied! <pre>train_df = pd.read_parquet(\"train.parquet\")\ntest_df = pd.read_parquet(\"test.parquet\")\ntrain_data = Dataset.from_pandas(train_df)\ntest_data = Dataset.from_pandas(test_df)\n</pre> train_df = pd.read_parquet(\"train.parquet\") test_df = pd.read_parquet(\"test.parquet\") train_data = Dataset.from_pandas(train_df) test_data = Dataset.from_pandas(test_df) In\u00a0[\u00a0]: Copied! <pre>model_id=\"google/flan-t5-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n</pre> model_id=\"google/flan-t5-large\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForSeq2SeqLM.from_pretrained(model_id) In\u00a0[\u00a0]: Copied! <pre>def preprocess_function(sample,padding=\"max_length\"):\n    model_inputs = tokenizer(sample[\"Human\"], max_length=256, padding=padding, truncation=True)\n    labels = tokenizer(sample[\"Assistant\"], max_length=256, padding=padding, truncation=True)\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n</pre> def preprocess_function(sample,padding=\"max_length\"):     model_inputs = tokenizer(sample[\"Human\"], max_length=256, padding=padding, truncation=True)     labels = tokenizer(sample[\"Assistant\"], max_length=256, padding=padding, truncation=True)     if padding == \"max_length\":         labels[\"input_ids\"] = [             [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]         ]     model_inputs[\"labels\"] = labels[\"input_ids\"]     return model_inputs In\u00a0[\u00a0]: Copied! <pre>train_tokenized_dataset = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\ntest_tokenized_dataset = test_data.map(preprocess_function, batched=True, remove_columns=test_data.column_names)\nprint(f\"Keys of tokenized dataset: {list(train_tokenized_dataset.features)}\")\n</pre> train_tokenized_dataset = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names) test_tokenized_dataset = test_data.map(preprocess_function, batched=True, remove_columns=test_data.column_names) print(f\"Keys of tokenized dataset: {list(train_tokenized_dataset.features)}\") In\u00a0[\u00a0]: Copied! <pre>lora_config = LoraConfig(\n r=16,\n lora_alpha=32,\n target_modules=[\"q\", \"v\"],\n lora_dropout=0.1,\n bias=\"none\",\n task_type=TaskType.SEQ_2_SEQ_LM\n)\n</pre> lora_config = LoraConfig(  r=16,  lora_alpha=32,  target_modules=[\"q\", \"v\"],  lora_dropout=0.1,  bias=\"none\",  task_type=TaskType.SEQ_2_SEQ_LM ) In\u00a0[\u00a0]: Copied! <pre>model = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n</pre> model = get_peft_model(model, lora_config) model.print_trainable_parameters() In\u00a0[\u00a0]: Copied! <pre>label_pad_token_id = -100\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=label_pad_token_id,\n    pad_to_multiple_of=8\n)\n</pre> label_pad_token_id = -100 data_collator = DataCollatorForSeq2Seq(     tokenizer,     model=model,     label_pad_token_id=label_pad_token_id,     pad_to_multiple_of=8 ) In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>output_dir=\"lora-flan-t5-large-chat\"\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=4,\n    learning_rate=1e-3,\n    num_train_epochs=1,\n    logging_dir=f\"{output_dir}/logs\",\n    logging_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    report_to=\"tensorboard\",\n    push_to_hub = True\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_tokenized_dataset,\n)\nmodel.config.use_cache = False\n\ntrainer.train()\npeft_save_model_id=\"lora-flan-t5-large-chat\"\ntrainer.model.save_pretrained(peft_save_model_id, push_to_hub=True)\ntokenizer.save_pretrained(peft_save_model_id, push_to_hub=True)\ntrainer.model.base_model.save_pretrained(peft_save_model_id, push_to_hub=True)\n</pre> output_dir=\"lora-flan-t5-large-chat\" training_args = Seq2SeqTrainingArguments(     output_dir=output_dir,     per_device_train_batch_size=4,     learning_rate=1e-3,     num_train_epochs=1,     logging_dir=f\"{output_dir}/logs\",     logging_strategy=\"epoch\",     save_strategy=\"epoch\",     report_to=\"tensorboard\",     push_to_hub = True )  trainer = Seq2SeqTrainer(     model=model,     args=training_args,     data_collator=data_collator,     train_dataset=train_tokenized_dataset, ) model.config.use_cache = False  trainer.train() peft_save_model_id=\"lora-flan-t5-large-chat\" trainer.model.save_pretrained(peft_save_model_id, push_to_hub=True) tokenizer.save_pretrained(peft_save_model_id, push_to_hub=True) trainer.model.base_model.save_pretrained(peft_save_model_id, push_to_hub=True) In\u00a0[\u00a0]: Copied! <pre>! cp -r /content/lora-flan-t5-large-chat/ /content/drive/MyDrive/Chatbot/\n</pre> ! cp -r /content/lora-flan-t5-large-chat/ /content/drive/MyDrive/Chatbot/ In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# Load peft config for pre-trained checkpoint etc.\npeft_model_id = \"lora-flan-t5-large-chat\"\nconfig = PeftConfig.from_pretrained(peft_model_id)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\nmodel = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0}).cuda()\nmodel.eval()\n\nsample = \"Human: \\nExplain me about the working of Artificial Intelligence. \\nAssistant: \"\ninput_ids = tokenizer(sample, return_tensors=\"pt\", truncation=True, max_length=256).input_ids.cuda()\noutputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9, max_length=256)\nprint(f\"{sample}\")\n\nprint(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])\n</pre> import torch from peft import PeftModel, PeftConfig from transformers import AutoModelForSeq2SeqLM, AutoTokenizer  # Load peft config for pre-trained checkpoint etc. peft_model_id = \"lora-flan-t5-large-chat\" config = PeftConfig.from_pretrained(peft_model_id)  model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path) tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)  model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0}).cuda() model.eval()  sample = \"Human: \\nExplain me about the working of Artificial Intelligence. \\nAssistant: \" input_ids = tokenizer(sample, return_tensors=\"pt\", truncation=True, max_length=256).input_ids.cuda() outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9, max_length=256) print(f\"{sample}\")  print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpresearchpapers/001_LoRA/data/","title":"Data","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom datasets import load_dataset\n</pre> import pandas as pd from datasets import load_dataset In\u00a0[\u00a0]: Copied! <pre>data = load_dataset(\"Fredithefish/Instruction-Tuning-with-GPT-4-RedPajama-Chat\")\ndata\n</pre> data = load_dataset(\"Fredithefish/Instruction-Tuning-with-GPT-4-RedPajama-Chat\") data In\u00a0[\u00a0]: Copied! <pre>df = data[\"train\"].to_pandas()\ndf\n</pre> df = data[\"train\"].to_pandas() df In\u00a0[\u00a0]: Copied! <pre>df[\"Assistant\"] = df[\"text\"].apply(lambda x: x.split(\"&lt;bot&gt;:\")[-1])\ndf[\"Human\"] = df[\"text\"].apply(lambda x: \"Human:\" + x.split(\"&lt;bot&gt;:\")[0].replace(\"&lt;human&gt;:\", \"\").replace(\"\\n\", \"\") + \". Assistant: \")\ndf\n</pre> df[\"Assistant\"] = df[\"text\"].apply(lambda x: x.split(\":\")[-1]) df[\"Human\"] = df[\"text\"].apply(lambda x: \"Human:\" + x.split(\":\")[0].replace(\":\", \"\").replace(\"\\n\", \"\") + \". Assistant: \") df In\u00a0[\u00a0]: Copied! <pre>df = df.sample(20000)\ndf\n</pre> df = df.sample(20000) df In\u00a0[\u00a0]: Copied! <pre>df = df[[\"Human\", \"Assistant\"]]\n</pre> df = df[[\"Human\", \"Assistant\"]] In\u00a0[\u00a0]: Copied! <pre>df.iloc[0][\"Human\"]\n</pre> df.iloc[0][\"Human\"] In\u00a0[\u00a0]: Copied! <pre>df[:15000].to_parquet(\"train.parquet\", index=False)\ndf[15000:].to_parquet(\"test.parquet\", index=False)\n</pre> df[:15000].to_parquet(\"train.parquet\", index=False) df[15000:].to_parquet(\"test.parquet\", index=False) In\u00a0[\u00a0]: Copied! <pre>df.to_parquet(\"chat_data.parquet\", index=False)\n</pre> df.to_parquet(\"chat_data.parquet\", index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpresearchpapers/003_DPO_Implementation/DPO_Part1_SFT/","title":"DPO Part1 SFT","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install transformers trl peft accelerate datasets bitsandbytes\n</pre> ! pip install transformers trl peft accelerate datasets bitsandbytes In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom dataclasses import dataclass, field\nfrom typing import Optional\n\nimport torch\nfrom datasets import load_dataset\nfrom peft import AutoPeftModelForCausalLM, LoraConfig\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments\n\nfrom trl import SFTTrainer\nfrom trl.trainer import ConstantLengthDataset\n\n\ndef chars_token_ratio(dataset, tokenizer, nb_examples=400):\n\"\"\"\n    Estimate the average number of characters per token in the dataset.\n    \"\"\"\n    total_characters, total_tokens = 0, 0\n    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n        text = prepare_sample_text(example)\n        total_characters += len(text)\n        if tokenizer.is_fast:\n            total_tokens += len(tokenizer(text).tokens())\n        else:\n            total_tokens += len(tokenizer.tokenize(text))\n\n    return total_characters / total_tokens\n\n\ndef print_trainable_parameters(model):\n\"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        if param.requires_grad:\n            trainable_params += param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )\n\n\ndef prepare_sample_text(example):\n\"\"\"Prepare the text from a sample of the dataset.\"\"\"\n    text = f\"{example['prompt']}\\n {example['response']}\"\n    return text\n\n\ndef create_datasets(tokenizer):\n    dataset = load_dataset(\n        \"Dahoas/full-hh-rlhf\",\n        split = \"test\",\n        use_auth_token=True\n    )\n    dataset = dataset.train_test_split(test_size=0.005, seed=None)\n    train_data = dataset[\"train\"]\n    valid_data = dataset[\"test\"]\n    print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")\n\n    chars_per_token = chars_token_ratio(train_data, tokenizer)\n    print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")\n\n    train_dataset = ConstantLengthDataset(\n        tokenizer,\n        train_data,\n        formatting_func=prepare_sample_text,\n        infinite=True,\n        seq_length=256,\n        chars_per_token=chars_per_token,\n    )\n    valid_dataset = ConstantLengthDataset(\n        tokenizer,\n        valid_data,\n        formatting_func=prepare_sample_text,\n        infinite=False,\n        seq_length=256,\n        chars_per_token=chars_per_token,\n    )\n    return train_dataset, valid_dataset\n\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"TabbyML/SantaCoder-1B\",\n    quantization_config=bnb_config,\n    device_map={\"\": 0},\n    trust_remote_code=True,\n    use_auth_token=True,\n)\nbase_model.config.use_cache = False\n\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\"c_attn\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"TabbyML/SantaCoder-1B\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"sft_santacoder1b\",\n    per_device_train_batch_size=10,\n    gradient_accumulation_steps=2,\n    per_device_eval_batch_size=10,\n    learning_rate=2e-4,\n    logging_steps=10,\n    max_steps=105,\n    report_to=\"tensorboard\",\n    save_steps=100,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=1,\n    optim=\"paged_adamw_32bit\",\n    fp16=True,\n    remove_unused_columns=False,\n    run_name=\"sft_santacoder1b\",\n)\n\ntrain_dataset, eval_dataset = create_datasets(tokenizer)\n\ntrainer = SFTTrainer(\n    model=base_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    peft_config=peft_config,\n    packing=True,\n    max_seq_length=None,\n    tokenizer=tokenizer,\n    args=training_args,\n)\ntrainer.train()\ntrainer.save_model(\"sft_santacoder1b\")\n\noutput_dir = os.path.join(\"sft_santacoder1b\", \"final_checkpoint\")\ntrainer.model.save_pretrained(output_dir)\n\n# Free memory for merging weights\ndel base_model\ntorch.cuda.empty_cache()\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.float16)\nmodel = model.merge_and_unload()\n\noutput_merged_dir = os.path.join(\"sft_santacoder1b\", \"final_merged_checkpoint\")\nmodel.save_pretrained(output_merged_dir, safe_serialization=True)\n</pre> import os from dataclasses import dataclass, field from typing import Optional  import torch from datasets import load_dataset from peft import AutoPeftModelForCausalLM, LoraConfig from tqdm import tqdm from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments  from trl import SFTTrainer from trl.trainer import ConstantLengthDataset   def chars_token_ratio(dataset, tokenizer, nb_examples=400):     \"\"\"     Estimate the average number of characters per token in the dataset.     \"\"\"     total_characters, total_tokens = 0, 0     for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):         text = prepare_sample_text(example)         total_characters += len(text)         if tokenizer.is_fast:             total_tokens += len(tokenizer(text).tokens())         else:             total_tokens += len(tokenizer.tokenize(text))      return total_characters / total_tokens   def print_trainable_parameters(model):     \"\"\"     Prints the number of trainable parameters in the model.     \"\"\"     trainable_params = 0     all_param = 0     for _, param in model.named_parameters():         all_param += param.numel()         if param.requires_grad:             trainable_params += param.numel()     print(         f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"     )   def prepare_sample_text(example):     \"\"\"Prepare the text from a sample of the dataset.\"\"\"     text = f\"{example['prompt']}\\n {example['response']}\"     return text   def create_datasets(tokenizer):     dataset = load_dataset(         \"Dahoas/full-hh-rlhf\",         split = \"test\",         use_auth_token=True     )     dataset = dataset.train_test_split(test_size=0.005, seed=None)     train_data = dataset[\"train\"]     valid_data = dataset[\"test\"]     print(f\"Size of the train set: {len(train_data)}. Size of the validation set: {len(valid_data)}\")      chars_per_token = chars_token_ratio(train_data, tokenizer)     print(f\"The character to token ratio of the dataset is: {chars_per_token:.2f}\")      train_dataset = ConstantLengthDataset(         tokenizer,         train_data,         formatting_func=prepare_sample_text,         infinite=True,         seq_length=256,         chars_per_token=chars_per_token,     )     valid_dataset = ConstantLengthDataset(         tokenizer,         valid_data,         formatting_func=prepare_sample_text,         infinite=False,         seq_length=256,         chars_per_token=chars_per_token,     )     return train_dataset, valid_dataset   bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.float16, )  base_model = AutoModelForCausalLM.from_pretrained(     \"TabbyML/SantaCoder-1B\",     quantization_config=bnb_config,     device_map={\"\": 0},     trust_remote_code=True,     use_auth_token=True, ) base_model.config.use_cache = False  peft_config = LoraConfig(     r=8,     lora_alpha=16,     lora_dropout=0.05,     target_modules=[\"c_attn\"],     bias=\"none\",     task_type=\"CAUSAL_LM\", )  tokenizer = AutoTokenizer.from_pretrained(\"TabbyML/SantaCoder-1B\", trust_remote_code=True) tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training   training_args = TrainingArguments(     output_dir=\"sft_santacoder1b\",     per_device_train_batch_size=10,     gradient_accumulation_steps=2,     per_device_eval_batch_size=10,     learning_rate=2e-4,     logging_steps=10,     max_steps=105,     report_to=\"tensorboard\",     save_steps=100,     lr_scheduler_type=\"cosine\",     warmup_steps=1,     optim=\"paged_adamw_32bit\",     fp16=True,     remove_unused_columns=False,     run_name=\"sft_santacoder1b\", )  train_dataset, eval_dataset = create_datasets(tokenizer)  trainer = SFTTrainer(     model=base_model,     train_dataset=train_dataset,     eval_dataset=eval_dataset,     peft_config=peft_config,     packing=True,     max_seq_length=None,     tokenizer=tokenizer,     args=training_args, ) trainer.train() trainer.save_model(\"sft_santacoder1b\")  output_dir = os.path.join(\"sft_santacoder1b\", \"final_checkpoint\") trainer.model.save_pretrained(output_dir)  # Free memory for merging weights del base_model torch.cuda.empty_cache()  model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.float16) model = model.merge_and_unload()  output_merged_dir = os.path.join(\"sft_santacoder1b\", \"final_merged_checkpoint\") model.save_pretrained(output_merged_dir, safe_serialization=True) In\u00a0[\u00a0]: Copied! <pre>! cp -r /content/sft_santacoder1b /content/drive/MyDrive\n</pre> ! cp -r /content/sft_santacoder1b /content/drive/MyDrive In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig\nimport torch\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"TabbyML/SantaCoder-1B\",\n    quantization_config=bnb_config,\n    device_map={\"\": 0},\n    trust_remote_code=True,\n    use_auth_token=True,\n)\nbase_model.config.use_cache = False\n</pre> from transformers import AutoModelForCausalLM, BitsAndBytesConfig from peft import LoraConfig import torch  bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_quant_type=\"nf4\",     bnb_4bit_compute_dtype=torch.float16, )  base_model = AutoModelForCausalLM.from_pretrained(     \"TabbyML/SantaCoder-1B\",     quantization_config=bnb_config,     device_map={\"\": 0},     trust_remote_code=True,     use_auth_token=True, ) base_model.config.use_cache = False In\u00a0[\u00a0]: Copied! <pre>print(base_model)\n</pre> print(base_model) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpresearchpapers/003_DPO_Implementation/DPO_Part2_DPO/","title":"DPO Part2 DPO","text":"In\u00a0[\u00a0]: Copied! <pre>! cp -r /content/drive/MyDrive/sft_santacoder1b /content/\n</pre> ! cp -r /content/drive/MyDrive/sft_santacoder1b /content/ In\u00a0[\u00a0]: Copied! <pre>from google.colab import drive\ndrive.mount('/content/drive')\n</pre> from google.colab import drive drive.mount('/content/drive') In\u00a0[\u00a0]: Copied! <pre>! pip install datasets peft transformers trl accelerate bitsandbytes\n</pre> ! pip install datasets peft transformers trl accelerate bitsandbytes In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre># 0. imports\nimport os\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional\n\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom peft import AutoPeftModelForCausalLM, LoraConfig\nfrom transformers import AutoTokenizer, HfArgumentParser, TrainingArguments\n\nfrom trl import DPOTrainer\n\ndef dpo_data(train_or_val):\n\n    dataset = load_dataset(\n        \"Dahoas/full-hh-rlhf\",\n        split = \"train\",\n        use_auth_token=True\n    )\n\n    original_columns = dataset.column_names\n\n    def return_prompt_and_responses(samples):\n        return {\n            \"prompt\": [prompt for prompt in samples[\"prompt\"]],\n            \"chosen\": samples[\"chosen\"],\n            \"rejected\": samples[\"rejected\"],\n        }\n\n    return dataset.map(\n        return_prompt_and_responses,\n        batched=True,\n        remove_columns=original_columns,\n    )\n\n\nif __name__ == \"__main__\":\n\n    # 1. load a pretrained model\n    model = AutoPeftModelForCausalLM.from_pretrained(\n        \"/content/sft_santacoder1b\",\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        load_in_4bit=True,\n    )\n    model.config.use_cache = False\n\n    model_ref = AutoPeftModelForCausalLM.from_pretrained(\n        \"/content/sft_santacoder1b/\",\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        load_in_4bit=True,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\"/content/sft_santacoder1b/\")\n    tokenizer.pad_token = tokenizer.eos_token\n\n    train_dataset = dpo_data(\"train\")\n    train_dataset = train_dataset.filter(\n        lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) &lt;= 256\n        and len(x[\"prompt\"]) + len(x[\"rejected\"]) &lt;= 256\n    )\n    eval_dataset = dpo_data(\"val\")\n    eval_dataset = eval_dataset.filter(\n        lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) &lt;= 256\n        and len(x[\"prompt\"]) + len(x[\"rejected\"]) &lt;= 256\n    )\n\n    training_args = TrainingArguments(\n        per_device_train_batch_size=2,\n        max_steps=505,\n        logging_steps=10,\n        save_steps=500,\n        gradient_accumulation_steps=4,\n        gradient_checkpointing=True,\n        learning_rate=2e-4,\n        evaluation_strategy=\"steps\",\n        eval_steps=100,\n        output_dir=\"dpo_santacoder1b\",\n        report_to=\"tensorboard\",\n        lr_scheduler_type=\"cosine\",\n        warmup_steps=2,\n        optim=\"paged_adamw_32bit\",\n        fp16=True,\n        remove_unused_columns=False,\n        run_name=\"dpo_llama2\",\n    )\n\n    peft_config = LoraConfig(\n        r=8,\n        lora_alpha=16,\n        lora_dropout=0.05,\n        target_modules=[\"c_attn\", \"c_proj\"],\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    dpo_trainer = DPOTrainer(\n        model,\n        model_ref,\n        args=training_args,\n        beta=0.1,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n        peft_config=peft_config,\n        max_prompt_length=128,\n        max_length=256,\n    )\n\n    # 6. train\n    dpo_trainer.train()\n    dpo_trainer.save_model(\"dpo_santacoder1b\")\n\n    # 7. save\n    output_dir = os.path.join(\"dpo_santacoder1b\", \"final_checkpoint\")\n    dpo_trainer.model.save_pretrained(output_dir)\n</pre> # 0. imports import os from dataclasses import dataclass, field from typing import Dict, Optional  import torch from datasets import Dataset, load_dataset from peft import AutoPeftModelForCausalLM, LoraConfig from transformers import AutoTokenizer, HfArgumentParser, TrainingArguments  from trl import DPOTrainer  def dpo_data(train_or_val):      dataset = load_dataset(         \"Dahoas/full-hh-rlhf\",         split = \"train\",         use_auth_token=True     )      original_columns = dataset.column_names      def return_prompt_and_responses(samples):         return {             \"prompt\": [prompt for prompt in samples[\"prompt\"]],             \"chosen\": samples[\"chosen\"],             \"rejected\": samples[\"rejected\"],         }      return dataset.map(         return_prompt_and_responses,         batched=True,         remove_columns=original_columns,     )   if __name__ == \"__main__\":      # 1. load a pretrained model     model = AutoPeftModelForCausalLM.from_pretrained(         \"/content/sft_santacoder1b\",         low_cpu_mem_usage=True,         torch_dtype=torch.float16,         load_in_4bit=True,     )     model.config.use_cache = False      model_ref = AutoPeftModelForCausalLM.from_pretrained(         \"/content/sft_santacoder1b/\",         low_cpu_mem_usage=True,         torch_dtype=torch.float16,         load_in_4bit=True,     )     tokenizer = AutoTokenizer.from_pretrained(\"/content/sft_santacoder1b/\")     tokenizer.pad_token = tokenizer.eos_token      train_dataset = dpo_data(\"train\")     train_dataset = train_dataset.filter(         lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) &lt;= 256         and len(x[\"prompt\"]) + len(x[\"rejected\"]) &lt;= 256     )     eval_dataset = dpo_data(\"val\")     eval_dataset = eval_dataset.filter(         lambda x: len(x[\"prompt\"]) + len(x[\"chosen\"]) &lt;= 256         and len(x[\"prompt\"]) + len(x[\"rejected\"]) &lt;= 256     )      training_args = TrainingArguments(         per_device_train_batch_size=2,         max_steps=505,         logging_steps=10,         save_steps=500,         gradient_accumulation_steps=4,         gradient_checkpointing=True,         learning_rate=2e-4,         evaluation_strategy=\"steps\",         eval_steps=100,         output_dir=\"dpo_santacoder1b\",         report_to=\"tensorboard\",         lr_scheduler_type=\"cosine\",         warmup_steps=2,         optim=\"paged_adamw_32bit\",         fp16=True,         remove_unused_columns=False,         run_name=\"dpo_llama2\",     )      peft_config = LoraConfig(         r=8,         lora_alpha=16,         lora_dropout=0.05,         target_modules=[\"c_attn\", \"c_proj\"],         bias=\"none\",         task_type=\"CAUSAL_LM\",     )      dpo_trainer = DPOTrainer(         model,         model_ref,         args=training_args,         beta=0.1,         train_dataset=train_dataset,         eval_dataset=eval_dataset,         tokenizer=tokenizer,         peft_config=peft_config,         max_prompt_length=128,         max_length=256,     )      # 6. train     dpo_trainer.train()     dpo_trainer.save_model(\"dpo_santacoder1b\")      # 7. save     output_dir = os.path.join(\"dpo_santacoder1b\", \"final_checkpoint\")     dpo_trainer.model.save_pretrained(output_dir) In\u00a0[\u00a0]: Copied! <pre>from transformers import AutoModelForCausalLM\nfrom peft import PeftModel\nmodel = AutoModelForCausalLM.from_pretrained(\n        \"/content/sft_santacoder1b/final_merged_checkpoint/\", return_dict=True, torch_dtype=torch.float16\n)\nmodel = PeftModel.from_pretrained(model, \"/content/dpo_santacoder1b/final_checkpoint/\")\nmodel.eval()\nmodel = model.merge_and_unload()\n\nmodel.save_pretrained(\"/content/dpo_santacoder1b/final_merged_checkpoint\")\ntokenizer.save_pretrained(\"/content/dpo_santacoder1b/final_merged_checkpoint\")\nmodel.push_to_hub(\"dpo-santacoder1b\")\ntokenizer.push_to_hub(\"dpo-santacoder1b\")\n</pre> from transformers import AutoModelForCausalLM from peft import PeftModel model = AutoModelForCausalLM.from_pretrained(         \"/content/sft_santacoder1b/final_merged_checkpoint/\", return_dict=True, torch_dtype=torch.float16 ) model = PeftModel.from_pretrained(model, \"/content/dpo_santacoder1b/final_checkpoint/\") model.eval() model = model.merge_and_unload()  model.save_pretrained(\"/content/dpo_santacoder1b/final_merged_checkpoint\") tokenizer.save_pretrained(\"/content/dpo_santacoder1b/final_merged_checkpoint\") model.push_to_hub(\"dpo-santacoder1b\") tokenizer.push_to_hub(\"dpo-santacoder1b\") In\u00a0[\u00a0]: Copied! <pre>! cp -r /content/dpo_santacoder1b /content/drive/MyDrive/SantaCoder-DPO\n</pre> ! cp -r /content/dpo_santacoder1b /content/drive/MyDrive/SantaCoder-DPO In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpresearchpapers/003_DPO_Implementation/DPO_Part3_Inference/","title":"DPO Part3 Inference","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install transformers accelerate bitsandbytes peft\n</pre> ! pip install transformers accelerate bitsandbytes peft In\u00a0[\u00a0]: Copied! <pre>from transformers import pipeline\npipe = pipeline(\"text-generation\", \"Vasanth/dpo-santacoder1b\", max_length=256, top_p=0.9, device=\"cuda\", no_repeat_ngram_size=3)\n</pre> from transformers import pipeline pipe = pipeline(\"text-generation\", \"Vasanth/dpo-santacoder1b\", max_length=256, top_p=0.9, device=\"cuda\", no_repeat_ngram_size=3) In\u00a0[\u00a0]: Copied! <pre>pipe(\"Human: Should you buy a case to protect your cell phone? Assistant:\")\n</pre> pipe(\"Human: Should you buy a case to protect your cell phone? Assistant:\") In\u00a0[\u00a0]: Copied! <pre>from transformers import pipeline\npipe1 = pipeline(\"text-generation\", \"TabbyML/SantaCoder-1B\", max_length=256, top_p=0.9, device=\"cuda\", no_repeat_ngram_size=3)\n</pre> from transformers import pipeline pipe1 = pipeline(\"text-generation\", \"TabbyML/SantaCoder-1B\", max_length=256, top_p=0.9, device=\"cuda\", no_repeat_ngram_size=3) In\u00a0[\u00a0]: Copied! <pre>pipe1(\"Should you buy a case to protect your cell phone?\")\n</pre> pipe1(\"Should you buy a case to protect your cell phone?\") In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"res/nlpresearchpapers/004_Platypus/Platypus/","title":"Platypus","text":"In\u00a0[\u00a0]: Copied! <pre>! pip install accelerate peft bitsandbytes transformers trl\n</pre> ! pip install accelerate peft bitsandbytes transformers trl <pre>Collecting accelerate\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 244.2/244.2 kB 4.4 MB/s eta 0:00:00\nCollecting peft\n  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 72.9/72.9 kB 11.6 MB/s eta 0:00:00\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 92.6/92.6 MB 9.0 MB/s eta 0:00:00\nCollecting transformers\n  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.4/7.4 MB 115.2 MB/s eta 0:00:00\nCollecting trl\n  Downloading trl-0.5.0-py3-none-any.whl (88 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 88.1/88.1 kB 13.0 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch&gt;=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\nCollecting safetensors (from peft)\n  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.3/1.3 MB 68.7 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\nCollecting huggingface-hub&lt;1.0,&gt;=0.14.1 (from transformers)\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 268.8/268.8 kB 33.6 MB/s eta 0:00:00\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\nCollecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1 (from transformers)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7.8/7.8 MB 115.8 MB/s eta 0:00:00\nRequirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\nCollecting datasets (from trl)\n  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 519.3/519.3 kB 53.1 MB/s eta 0:00:00\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.14.1-&gt;transformers) (4.7.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (1.12)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (3.1.2)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch&gt;=1.10.0-&gt;accelerate) (2.0.0)\nRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch&gt;=1.10.0-&gt;accelerate) (3.27.2)\nRequirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-&gt;torch&gt;=1.10.0-&gt;accelerate) (16.0.6)\nRequirement already satisfied: pyarrow&gt;=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;trl) (9.0.0)\nCollecting dill&lt;0.3.8,&gt;=0.3.0 (from datasets-&gt;trl)\n  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 115.3/115.3 kB 17.1 MB/s eta 0:00:00\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;trl) (1.5.3)\nCollecting xxhash (from datasets-&gt;trl)\n  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 194.1/194.1 kB 24.0 MB/s eta 0:00:00\nCollecting multiprocess (from datasets-&gt;trl)\n  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 134.8/134.8 kB 18.7 MB/s eta 0:00:00\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets-&gt;trl) (3.8.5)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;transformers) (2023.7.22)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;trl) (23.1.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;trl) (6.0.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;trl) (4.0.3)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;trl) (1.9.2)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;trl) (1.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets-&gt;trl) (1.3.1)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-&gt;torch&gt;=1.10.0-&gt;accelerate) (2.1.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets-&gt;trl) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets-&gt;trl) (2023.3)\nRequirement already satisfied: mpmath&gt;=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-&gt;torch&gt;=1.10.0-&gt;accelerate) (1.3.0)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;datasets-&gt;trl) (1.16.0)\nInstalling collected packages: tokenizers, safetensors, bitsandbytes, xxhash, dill, multiprocess, huggingface-hub, transformers, datasets, accelerate, trl, peft\nSuccessfully installed accelerate-0.21.0 bitsandbytes-0.41.1 datasets-2.14.4 dill-0.3.7 huggingface-hub-0.16.4 multiprocess-0.70.15 peft-0.4.0 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0 trl-0.5.0 xxhash-3.3.0\n</pre> In\u00a0[\u00a0]: Copied! <pre>from huggingface_hub import notebook_login\nnotebook_login()\n</pre> from huggingface_hub import notebook_login notebook_login() In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom datasets import load_dataset, Dataset\nfrom peft import LoraConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom trl import SFTTrainer\nimport os\n\ndef platypus_training():\n    data = load_dataset(\"garage-bAInd/Open-Platypus\", split=\"train\")\n    data_df = data.to_pandas()\n    data_df[\"text\"] = data_df[[\"instruction\", \"output\"]].apply(lambda x: x[\"instruction\"] + \" \" + x[\"output\"], axis=1)\n    data_df.drop([\"instruction\", \"output\"], axis=1, inplace=True)\n    data = Dataset.from_pandas(data_df)\n    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n    tokenizer.pad_token = tokenizer.eos_token\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        \"meta-llama/Llama-2-7b-hf\", quantization_config=bnb_config, device_map={\"\": 0}\n    )\n\n    model.config.use_cache=False\n    model.config.pretraining_tp=1\n    peft_config = LoraConfig(\n        r=16,\n        lora_alpha=16,\n        target_modules = [\"gate_proj\" , \"down_proj\", \"up_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n\n    training_arguments = TrainingArguments(\n        output_dir=\"platypus_llama_7b\",\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_32bit\",\n        learning_rate=2e-4,\n        lr_scheduler_type=\"cosine\",\n        save_strategy=\"steps\",\n        save_steps = 50,\n        save_total_limit = 100,\n        logging_steps=10,\n        num_train_epochs=1,\n        max_steps=110,\n        fp16=True,\n        push_to_hub=True\n    )\n\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=data,\n        peft_config=peft_config,\n        dataset_text_field=\"text\",\n        args=training_arguments,\n        tokenizer=tokenizer,\n        packing=False,\n        max_seq_length=512\n    )\n    trainer.train()\n    trainer.push_to_hub()\n    trainer.save_model(\"platypus_llama_7b\")\n\n    output_dir = os.path.join(\"platypus_llama_7b\", \"final_checkpoint\")\n    trainer.model.save_pretrained(output_dir)\n\nif __name__ == \"__main__\":\n    platypus_training()\n</pre> import torch from datasets import load_dataset, Dataset from peft import LoraConfig from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments from trl import SFTTrainer import os  def platypus_training():     data = load_dataset(\"garage-bAInd/Open-Platypus\", split=\"train\")     data_df = data.to_pandas()     data_df[\"text\"] = data_df[[\"instruction\", \"output\"]].apply(lambda x: x[\"instruction\"] + \" \" + x[\"output\"], axis=1)     data_df.drop([\"instruction\", \"output\"], axis=1, inplace=True)     data = Dataset.from_pandas(data_df)     tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")     tokenizer.pad_token = tokenizer.eos_token     bnb_config = BitsAndBytesConfig(         load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True     )      model = AutoModelForCausalLM.from_pretrained(         \"meta-llama/Llama-2-7b-hf\", quantization_config=bnb_config, device_map={\"\": 0}     )      model.config.use_cache=False     model.config.pretraining_tp=1     peft_config = LoraConfig(         r=16,         lora_alpha=16,         target_modules = [\"gate_proj\" , \"down_proj\", \"up_proj\"],         lora_dropout=0.05,         bias=\"none\",         task_type=\"CAUSAL_LM\"     )      training_arguments = TrainingArguments(         output_dir=\"platypus_llama_7b\",         per_device_train_batch_size=8,         gradient_accumulation_steps=4,         optim=\"paged_adamw_32bit\",         learning_rate=2e-4,         lr_scheduler_type=\"cosine\",         save_strategy=\"steps\",         save_steps = 50,         save_total_limit = 100,         logging_steps=10,         num_train_epochs=1,         max_steps=110,         fp16=True,         push_to_hub=True     )      trainer = SFTTrainer(         model=model,         train_dataset=data,         peft_config=peft_config,         dataset_text_field=\"text\",         args=training_arguments,         tokenizer=tokenizer,         packing=False,         max_seq_length=512     )     trainer.train()     trainer.push_to_hub()     trainer.save_model(\"platypus_llama_7b\")      output_dir = os.path.join(\"platypus_llama_7b\", \"final_checkpoint\")     trainer.model.save_pretrained(output_dir)  if __name__ == \"__main__\":     platypus_training() In\u00a0[\u00a0]: Copied! <pre>! cp -r /content/platypus_llama_7b /content/drive/MyDrive/\n</pre> ! cp -r /content/platypus_llama_7b /content/drive/MyDrive/ In\u00a0[\u00a0]: Copied! <pre>torch.cuda.empty_cache()\n</pre> torch.cuda.empty_cache() In\u00a0[\u00a0]: Copied! <pre># from peft import AutoPeftModelForCausalLM\n# from transformers import BitsAndBytesConfig\n# import os\n# import torch\n\n# model = AutoPeftModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/platypus_llama_7b/final_checkpoint\", device_map={\"\": 0})\n# model = model.merge_and_unload()\n# output_merged_dir = os.path.join(\"/content/drive/MyDrive/platypus_llama_7b\", \"final_merged_checkpoint\")\n# model.save_pretrained(output_merged_dir, safe_serialization=True)\n</pre> # from peft import AutoPeftModelForCausalLM # from transformers import BitsAndBytesConfig # import os # import torch  # model = AutoPeftModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/platypus_llama_7b/final_checkpoint\", device_map={\"\": 0}) # model = model.merge_and_unload() # output_merged_dir = os.path.join(\"/content/drive/MyDrive/platypus_llama_7b\", \"final_merged_checkpoint\") # model.save_pretrained(output_merged_dir, safe_serialization=True) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}]}