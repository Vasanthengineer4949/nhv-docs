
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="NHV Channel Documents">
      
      
        <meta name="author" content="Vasanth P">
      
      
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.2, mkdocs-material-9.2.8">
    
    
      
        <title>Evaluation of LLMs Is All You Need - NHV Channel Docs</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.046329b4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.85d0ee34.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="purple">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#evaluation-of-llms-is-all-you-need" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="NHV Channel Docs" class="md-header__button md-logo" aria-label="NHV Channel Docs" data-md-component="logo">
      
  <img src="../img/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            NHV Channel Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Evaluation of LLMs Is All You Need
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="red" data-md-color-accent="purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="red" data-md-color-accent="lime"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/Vasanthengineer4949/nhv-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Vasanthengineer4949/nhv-docs
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../nlpprojs/" class="md-tabs__link">
        
  
    
  
  NLP Projects

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../nlpresearchpapers/" class="md-tabs__link">
        
  
    
  
  NLP Research Papers

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../llmsrelated/" class="md-tabs__link">
        
  
    
  
  LLMs Related

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../mlbootcamp/" class="md-tabs__link">
        
  
    
  
  ML Bootcamp

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../embeddings/" class="md-tabs__link">
        
  
    
  
  Transformer Embeddings

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="NHV Channel Docs" class="md-nav__button md-logo" aria-label="NHV Channel Docs" data-md-component="logo">
      
  <img src="../img/logo.png" alt="logo">

    </a>
    NHV Channel Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/Vasanthengineer4949/nhv-docs" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    Vasanthengineer4949/nhv-docs
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../nlpprojs/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP Projects
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../nlpresearchpapers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    NLP Research Papers
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../llmsrelated/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLMs Related
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../mlbootcamp/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ML Bootcamp
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../embeddings/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer Embeddings
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluation" class="md-nav__link">
    Evaluation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-is-a-good-evaluation" class="md-nav__link">
    What is a good evaluation?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-the-conventional-methods-of-evaluation-doesnt-work-for-llms" class="md-nav__link">
    Why the conventional methods of evaluation doesn't work for LLMs?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#critical-questions-of-evaluation" class="md-nav__link">
    Critical Questions of Evaluation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-to-evaluate" class="md-nav__link">
    Why to evaluate?
  </a>
  
    <nav class="md-nav" aria-label="Why to evaluate?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#increase-model-performance" class="md-nav__link">
    Increase model performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#better-human-llm-interaction" class="md-nav__link">
    Better Human-LLM interaction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#safety-and-reliability" class="md-nav__link">
    Safety and Reliability
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-to-evaluate" class="md-nav__link">
    What to evaluate
  </a>
  
    <nav class="md-nav" aria-label="What to evaluate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#natural-language-processing-tasks" class="md-nav__link">
    Natural Language Processing Tasks
  </a>
  
    <nav class="md-nav" aria-label="Natural Language Processing Tasks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#natural-language-understanding" class="md-nav__link">
    Natural Language Understanding
  </a>
  
    <nav class="md-nav" aria-label="Natural Language Understanding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sentiment-analysis" class="md-nav__link">
    Sentiment Analysis
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#text-classification" class="md-nav__link">
    Text Classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#natural-language-inference" class="md-nav__link">
    Natural Language Inference
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semantic-understanding" class="md-nav__link">
    Semantic Understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reasoning" class="md-nav__link">
    Reasoning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#natural-language-generation" class="md-nav__link">
    Natural Language Generation
  </a>
  
    <nav class="md-nav" aria-label="Natural Language Generation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#summarization" class="md-nav__link">
    Summarization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dialogue" class="md-nav__link">
    Dialogue
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#translation" class="md-nav__link">
    Translation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#question-answering" class="md-nav__link">
    Question Answering
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#open-ended-generation-tasks" class="md-nav__link">
    Open Ended Generation tasks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multilingual-tasks" class="md-nav__link">
    Multilingual Tasks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#factuality" class="md-nav__link">
    Factuality
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#robustness-ethic-bias-and-trustworthiness" class="md-nav__link">
    Robustness, Ethic, Bias and Trustworthiness
  </a>
  
    <nav class="md-nav" aria-label="Robustness, Ethic, Bias and Trustworthiness">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#robustness" class="md-nav__link">
    Robustness
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ethics-and-bias" class="md-nav__link">
    Ethics and Bias
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#trustworthiness" class="md-nav__link">
    Trustworthiness
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#social-science" class="md-nav__link">
    Social Science
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stem-science-technology-engineering-and-mathematics" class="md-nav__link">
    STEM - Science, Technology, Engineering and Mathematics
  </a>
  
    <nav class="md-nav" aria-label="STEM - Science, Technology, Engineering and Mathematics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#math" class="md-nav__link">
    Math
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#science" class="md-nav__link">
    Science
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#engineering" class="md-nav__link">
    Engineering
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#application" class="md-nav__link">
    Application
  </a>
  
    <nav class="md-nav" aria-label="Application">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#agent-application" class="md-nav__link">
    Agent Application
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#search-and-recommendation" class="md-nav__link">
    Search and Recommendation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#personality-testing" class="md-nav__link">
    Personality Testing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#specific-applications" class="md-nav__link">
    Specific applications
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#where-to-evaluate" class="md-nav__link">
    Where to evaluate
  </a>
  
    <nav class="md-nav" aria-label="Where to evaluate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#chatbot-arena" class="md-nav__link">
    Chatbot Arena
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mt-bench" class="md-nav__link">
    MT-Bench
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#helm-holistic-evaluation-of-language-models" class="md-nav__link">
    HELM - Holistic Evaluation of Language Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bigbench-beyond-the-imitation-game-benchmark" class="md-nav__link">
    BIGBench - Beyond the Imitation Game Benchmark
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kola-knowledge-oriented-language-model-evaluation" class="md-nav__link">
    KOLA - Knowledge Oriented Language Model Evaluation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dynabench" class="md-nav__link">
    DynaBench
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mmlu-measuring-massive-multitask-language-understanding" class="md-nav__link">
    MMLU - Measuring Massive Multitask Language Understanding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alpacaeval" class="md-nav__link">
    AlpacaEval
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#open-llm" class="md-nav__link">
    Open LLM
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glue-x" class="md-nav__link">
    GLUE-X
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#promptbench" class="md-nav__link">
    PromptBench
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#specific-task-benchmarks" class="md-nav__link">
    SPECIFIC TASK BENCHMARKS
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#toolbench" class="md-nav__link">
    ToolBench
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#how-to-evaluate" class="md-nav__link">
    How to evaluate
  </a>
  
    <nav class="md-nav" aria-label="How to evaluate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#automatic-evaluation" class="md-nav__link">
    Automatic evaluation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#human-evaluation" class="md-nav__link">
    Human evaluation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#success-and-failure-cases-of-llms" class="md-nav__link">
    Success and Failure Cases of LLMs
  </a>
  
    <nav class="md-nav" aria-label="Success and Failure Cases of LLMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#success" class="md-nav__link">
    Success
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#failure" class="md-nav__link">
    Failure
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="evaluation-of-llms-is-all-you-need">Evaluation of LLMs Is All You Need</h1>
<h2 id="introduction">Introduction</h2>
<p>🔍 <strong>Unlocking the Unseen:</strong> Delve into the world of LLM evaluation, a facet often underestimated yet holding the power to transform your model's prowess! 💎</p>
<p>🔢 <strong>Crucial Queries Explored:</strong></p>
<ul>
<li>
<p><strong>Why Evaluate LLMs?</strong> Discover the critical need for evaluation in maximizing model potential. Uncover why this step is a cornerstone of success, and how it can take your model from good to extraordinary.</p>
</li>
<li>
<p><strong>What to Evaluate?</strong> Explore the multifaceted approach to evaluation. Understand the dimensions that demand attention – from coherence to context handling – and how they collectively shape model effectiveness.</p>
</li>
<li>
<p><strong>Where to Evaluate From?</strong> Learn about the datasets that fuel evaluation. Explore the diverse sources required to gauge the model's performance across various aspects addressed in the 'What' section.</p>
</li>
<li>
<p><strong>How to Execute Evaluation?</strong> Unveil the strategies and techniques to conduct effective evaluation. From methodologies to metrics, this section provides the roadmap to assessing and enhancing model powers</p>
</li>
</ul>
<p>🚀 <strong>Elevate Your Model:</strong> Elevating your model's performance is just a click away! Through comprehensive evaluation, you can turn setbacks into triumphs, failures into stepping stones towards success. 📈 This video is your complete A-to-Z guide, providing insights that can reshape your approach to LLMs.</p>
<h2 id="evaluation">Evaluation</h2>
<ul>
<li>Model evaluation is the process of analyzing the performance of the model with the help of some metrics</li>
<li>Evaluating an LLM performance involves assessing factors such as language fluency, coherence, contextual understanding, factual accuracy, and ability to generate relevant and meaningful responses.</li>
</ul>
<h2 id="what-is-a-good-evaluation">What is a good evaluation?</h2>
<ul>
<li><strong>Correlated with outcomes:</strong> Appropriate metrics used for appropriate models</li>
<li><strong>Very less number of metrics, in an ideal world single metric:</strong> Easy to track and monitor and make a judgement accordingly</li>
<li><strong>Fast and automatic as possible to compute:</strong> We can't completely automate the evaluation. It is important to have a human intervention but yet the evaluation should be as automated and fast as possible</li>
</ul>
<h2 id="why-the-conventional-methods-of-evaluation-doesnt-work-for-llms">Why the conventional methods of evaluation doesn't work for LLMs?</h2>
<ul>
<li>
<p>The data used while training and production are always not the same. It can be as different as possible</p>
</li>
<li>
<p>Another key bottleneck is that in LLMs we wont have definitive results. It has a complex generation behavior which is hard to understand. Though the sentence generated would be different from the ground truth the generated sentence will provide the same contextual meaning.</p>
</li>
<li>
<p><strong>For eg:</strong></p>
</li>
</ul>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>In Traditional ML, lets consider a scenario of sentiment analysis
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    pred = [P, N, P, P]
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    label = [P, N, P, N]
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>For the above set to be evaluated we can use metrices like accuracy which here will be 0.75 but that cannot be the case for LLMS
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>For LLMs, lets consider a case of summarization of a context given
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    pred = Usually LLMs works very well with wide variety   of NLP tasks because they are great generalists by nature
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    label = LLMs are great generalists, so they usually work pretty good with variety of NLP tasks
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>Both convey the same meaning if we see it in a contextual way then the model can be given 100% but usually traditional methods are not qualitative but quantitative.
</code></pre></div>
<p>Thus it is hard to have a conventional metric to quantify the evaluation.</p>
<h2 id="critical-questions-of-evaluation">Critical Questions of Evaluation</h2>
<p>There are four main questions to consider in evaluation. They are:</p>
<ul>
<li>Why to evaluate?</li>
<li>What to evaluate?</li>
<li>Where to evaluate?</li>
<li>How to evaluate?</li>
</ul>
<p>Now lets answer each question in detail one by one right now</p>
<h2 id="why-to-evaluate">Why to evaluate?</h2>
<p>Here are some of the reasons why evaluation is very necessary and why it is said to be one of the most underrated aspect of the LLM pipeline</p>
<h3 id="increase-model-performance">Increase model performance</h3>
<p>By evaluating the model one can understand the strengths and weaknesses of a model. Once a models weaknesses are known one can then move onto the next steps to increase the performance working on the weaknesses of the model.</p>
<p><strong>For eg:</strong> PromptBench indicates a fact that the current LLMs are sensitive to adversarial prompts which implies that you can gain better performance with careful prompt engineering</p>
<h3 id="better-human-llm-interaction">Better Human-LLM interaction</h3>
<p>With better evaluations it can provide better guidance for human-LLMs interaction which can lead to some better experience of the users</p>
<p><strong>For eg:</strong> Once you know if your model is exhibiting an emotion for a specific way of interaction then a work around could be made to make the interaction better to get the desired output from the model</p>
<h3 id="safety-and-reliability">Safety and Reliability</h3>
<p>LLMs have a broad applicability and are used in various sectors even in some sectors which may require safety and reliability like some financial or healthcare institutions. So it is important to ensure the safety and reliability of the model</p>
<p>Thus it is important to have evaluation as one of the most important discipline in the LLM building pipeline</p>
<h2 id="what-to-evaluate">What to evaluate</h2>
<p>Once we find the answer to the question we can claim the strengths and weaknesses of LLMs. the answer to the question is the different tasks which are there to evaluate against the model. Here are the broad categories in which there are the tasks against which the model to be evaluated.</p>
<ul>
<li>Natural Language Processing</li>
<li>Robustness, Ethics, Bias and Trustworthiness</li>
<li>Social Science</li>
<li>STEM</li>
<li>Applications</li>
</ul>
<h3 id="natural-language-processing-tasks">Natural Language Processing Tasks</h3>
<p>The main objective behind the development of LLMs was to get enhanced performance in NLP tasks be it understanding or generation</p>
<h4 id="natural-language-understanding">Natural Language Understanding</h4>
<p>It represents wide range of tasks that aims at model having a better understanding of the input provided. Here are the tasks which comes under the umbrella of NLU</p>
<h5 id="sentiment-analysis">Sentiment Analysis</h5>
<p>Sentiment analysis involves analyzing and interpreting the emotion in text. Typically it is binary or triple. LLMS have shown great performance overall but with low resourced language it wasnt able to perform as expected</p>
<h5 id="text-classification">Text Classification</h5>
<p>Though it is as similar as sentiment analysis, it just not focuses on the sentiment it also includes all the processing and different aspects of the text. Again as provided in sentiment analysis the inference holds here as well with great performance but there are some future work to be done with low resourced languages</p>
<h5 id="natural-language-inference">Natural Language Inference</h5>
<p>It is a task of determnining whether the given hypothesis logically follows the given context known as the premise. ChatGPT outperforms others in handling factual input. It is possible due to RLHF training process which helps in favoring to human preferences. But usally LLMS have poorly in case of NLI tasks and there are large improvements needed in this field</p>
<h5 id="semantic-understanding">Semantic Understanding</h5>
<p>It refers to understanding of the language and the concepts associated. It involves the interpretation and comprehension of words, phrases, sentences and the relationship between them thus processing goes beyond the surface level and understanding the underlying meaning and intent. In individual events LLMs possess a great understanding but with multiple events the performance among events has been subpar. With evaluating against basic phrases the LLMs perform poorly. So in general LLMs semantic understanding performance is poor with lot of room for improvment</p>
<h4 id="reasoning">Reasoning</h4>
<p>Reasoning intuitively if we see also involves semantic understanding which has been a significant challenge for models. To effectively handle reasoning tasks the model not only needs to understand the provided information but also utitlixe reasoning based on the understood information. The evaluation can be broadly categorized into mathematical reasoning, commonsense reasoning, logical and domain-specific reasoning. </p>
<p>Under these categories overall performance has been satisfactory but with high complexity in mathematical or logical reasoning the model performance has been subpar. Models have shown  good performance with commonsense reasoning but with domain specific reasoning there are lots of room for improvement. So in general reasoning capabilities of model needs to improve a lot</p>
<h4 id="natural-language-generation">Natural Language Generation</h4>
<p>It evaluates the capabilities of LLMs in generating specific texts which consists of several tasks including summarization, dialogue generation, machine translation, QA and open ended generation applications</p>
<h5 id="summarization">Summarization</h5>
<p>It is one of the most popular tasks going right now which aims to learn a concise abstract for the given input sentences. In controllable text summarization LLMs have been more extractive compared to human summaries. On general summarization has been having a general performance in summarization tasks and there are requirements for further improvement</p>
<h5 id="dialogue">Dialogue</h5>
<p>Evaluating the performance of LLMs on dialogue tasks is crucial to the develoipment of dialogue system which acts as the interface for human LLM interaction. On general with good NLU capabilites it has given some good performance but here are some of the challenges for the models in which the models tend to make errors usually: long-term multi-turn conversational dependency, fundamental reasoning failure and hallucination</p>
<h5 id="translation">Translation</h5>
<p>LLMs are explicitly not trained for translation task but still they have shown great performance for translation tasks. On an overall factor LLMs have performed translation very well with a scenario like X -&gt; Eng with English being the target language but in the vice versa scenario it has not performed very well and with low resourced languages it has been more worse</p>
<h5 id="question-answering">Question Answering</h5>
<p>QA is a crucial technology in acting as an interface for human-LLM interaction and it has been found useful in many scenarios like search engines, intelligent customer services, QA systems and may more. Overall LLMs have been nearly flawless with the performance on QA tasks but there are potential improvement in answering questions based on social, event and commonsense knowledge</p>
<h5 id="open-ended-generation-tasks">Open Ended Generation tasks</h5>
<p>There are different generation tasks other than the tasks discussed above in which there are tasks like sentence style transfer, variety of writing tasks such as informative, professional, creative and many more. In general LLMs have shown great proficiency in their writing capabilities</p>
<h5 id="multilingual-tasks">Multilingual Tasks</h5>
<p>LLMs generally have performed poorly when it comes to non-Latin languages like Indic languages and even worse with languages with limited resources. So there is a huge room of improvement for LLMs with multilingual tasks</p>
<h5 id="factuality">Factuality</h5>
<p>Factuality in the context of LLMs refers to the extent to which the text generated by the model align with real world truths and verifiable facts. Thus evaluating factuality is of great importance in order to trust the model and use it. For a model to be factual it should not generate misleading or false information also known as factual hallucination. To evaluate the model on this aspect TruthfulQA can be used as a benchmark whcih is designed to cause models to make mistakes in providing factual answers. the findings implicate that increasing sizes does not make the model truthful for which there is a improvement needed</p>
<h3 id="robustness-ethic-bias-and-trustworthiness">Robustness, Ethic, Bias and Trustworthiness</h3>
<p>Some of the crucial aspects of evaluation of LLMs includes robustness, ethics, bias and trustworthiness all checking its personal characteristics under circumstances</p>
<h4 id="robustness">Robustness</h4>
<p>Robustness studies the stability of the model when facing unexpected inputs such as adversarial prompts and OOD. For evaluating on this aspect there are different benchmarks like AdvGLUE, PromptBench and many more. On an overall aspect the model is said to be prompt sensitive and model generally tends to exhibit subpar performance with adversarial prompts</p>
<h4 id="ethics-and-bias">Ethics and Bias</h4>
<p>LLMs have been found to internailize, spread and potentially magnify harmful information existing in the open domain corpus in which it was pretrained on thus exhibiting some bias and its own ethics. When role playing was introduced to the model it caused biased toxicity to some specific entitites upto 6x. Also LLMs were found to have moral biases and cultural values. All these might result in serious risk after deployment of LLMs into the society</p>
<h4 id="trustworthiness">Trustworthiness</h4>
<p>A model is said to be evaluated for its trustworthiness in the following eight aspects: </p>
<ul>
<li>toxicity</li>
<li>stereotype bias</li>
<li>adversarial robustness</li>
<li>OOD robustness</li>
<li>adversarial demonstration robustness</li>
<li>privacy</li>
<li>machine ethics</li>
<li>fairness</li>
</ul>
<p>To deploy a model into a real world scenario it is important for the model to be trustworthy</p>
<h3 id="social-science">Social Science</h3>
<p>Social science involves the study of human society and the individual behaviours also including some subjects like economics, sociology, political science, law. Evaluating the performance of of LLMs in social science is important to know its social problem solving ability and applicability of knowledge to such problems. Overall LLMs has benefitted individuals in addressing social science related tasks improving productivity</p>
<h3 id="stem-science-technology-engineering-and-mathematics">STEM - Science, Technology, Engineering and Mathematics</h3>
<p>Evaluation of models in STEM can help in various aspects like personal education, research, etc... to increase productivity. </p>
<h4 id="math">Math</h4>
<p>In mathematics with simple arithmetic tasks it has performed very well but with tasks like trigonometry, logarithm the performance has been subpar finding it challenging. It has been competent handling fractions, decimal numbers, negative and irrational numbers as well but has failed poorly with lengthy complex and challenging mathematical equations and problems. In general the effectivenss of LLMs is highly influenced by the complexity of the problem</p>
<h4 id="science">Science</h4>
<p>In science it has provided some commendable performance with biology and general simple science related tasks but there are improvments needed with chemisty and physics related tasks especially in physics since the model performs worse in physics than in chemistry problems. Thus improvement is needed in this field</p>
<h4 id="engineering">Engineering</h4>
<p>In the order of difficulty the tasks can be ordered as code generation, software engineering and commonsense planning where in the code generation and software engineering aspect LLMs outperform SOTA outputs and even human outputs but improvements are needed in common sense planning showing there is need for improvement in complex engineering tasks</p>
<h3 id="application">Application</h3>
<h4 id="agent-application">Agent Application</h4>
<p>Instead of just using LLMs on general language tasks LLMs have been equipped with tools lately to expand the capabilites of LLM like ToolLM a comprehensive framework to equip llms with tool use capabilites and there are some other models like KOSMOS-1 for general patterns understanding, TALM again for utilization of tools, Toolformer for optimal use of specific APIs and so on and so forth.</p>
<h4 id="search-and-recommendation">Search and Recommendation</h4>
<p>Assessment of LLMS in search and recommendation is broadly categoried into two areas where firstly in information retrieval LLMs have outperformed SOTA models right now and people find it easy and less time consuming to search in ChatGPT than in Google search.</p>
<p>With enormous NLP capabilites LLMs have proven to be a way to build recommendation systems comprehending user preferences, item descriptions and any type of contextual informations. But there have been scenarios of unfair recommendations from LLMs like ChatGPT which emphasizes the importance of evaluation of fairness in recommendation</p>
<h4 id="personality-testing">Personality Testing</h4>
<p>It measures the personality traits and behavioral tendencies of LLMs applied in wide range ofg tasks showing that LLMs can perform like humans but still there are limitations in current model to effectively understand and generate humour</p>
<h4 id="specific-applications">Specific applications</h4>
<p>LLMs have been said to have a broad applicability but when they are applied on tasks like log parsing, game designing it showed some limitations with the tasks but with potential to improve</p>
<h2 id="where-to-evaluate">Where to evaluate</h2>
<p>Now that we know the tasks/facets in which the model needs to be evaluated lets move on to the next section which is about the benchmarks for LLM. Here are some of the benchmarks for LLMs</p>
<h3 id="chatbot-arena">Chatbot Arena</h3>
<ul>
<li>Chatbot Arena is a platform which is used to compare the performances of diverse chatbot models with anonymous user engagement and voting which shows the preferences of the users in realtime scenarios</li>
<li>Thus chatbot arena is a benchmark which provides insights about the strengths and weaknesses of Chatbot models</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://chat.lmsys.org/">Link</a></p>
</blockquote>
<h3 id="mt-bench">MT-Bench</h3>
<ul>
<li>
<p>MT-Bench is another benchmark to evaluate the conversational capability of a LLM.</p>
</li>
<li>
<p>It evaluates on multi-turn dialogues using questions which are comprehensive in nature created in order to handle the conversations which replicates real case scenarios showing the closest to models practice performance</p>
</li>
<li>
<p>In simple words it is used to evaluate the model's multi turn conversation capability</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://huggingface.co/spaces/lmsys/mt-bench">Link</a></p>
</blockquote>
<h3 id="helm-holistic-evaluation-of-language-models">HELM - Holistic Evaluation of Language Models</h3>
<ul>
<li>
<p>A benchmark as the name suggests provides a comprehensive assessment of LLMs</p>
</li>
<li>
<p>It evaluates the LLMs across 42 scenarios with 59 metrics. Some of the scenarios are QA, IR, Summarization, Reasoning, Coherence, etc... including domain specific knowledge. Some of the metrics are pass@1, rouge, f1, etc...</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://crfm.stanford.edu/helm/latest/">Link</a></p>
</blockquote>
<h3 id="bigbench-beyond-the-imitation-game-benchmark">BIGBench - Beyond the Imitation Game Benchmark</h3>
<ul>
<li>
<p>Bigbench is one of the famous benchmarks going around right now which provides around 204 challeging tasks from 450 authors.</p>
</li>
<li>
<p>It covers various domains like math, biology, commonsense reasoning and many more</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://github.com/google/BIG-bench">Link</a></p>
</blockquote>
<h3 id="kola-knowledge-oriented-language-model-evaluation">KOLA - Knowledge Oriented Language Model Evaluation</h3>
<ul>
<li>
<p>KoLA focuses on the comprehension capabilities of the models</p>
</li>
<li>
<p>It is an important benchmark to assess the indepth language understanding and reasoning capabilities of the LLMs</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://github.com/THU-KEG/KoLA">Link</a></p>
</blockquote>
<h3 id="dynabench">DynaBench</h3>
<ul>
<li>
<p>A benchmark which supports dynamic benchmarking</p>
</li>
<li>
<p>With Dynabench, it collects human-in-the-loop data dynamically, against the current state-of-the-art, in a way that more accurately measures progress</p>
</li>
<li>
<p>It is a more robust way considering it is a crowd sourced mechanism</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://dynabench.org/">Link</a></p>
</blockquote>
<h3 id="mmlu-measuring-massive-multitask-language-understanding">MMLU - Measuring Massive Multitask Language Understanding</h3>
<ul>
<li>
<p>MMLU is a benchmark designed to evaluate the models in a zero shot and few shot setting making it more challenging</p>
</li>
<li>
<p>It covers 57 different subjects in STEM, humanities, social science and many more domains</p>
</li>
<li>
<p>It has difficulty levels from elementary to professional where it tests both the world knowledge and problem solving ability accordingly</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://github.com/hendrycks/test">Link</a></p>
</blockquote>
<h3 id="alpacaeval">AlpacaEval</h3>
<ul>
<li>
<p>It is an automated evaluation benchmark of instuction following models which asssesses the performance of the models across NLP tasks with almost 20K annotations</p>
</li>
<li>
<p>It provides an idea on models robustness, diversity and many more capabilites of LLMS in various domains</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://github.com/tatsu-lab/alpaca_eval">Link</a></p>
</blockquote>
<h3 id="open-llm">Open LLM</h3>
<ul>
<li>
<p>Huggingface Open LLM leaderboard servs an evaluation benchmark by having a public competitive platform </p>
</li>
<li>
<p>It compares and assesses the different models performance on various tasks</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Link</a></p>
</blockquote>
<h3 id="glue-x">GLUE-X</h3>
<ul>
<li>
<p>It is an attempt to create an unified benchmark consisting of 14 publicly available dataset for evaluating the model performance on OOD scenarios</p>
</li>
<li>
<p>It mainly focuses the robustness of the model under various scenarios</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://github.com/YangLinyi/GLUE-X">Link</a></p>
</blockquote>
<h3 id="promptbench">PromptBench</h3>
<ul>
<li>
<p>PromptBench is an another benchmark which focuses on the OOD and adversarial robustness of th model</p>
</li>
<li>
<p>Ir provides a standardized evaluation framework to compare the different prompt engineeering methods and their impact on model performance thus leading to enhancement of model performance</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://github.com/microsoft/promptbench">Link</a></p>
</blockquote>
<h3 id="specific-task-benchmarks">SPECIFIC TASK BENCHMARKS</h3>
<p>The above mentioned benchmarks mainly focuses on the general language level capabilities of the model. The following table shows for specific task</p>
<table>
<thead>
<tr>
<th>Task Name</th>
<th>Task Focus</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>SOCKET</strong></td>
<td>Social Knowledge of LLMs</td>
<td><a href="https://huggingface.co/datasets/Blablablab/SOCKET">Link</a></td>
</tr>
<tr>
<td><strong>ARB</strong></td>
<td>Advanced Reasoning Tasks across multiple domains</td>
<td><a href="https://github.com/TheDuckAI/arb">Link</a></td>
</tr>
<tr>
<td><strong>TRUSTGPT</strong></td>
<td>Ethical considerations - Toxicity, Bias and Values Alignment in context</td>
<td><a href="https://github.com/HowieHwong/TrustGPT">Link</a></td>
</tr>
<tr>
<td><strong>CEval</strong></td>
<td>Reasoning Capabilities in Chinese</td>
<td><a href="hhttps://cevalbenchmark.com/">Link</a></td>
</tr>
<tr>
<td><strong>M3Exam</strong></td>
<td>Comprehensive framework with multiple exams in modalities, languages and levels</td>
<td><a href="https://github.com/DAMO-NLP-SG/M3Exam">Link</a></td>
</tr>
<tr>
<td><strong>MATH</strong></td>
<td>Math Reasoning and Problem solving</td>
<td><a href="https://github.com/hendrycks/math">Link</a></td>
</tr>
<tr>
<td><strong>APPS</strong></td>
<td>Rigorous benchmark to evaluate LLM's capability to generate python code</td>
<td><a href="https://github.com/hendrycks/apps">Link</a></td>
</tr>
<tr>
<td><strong>CUAD</strong></td>
<td>Legal documents review</td>
<td><a href="https://www.atticusprojectai.org/cuad">Link</a></td>
</tr>
<tr>
<td><strong>CVALUES</strong></td>
<td>LLMs safety and responsibility standards</td>
<td><a href="#specific-task-benchmarks">Unable to find</a></td>
</tr>
</tbody>
</table>
<p>Along with the above mentioned benchmarks there are lots of other benchmarks like ANLI, LIT, CoQA, LAMBADA, HellaSwag, LogiQA, MultiNLI and many more. There is a very recent benchmark which came by the name of ToolBench</p>
<h3 id="toolbench">ToolBench</h3>
<ul>
<li>
<p>A benchmark which is used to evaluate tool-augmented LLMs</p>
</li>
<li>
<p>It has 53 commonly used API tools , 264 dialogues and 568 API calls</p>
</li>
<li>
<p>It focuses on enhancing the LLMs practical application</p>
</li>
</ul>
<blockquote>
<p>Benchmark: <a class="md-button" href="https://github.com/OpenBMB/ToolBench">Link</a></p>
</blockquote>
<h2 id="how-to-evaluate">How to evaluate</h2>
<p>Now that we have the benchmark to evaluate it is time to find the answer for the question of how to evaluate. There are two main ways of evaluation. They are:</p>
<h3 id="automatic-evaluation">Automatic evaluation</h3>
<ul>
<li>
<p>Automatic evaluation is the most popular method of evaluation which involves using popular metrics or indicators like ROUGE, BLEU, Accuracy, F1-score, etc... due to its very less complexity</p>
</li>
<li>
<p>With NLU and math tasks which is deterministic in nature this evaluation protocol is used</p>
</li>
<li>
<p>Though there are lots of advantages it is not the best way to evaluate due its subjective nature</p>
</li>
<li>
<p>LLM-Eval is a recently proposed method fo automatic evaluation of open domain conversations</p>
</li>
<li>
<p>An another LLM based method is PandaLM which uses an LLM as a judge to evaluate different models</p>
</li>
</ul>
<h3 id="human-evaluation">Human evaluation</h3>
<ul>
<li>
<p>The capabilites of LLMs has surpassed the standard performance metrics requiring some human evaluation in cases where automatic evaluation is not suitable. <strong>For eg:</strong> Open ended generation</p>
</li>
<li>
<p>Usually it is more preferable to have human evaluation for generation tasks</p>
</li>
<li>
<p>It is more accurate, comprehensive and close to the real world scenario. But it is highly complex and costly</p>
</li>
<li>
<p>In a practical scenario, both the evaluation methods are considered and weighed based on the situation - <strong>AUTO HUMAN EVAL - THE WAY TO GO</strong></p>
</li>
</ul>
<h2 id="success-and-failure-cases-of-llms">Success and Failure Cases of LLMs</h2>
<h3 id="success">Success</h3>
<ul>
<li>Generation of text with fluency</li>
<li>NLU tasks such as classification</li>
<li>Robust contextual comprehension</li>
<li>Satisfying performance across NLP tasks</li>
</ul>
<h3 id="failure">Failure</h3>
<ul>
<li>Bias and Inaccuracy while generation</li>
<li>Limited comprehending abilities with complex reasoning and logic tasks</li>
<li>No dynamic knowledge updataion</li>
<li>Prompt sensitive</li>
<li>Subpar performance in counterfactual tasks</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      &copy; 2023 <a href="https://github.com/Vasanthengineer4949"  target="_blank" rel="noopener">Vasanth</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://www.youtube.com/@NeuralHackswithVasanth" target="_blank" rel="noopener" title="www.youtube.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M549.655 124.083c-6.281-23.65-24.787-42.276-48.284-48.597C458.781 64 288 64 288 64S117.22 64 74.629 75.486c-23.497 6.322-42.003 24.947-48.284 48.597-11.412 42.867-11.412 132.305-11.412 132.305s0 89.438 11.412 132.305c6.281 23.65 24.787 41.5 48.284 47.821C117.22 448 288 448 288 448s170.78 0 213.371-11.486c23.497-6.321 42.003-24.171 48.284-47.821 11.412-42.867 11.412-132.305 11.412-132.305s0-89.438-11.412-132.305zm-317.51 213.508V175.185l142.739 81.205-142.739 81.201z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/vasanthengineer4949/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/Vasanthengineer4949" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.instagram.com/vasanthengineer4949/" target="_blank" rel="noopener" title="www.instagram.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M224.1 141c-63.6 0-114.9 51.3-114.9 114.9s51.3 114.9 114.9 114.9S339 319.5 339 255.9 287.7 141 224.1 141zm0 189.6c-41.1 0-74.7-33.5-74.7-74.7s33.5-74.7 74.7-74.7 74.7 33.5 74.7 74.7-33.6 74.7-74.7 74.7zm146.4-194.3c0 14.9-12 26.8-26.8 26.8-14.9 0-26.8-12-26.8-26.8s12-26.8 26.8-26.8 26.8 12 26.8 26.8zm76.1 27.2c-1.7-35.9-9.9-67.7-36.2-93.9-26.2-26.2-58-34.4-93.9-36.2-37-2.1-147.9-2.1-184.9 0-35.8 1.7-67.6 9.9-93.9 36.1s-34.4 58-36.2 93.9c-2.1 37-2.1 147.9 0 184.9 1.7 35.9 9.9 67.7 36.2 93.9s58 34.4 93.9 36.2c37 2.1 147.9 2.1 184.9 0 35.9-1.7 67.7-9.9 93.9-36.2 26.2-26.2 34.4-58 36.2-93.9 2.1-37 2.1-147.8 0-184.8zM398.8 388c-7.8 19.6-22.9 34.7-42.6 42.6-29.5 11.7-99.5 9-132.1 9s-102.7 2.6-132.1-9c-19.6-7.8-34.7-22.9-42.6-42.6-11.7-29.5-9-99.5-9-132.1s-2.6-102.7 9-132.1c7.8-19.6 22.9-34.7 42.6-42.6 29.5-11.7 99.5-9 132.1-9s102.7-2.6 132.1 9c19.6 7.8 34.7 22.9 42.6 42.6 11.7 29.5 9 99.5 9 132.1s2.7 102.7-9 132.1z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/Vasanth494949" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.top", "navigation.path"], "search": "../assets/javascripts/workers/search.dfff1995.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.dff1b7c8.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>