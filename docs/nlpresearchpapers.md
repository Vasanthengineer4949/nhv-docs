# NLP Research Papers Playlist

Unleash the Power of Language Models! ğŸ’¬âœ¨ Dive into the world of cutting-edge AI with our Trending Concepts in LLMs playlist! ğŸš€ From mind-bending implementations to mind-blowing breakthroughs, we've curated the ultimate collection of videos that explore the latest and greatest in Language Model advancements. Whether you're an AI enthusiast, a tech guru, or just curious about the future, join us on a journey through the hottest trends shaping the world of AI-powered communication. Get ready to be amazed, inspired, and future-ready â€“ press play now and ride the wave of linguistic innovation! 

<iframe width="630" height="330" src="https://www.youtube.com/embed/playlist?list=PLrzE9U41BOPCPXiwm_EE935SE3dh1P2tE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

---
## Videos available
Here are the videos available in the playlist

|Video Name|Link|
|--------|------|
|LoRA Explanation and Implementation|[Link](https://youtu.be/r80HWjFUAGU?si=B0_Os0sfi9j9-IkY)|
|DPO Explanation|[Link](https://youtu.be/P1ji_HlNQ-s?si=rfCiwi7373Luvn6o)|
|DPO Implementation|[Link](https://youtu.be/P1ji_HlNQ-s?si=rFhEgRE3Rtf-gVb6)|
|Platypus Explanation and Implementation|[Link](https://youtu.be/FNKm1VLU__4?si=2W2pIBGa19LS3k5l)|

---
## 1. LoRA - Low Rank Adaptation of LLM Paper Explanation and Implementation

ğŸ” Explore the Cutting-Edge LoRA Paper: Revolutionizing Model Optimization for Experts! ğŸ“–

ğŸ“š Are you ready to unravel the groundbreaking LoRA method that's sending shockwaves through the research world? ğŸŒŸ In this video, I'll take you on an in-depth journey through the LoRA paper, where you'll grasp every intricate detail of this game-changing technique.

ğŸ¯ Imagine boosting your model's performance to new heights with an ingenious approach! The LoRA technique introduces the concept of utilizing low-rank weight decomposition matrices. But that's not all â€“ brace yourself for a paradigm shift as we delve into the method that achieves unparalleled performance. ğŸš€

ğŸ”¥ Say goodbye to the days of laborious updates to the entire pre-trained model. LoRA's brilliance lies in its ability to update only the weights of these specialized matrices. ğŸ“Š This translates to lightning-fast computations and remarkable efficiency, propelling your finetuning of Large Language Models (LLMs) into a new era.

ğŸŒ The LoRA technique isn't just a hidden gem â€“ it's now a cornerstone of modern research. ğŸ“¢ Countless studies have harnessed its power, making it a must-know tool in every researcher's arsenal. Whether you're a seasoned pro or an aspiring academic, embracing LoRA could be the key to unlocking your projects' true potential.



<iframe width="630" height="330" src="https://www.youtube.com/embed/r80HWjFUAGU?si=wI5e9sSR_lDEjvOP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

> Resources: [Slide](res/nlpresearchpapers/001_LoRA/LoRA.pdf){ .md-button } [Data Notebook](res/nlpresearchpapers/001_LoRA/data.ipynb){ .md-button } [Chatbot Notebook](res/nlpresearchpapers/001_LoRA/Chatbot.ipynb){ .md-button }

---
## 2. DPO - Direct Preference Optimization Paper Explanation

ğŸš€ Uncover the Future of Model Optimization: A Deep Dive into the DPO Paper! ğŸ“š

ğŸ”¥ Get ready to explore the cutting-edge world of AI optimization in this comprehensive video breakdown of the DPO paper. This revolutionary method presents a compelling alternative to RLHF (Reinforcement Learning with Human Feedback)

ğŸŒŸ Unlike traditional approaches, the DPO technique operates by computing the log probabilities of preferred and dispreferred outcomes under a model's guidance. But here's where it gets truly exciting â€“ the method strategically optimizes model parameters. How? By elevating the likelihood of preferred responses while decreasing the occurrences of dispreferred ones. The result? A model fine-tuned to align with human preferences in an unprecedented way!

ğŸ”‘ Say farewell to the complexity of conventional RLHF algorithms, because the DPO method operates without the need for a reward model. Instead, it harnesses the power of calculated log probabilities to reshape the model's behavior based on human choices.

âš™ï¸ This video is your ticket to understanding the groundbreaking DPO paper, whether you're an AI enthusiast aiming to stay on the cutting edge or a curious mind seeking to grasp the future of model optimization. By hitting that play button, you're stepping into the forefront of AI evolution.

ğŸŒˆ Don't miss this opportunity to be a part of the AI revolution. Watch now, expand your horizons, and gain insights that could redefine your approach to enhancing models. Your journey towards mastering model optimization starts here! ğŸ“ğŸ¤–


<iframe width="630" height="330" src="https://www.youtube.com/embed/q_BvQyusEjU?si=knR50gD_LPZzB3WA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

> Resources: [Slides](res/nlpresearchpapers/002_DPO_Explanation/DPO.pdf){ .md-button }

---
## 3. DPO - Direct Preference Optimization Paper Implementation

ğŸ”¥ Unleash DPO Power: Step-by-Step Implementation Guide using TRL Library on FREE Google Colab! ğŸš€

ğŸ“š Experience the magic of AI optimization hands-on in this electrifying second installment on DPO â€“ Direct Preference Optimization! Witness the groundbreaking TRL (Transformer Reinforcement Learning) library by @HuggingFace in action, right here on Google Colab for FREE.

ğŸŒŸ Brace yourself for an immersive journey as we demystify the implementation of DPO. Follow along as I guide you through the entire training pipeline, showcasing each intricate step on the SantaCoder1B model â€“ all within the accessible realms of Google Colab's free version!

âš™ï¸ Unlock the secret to DPO implementation with two pivotal steps. Step one: the training of an SFT (Supervised Fine-Tuning) model. Discover how we harness the prowess of the SFT Trainer to skillfully mold the model on the Dahoas/full-hh-rlhf dataset, setting the stage for DPO's transformative power.

âœ¨ But wait, the journey doesn't stop there! Step two is where the true magic happens. Immerse yourself in the art of creating the DPO model using the prowess of the SFT model, all while enjoying the computational efficiency made possible by the ingenious QLoRA technique, perfectly tailored for Google Colab.

ğŸš€ Don't miss this golden opportunity to witness AI optimization unfold, right at your fingertips. Whether you're an experienced coder or an enthusiastic learner, this video is your golden ticket to mastering DPO implementation, revolutionizing the way you enhance your models.

ğŸŒˆ Ready to embark on a journey that merges theory with hands-on practice? Hit that play button and join me in unraveling the marvels of DPO implementation â€“ all done on the Google Colab platform. Your voyage to becoming an AI implementation virtuoso starts NOW! ğŸ“ğŸ¤–



<iframe width="630" height="330" src="https://www.youtube.com/embed//P1ji_HlNQ-s?si=BHjxXCDUoyY6di7D" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

> Resources: [SFT NB](res/nlpresearchpapers/003_DPO_Implementation/DPO_Part1_SFT.ipynb){ .md-button } [Train NB](res/nlpresearchpapers/003_DPO_Implementation/DPO_Part2_DPO.ipynb){ .md-button } [Inference NB](res/nlpresearchpapers/003_DPO_Implementation/DPO_Part3_Inference.ipynb){ .md-button }

---
## 4. Platypus - Cheap, Quick way to Refine LLMs Paper Explanation and Implementation


ğŸ“š Delve into the Intricacies of the Platypus Paper: A Cheap and Quick way to refine LLM Models!

ğŸ”¥ Uncover the secrets behind the groundbreaking Platypus paper in this comprehensive video breakdown. This family of finely-tuned and merged Large Language Models (LLMs) takes the lead on Huggingface's OpenLLM Leaderboard.

ğŸ’¡ The video dissects three key contributions. First, witness the meticulous curation of the Open Platypus dataset. Next, delve into the intricacies of the innovative Finetuning and Merging procedure, spotlighting the ingenious LoRA modules. The Platypus Llama2 model serves as the foundational base, setting the stage for the model's prowess.

ğŸš€ The final contribution reveals the step-by-step procedure of Contamination Check and the vital process of removing contaminated data.

ğŸŒŸ Whether you're a language model enthusiast or a research aficionado, this video is your gateway to understanding the monumental Platypus advancements in LLM models. Hit play now to unlock a world of cutting-edge insights! ğŸŒˆğŸ“–


<iframe width="630" height="330" src="https://www.youtube.com/embed//FNKm1VLU__4?si=HkVZ94nvfZhwB_wP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

> Resources: [Slides](res/nlpresearchpapers/004_Platypus/Platypus.pdf){ .md-button } [Platypus Train Notebook](res/nlpresearchpapers/004_Platypus/Platypus.ipynb){ .md-button }